[
  {
    "id": "411989",
    "input": "Generate a title for the following abstract of a paper: Wearable monitoring devices for ubiquitous health care are becoming a reality that has to deal with limited battery autonomy. Several researchers focus their efforts in reducing the energy consumption of these motes: from efficient micro-architectures, to on-node data processing techniques. In this paper we focus in the optimization of the energy consumption of monitoring devices for the prediction of symptomatic events in chronic diseases in real time. To do this, we have developed an optimization methodology that incorporates information of several sources of energy consumption: the running code for prediction, and the sensors for data acquisition. As a result of our methodology, we are able to improve the energy consumption of the computing process up to 90% with a minimal impact on accuracy. The proposed optimization methodology can be applied to any prediction modeling scheme to introduce the concept of energy efficiency. In this work we test the framework using Grammatical Evolutionary algorithms in the prediction of chronic migraines. Wearable sensory devices are becoming the enabling technology for many applications in healthcare and well-being, where computational elements are tightly coupled with the human body to monitor specific events about their subjects. Classification algorithms are the most commonly used machine learning modules that detect events of interest in these systems. The use of accurate and resource-efficient classification algorithms is of key importance because wearable nodes operate on limited resources on one hand and intend to recognize critical events (e.g., falls) on the other hand. These algorithms are used to map statistical features extracted from physiological signals onto different states such as health status of a patient or type of activity performed by a subject. Conventionally selected features may lead to rapid battery depletion, mainly due to the absence of computing complexity criterion while selecting prominent features. In this paper, we introduce the notion of power-aware feature selection, which aims at minimizing energy consumption of the data processing for classification applications such as action recognition. Our approach takes into consideration the energy cost of individual features that are calculated in real-time. A graph model is introduced to represent correlation and computing complexity of the features. The problem is formulated using integer programming and a greedy approximation is presented to select the features in a power-efficient manner. Experimental results on thirty channels of activity data collected from real subjects demonstrate that our approach can significantly reduce energy consumption of the computing module, resulting in more than 30 percent energy savings while achieving 96.7 percent classification accuracy. Many wearable embedded systems benefit from classification algorithms where statistical features extracted from physiological signals are mapped onto different user's states such as health status of a patient or type of activity performed by a subject. Conventionally selected features lead to rapid battery depletion in these battery-operated systems, mainly due to the absence of computing complexity criterion while selecting prominent features. In this paper, we introduce the notion of power-aware feature selection, which minimizes energy consumption of the signal processing for classification applications. Our approach takes into consideration the energy cost of individual features that are calculated in real-time. The problem is formulated using integer programming and a greedy approximation is presented to select the features in a power-efficient manner. Experimental results on thirty channels of activity data demonstrate that our approach can significantly reduce energy consumption of the computing module resulting in more than 30$% energy savings while achieving 96.7% classification accuracy. Body Sensor Networks (BSN) provide a way to gather continuous observations of human movements, which has a potential of improving medical care quality, and enabling continuous remote patient monitoring. Despite their potential, BSNs face serious werability constraints. Energy optimization is essential since werability is most affected by the battery size of the device. In this paper, we introduce a burst communication technique that takes advantage of data buffering to achieve lower energy-per-bit cost with a possibly higher packet size or more energy efficient communication scheme suited for higher data rates. Our energy model combines the knowledge of the signal processing required to complete a task with the deadline associated with that task to define the optimal burst transmission schedule. Based on the selected energy model, we formulate an optimization function that minimizes the overall energy cost of communication for a given signal processing task. We demonstrate the effectiveness of our approach in sway monitoring BSN applications with a short, medium, and long deadlines. We further demonstrate the relationship between the task deadline extension and the energy cost of the system. Our results show that the proposed approach can improve the cost of communication 85-95% compared to streaming data to the basestation as it becomes available."
  },
  {
    "id": "412248",
    "input": "Generate a title for the following abstract of a paper: Haptic interaction has for a long time been a promise that has not fully been realized in everyday technology due to several reasons. Already for more than 20 years the research community in the field of human-technology interaction has identified multimodal interaction as a potential next mainstream interaction paradigm to replace graphical user interfaces. At the same time, both personal computers and mobile devices have developed rapidly allowing more computing power, more sophisticated feedback through different channels such as display and audio, and more ways of interaction to be used in everyday computing tasks. Within the past few years, haptic interaction has been under rapid research and development. In this article, we will give an introduction to the present state of the art in haptic interaction technology and its promises in mainstream information and communication technology. Already for more than 20 years the research community in the field of human-technology interaction has identified multimodal interaction as a potential next mainstream interaction paradigm to replace graphical user interfaces. During this time, both personal computers and mobile devices have developed rapidly allowing more computing power, more accurate output through different channels such as display and audio, and more ways of interaction to be used in everyday tasks. Within the past few years, haptic interaction has been under rapid research and development. In this short paper I will give an introduction to my invited talk on the development of haptic interaction technology and its promises in mainstream information and communication technology. Haptic gestures and sensations through the sense of touch are currently unavailable in remote communication. There are two main reasons for this: good quality haptic technology has not been widely available and knowledge on the use of this technology is limited. To address these challenges, we studied how users would like to, and managed to create spatial haptic information by gesturing. Two separate scenario-based experiments were carried out: an observation study without technological limitations, and a study on gesturing with a functional prototype with haptic actuators. The first study found three different use strategies for the device. The most common gestures were shaking, smoothing and tapping. Multimodality was requested to create the context for the communication and to aid the interpretation of haptic stimuli. The second study showed that users were able to utilize spatiality in haptic messages (e.g., forward-backward gesture for agreement). However, challenges remain in presenting more complex information via remote haptic communication. The results give guidance for communication activities that are usable in spatial haptic communication, and how to make it possible to enable this form of communication in reality. In September of 2007, the Tampere Unit for Computer Human Interaction (TAUCHI) at the University of Tampere and The Unit of Human-Centered Technology (IHTE) at the Tampere University of Technology initiated a joint effort to increase collaboration in the field of human-technology interaction (HTI). One of the main aims was to develop higher quality education for university students and to carry out joint internationally recognized HTI research. Both research units have their own master and postgraduate students while the focus of education is at IHTE on usability and humancentered design of interactive products and services whereas TAUCHI focuses on human-technology interaction developing it by harmonizing the potential of technology with human abilities, needs, and limitations. Based on our joint analysis we know now that together TAUCHI and IHTE are offering an internationally competitive master's program consisting of more than 40 basic, intermediate and advanced level courses. Although both units are partners in the national Graduate School in User- Centered Information Technology (UCIT) led by TAUCHI we have recognized a clear need for developing and systematizing our doctoral education."
  },
  {
    "id": "412348",
    "input": "Generate a title for the following abstract of a paper: Due to the increasing interest in 3D models in various applications there is a growing need to support e.g. the automatic search or the classification in such databases. As the description of 3D objects is not canonical it is attractive to use invariants for their representation. We recently published a methodology to calculate invariants for continuous 3D objects defined in the real domain R 3 by integrating over the group of Euclidean motion with monomials of a local neighborhood of voxels as kernel functions and we applied it successfully for the classification of scanned pollen in 3D. In this paper we are going to extend this idea to derive invariants from discrete structures, like polygons or 3D-meshes by summing over monomials of discrete features of local support. This novel result for a space-invariant description of discrete structures can be derived by extending Haar integrals over the Euclidean transformation group to Dirac delta functions. Due to the increasing amount of 3D data for various applications there is a growing need for classification and search in such databases. As the representation of 3D objects is not canonical and objects often occur at different spatial position and in different rotational poses, the question arises how to compare and classify the objects. One way is to use invariant features. Group Integration is a constructive approach to generate invariant features. Several variants of Group Integration features are already proposed. In this paper we present two main extensions, we include local directional information and use the Spherical Harmonic Expansion to compute more descriptive features. We apply our methods to 3D-volume data (Pollen grains) and 3D-surface data (Princeton Shape Benchmark) 3D volumetric microscopical techniques (e.g. confocal laser scanning microscopy) have become a standard tool in biomedical applications to record three-dimensional objects with highly anisotropic morphology. To analyze these data in high-throughput experiments, reliable, easy to use and generally applicable pattern recognition tools are required. The major problem of nearly all existing applications is their high specialization to exact one problem, and the their time-consuming adaption to new problems, that has to be done by pattern recognition experts. We therefore search for a tool that can be adapted to new problems just by an interactive training process. Our main idea is therefore to combine object segmentation and recognition into one step by computing voxel-wise gray scale invariants (using nonlinear kernel functions and Haar-integration) on the volumetric multi-channel data set and classify each voxel using support vector machines. After the selection of an appropriate set of nonlinear kernel functions (which allows to integrate previous knowledge, but still needs some expertise), this approach allows a biologist to adapt the recognition system for his problem just by interactively selecting several voxels as training points for each class of objects. Based on these points the classification result is computed and the biologist may refine it by selecting additional training points until the result meets his needs. In this paper we present the theoretical background and a fast approximative algorithm using FFTs for computing Haar-integrals over the very rich class of nonlinear 3-point-kernel functions. The approximation still fulfils the invariance conditions. The experimental application for the recognition of different cell cores of the chorioallantoic membrane is presented in the accompanying paper [1] and in the technical report [2] In this paper we employ different methods for construct- ing histograms from invariant features that are computed locally around a set of salient points. These points repre- sent, together with their neighborhood, the most important visual information in an image. The features used for con- structing the histograms are evaluations of Haar integrals with nonlinear kernel functions. The resulting histograms are able to preserve the local structure of the image in addi- tion to the fact that they are invariant to Euclidean motion. We study and compare the performance of the different his- togram construction methods for a database that consists of 15000 images. ity to use more than one kernel function to build up a fea- ture space and use it for image retrieval. In this paper, we study both cases (using a single or a set of kernel functions) for the construction of the invariant- feature histogram for the purpose of content-based image retrieval. We concentrate on extracting the features aroun d the salient points only. We use the HSV color space as it was found that it performs better than the RGB color space (1). In the case of using a set of kernels to extract fea- ture vectors around the points, we distribute the resulting vectors in several clusters and construct a histogram of the cluster numbers rather than the feature vectors themselves. This is done in order to overcome the high dimensional- ity of the feature vectors (which makes histogram construc- tion difficult). We consider both one-dimensional and two- dimensional cluster-number histograms. The latter reflect s the spatial relationship between the pattern at each point and the patterns of its neighboring points based on cluster"
  },
  {
    "id": "411826",
    "input": "Generate a title for the following abstract of a paper: A framework for cooperative goal-satisfaction in large- scale environments is presented in this paper, focusing on a low complexity physics-oriented approach. The multi-agent systems with which we deal are modeled by a physics-oriented model. According to the model, agent-systems inherit physical properties, and therefore the evolution of the computational systems is similar to the evolution of physical systems. To enable implementation of the model, we provide a detailed algorithm to be used by a single agent within the system. The model and the algorithm are appropriate for large-scale Distributed Problem Solver systems, in which agents try to increase the benefits of the whole system. The complexity is very low, and in some specific cases it has proven to be optimal. The analysis and assessment of the algorithm are performed via the well-known behavior and properties of the physical system which models the computational system. We demonstrate the applicability of a low complexity physics-oriented approach to a large-scale transportation problem. The framework is based on modeling cooperative MAS by a physics-oriented model. According to the model, agent systems inherit physical properties, and therefore the evolution of the computational systems is similar to the evolution of physical systems. We provide a detailed algorithm to be used by a single agent and implement this algorithm in our simulations. Via these we demonstrate effective task allocation and execution in an open, dynamic MAS that consists of thousands of agents and tasks. Task execution in multi-agent environments may require cooperation among agents. Given a set of agents and a set of tasks which they have to satisfy, we consider situations where each task should be attached to a group of agents that will perform the task. Task allocation to groups of agents is necessary when tasks cannot be performed by a single agent. However it may also be beneficial when groups perform more efficiently with respect to the single agents' performance. In this paper we present several solutions to the problem of task allocation among autonomous agents, and suggest that the agents form coalitions in order to perform tasks or improve the efficiency of their performance. We present efficient distributed algorithms with low ratio bounds and with low computational complexities. These properties are proven theoretically and supported by simulations and an implementation in an agent system. Our methods are based on both the algorithmic aspects of combinatorics and approximation algorithms for NP-hard problems. We first present an approach to agent coalition formation where each agent must be a member of only one coalition. Next, we present the domain of overlapping coalitions. We proceed with a discussion of the domain where tasks may have a precedence order. Finally, we discuss the case of implementation in an open, dynamic agent system. For each case we provide an algorithm that will lead agents to the formation of coalitions, where each coalition is assigned a task. Our algorithms are any-time algorithms, they are simple, efficient and easy to implement. In this paper we present a model for coalition formation and payoff distribution in general environments. We focus on a reduced complexity kernel-oriented coalition formation model, and provide a detailed algorithm for the activity of the single rational agent. The model is partitioned into a social level and a strategic level, to distinguish between regulations that must be agreed upon and are forced by agent-designers, and strategies by which each agent acts at will. In addition, we present an implementation of the model and simulation results. From these we conclude that implementing the model for coalition formation among agents increases the benefits of the agents with reasonable time consumption. It also shows that more coalition formations yield more benefits to the agents."
  },
  {
    "id": "41770",
    "input": "Generate a title for the following abstract of a paper: A common objective in learning a model from data is to recover its network structure, while the model parameters are of minor in(cid:173) terest. For example, we may wish to recover regulatory networks from high-throughput data sources. In this paper we examine how Bayesian regularization using a product of independent Dirichlet priors over the model parameters affects the learned model struc(cid:173) ture in a domain with discrete variables. We show that a small scale parameter - often interpreted as \"equivalent sample size\" or \"prior strength\" - leads to a strong regularization of the model structure (sparse graph) given a sufficiently large data set. In par(cid:173) ticular, the empty graph is obtained in the limit of a vanishing scale parameter. This is diametrically opposite to what one may expect in this limit, namely the complete graph from an (unregularized) maximum likelihood estimate. Since the prior affects the parame(cid:173) ters as expected, the scale parameter balances a trade-off between regularizing the parameters vs. the structure of the model. We demonstrate the benefits of optimizing this trade-off in the sense of predictive accuracy. In this paper we present decomposable priors, a family of priors over structure and parameters of tree belief nets for which Bayesian learning with complete observations is tractable, in the sense that the posterior is also decomposable and can be completely determined analytically in polynomial time. This follows from two main results: First, we show that factored distributions over spanning trees in a graph can be integrated in closed form. Second, we examine priors over tree parameters and show that a set of assumptions similar to (Heckerman and al., 1995) constrain the tree parameter priors to be a compactly parametrized product of Dirichlet distributions. Besides allowing for exact Bayesian learning, these results permit us to formulate a new class of tractable latent variable models in which the likelihood of a data point is computed through an ensemble average over tree structures. We present a novel message passing algorithm for approximating the MAP prob- lem in graphical models. The algorithm is similar in structure to max-product but unlike max-product it always converges, and can be proven to find the exact MAP solution in various settings. The algorithm is derived via b lock coordinate descent in a dual of the LP relaxation of MAP, but does not require any tunable parameters such as step size or tree weights. We also describe a generalization of the method to cluster based potentials. The new method is tested on synthetic and real-world problems, and compares favorably with previous approaches. Graphical models are an effective approach for modeling complex objects via local interactions. In such models, a distribution over a set of variables is assumed to factor according to cliques of a graph with potentials assigned to each clique. Finding the assign ment with highest probability in these models is key to using them in practice, and is often referred to as the MAP (maximum aposteriori) assignment problem. In the general case the problem is NP hard, with complexity exponential in the tree-width of the underlying graph. Linear programming (LP) relaxations have proven very useful in approximating the MAP problem, and often yield satisfactory empirical results. These appr oaches relax the constraint that the solution is integral, and generally yield non-integral solutions. H owever, when the LP solution is integral, it is guaranteed to be the exact MAP. For some classes of problems the LP relaxation is provably correct. These include the minimum cut problem and maximum weight matching in bi-partite graphs (8). Although LP relaxations can be solved using standard LP solvers, this may be computationally intensive for large problems (13). The key problem with generic LP solvers is that they do not use the graph structure explicitly and thus may be sub-optimal in terms of computational efficiency. The max-product method (7) is a message passing algorithm that is often used to approximate the MAP problem. In contrast to generic LP solvers, it makes direct use of the graph structure in constructing and passing messages, and is also very simple to implement. The relation between max-product and the LP relaxation has remained largely elusive, although there are some notable exceptions: For tree-structured graphs, max-product and LP both yield the exact MAP. A recent result (1) showed that for maximum weight matching on bi-partite graphs max-product and LP also yield the exact MAP (1). Finally, Tree-Reweighted max-product (TRMP) algorithms (5, 10) were shown to converge to the LP solution for binary xi variables, as shown in (6). In this work, we propose the Max Product Linear Programming algorithm (MPLP) - a very simple variation on max-product that is guaranteed to converge, and has several advantageous properties. MPLP is derived from the dual of the LP relaxation, and is equivalent to block coordinate descent in the dual. Although this results in monotone improvement of the dual objective, global convergence is not always guaranteed since coordinate descent may get stuck in suboptimal points. This can be remedied using various approaches, but in practice we have found MPLP to converge to the LP The prediction of organic reaction outcomes is a fundamental problem in computational chemistry. Since a reaction may involve hundreds of atoms, fully exploring the space of possible transformations is intractable. The current solution utilizes reaction templates to limit the space, but it suffers from coverage and efficiency issues. In this paper, we propose a template-free approach to efficiently explore the space of product molecules by first pinpointing the reaction center - the set of nodes and edges where graph edits occur. Since only a small number of atoms contribute to reaction center, we can directly enumerate candidate products. The generated candidates are scored by a Weisfeiler-Lehman Difference Network that models high-order interactions between changes occurring at nodes across the molecule. Our framework outperforms the top-performing template-based approach with a 10% margin, while running orders of magnitude faster. Finally, we demonstrate that the model accuracy rivals the performance of domain experts."
  },
  {
    "id": "41233",
    "input": "Generate a title for the following abstract of a paper: There still exists an open question on how formal models can be fully realized in the system development phase. The Model Driven Development (MDD) approach has been recently introduced to deal with such a critical issue for building high assurance software systems. There still exists an open question on how formal models can be fully realized in the system development phase. The Model Driven Development (MDD) approach has been recently introduced to deal with such a critical issue for building high assurance software systems. The MDD approach focuses on the transformation of high-level design models to system implementation modules. However, this emerging development approach lacks an adequate procedure to address security issues derived from formal security models. In this paper, we propose an empirical framework to integrate security model representation, security policy specification, and systematic validation of security model and policy, which would be eventually used for accommodating security concerns during the system development. We also describe how our framework can minimize the gap between security models and the development of secure systems. In addition, we overview a proof-of-concept prototype of our tool that facilitates existing software engineering mechanisms to achieve the above-mentioned features of our framework. Model-driven approach has recently received much attention in developing secure software and systems. In addition, software developers have attempted to employ such an emerging approach in the early stage of software development life cycle. However, security concerns are rarely considered and practiced due to the lack of appropriate systematic mechanisms and tools. In this paper, we introduce a multilayered software development life cycle (SDLC), which is based on an assurance management framework (AMF), focusing on the development of authorization systems. AMF facilitates comprehensive realization of formal security model, security policy specification and verification, generation of security enforcement codes, and rigorous conformance testing. We also articulate our experience in analyzing role-based authorization requirements and realizing those requirements in constructing a role-based authorization system. Verification and testing are the important step for software assurance. However, such crucial and yet challenging tasks have not been widely adopted in building access control systems. In this paper we propose a methodology to support automatic analysis and conformance testing for access control systems, integrating those features to Assurance Management Framework (AMF). Our methodology attempts to verify formal specifications of a role-based access control model and corresponding policies with selected security properties. Also, we systematically articulate testing cases from formal specifications and validate conformance to the system design and implementation using those cases. In addition, we demonstrate feasibility and effectiveness of our methodology using SAT and Alloy toolset. Today's home and local-area network environments consist of various types of personal equipments, network devices, and corresponding services. Since such prevalent home network environments frequently deal with private and sensitive information, it is crucial to legitimately provide access control for protecting such emerging environments. As a result, the open services gateway initiative (OSGi) attempted to address this critical issue. However, the current OSGi authorization mechanism is not rigorous enough to fulfill security requirements involved in dynamic OSGi environments. In this paper, we provide a systematic way to adopt a role-based access control (RBAC) approach in OSGi environments. We demonstrate how our authorization framework can achieve important RBAC features and enhance existing primitive access control modules in OSGi service environments. Also, we describe a proof-of-concept prototype of the proposed framework to discuss the feasibility of our approach using an open source implementation of OSGi framework known as Knopflerfish."
  },
  {
    "id": "411736",
    "input": "Generate a title for the following abstract of a paper: The propositional mu-calculus is a propositional logic of programs which incorporates a least fixpoint operator and subsumes the propositional dynamic logic of Fischer and Ladner, the infinite looping construct of Streett, and the game logic of Parikh. We give an elementary time decision procedure, using a reduction to the emptiness problem for automata on infinite trees. A small model theorem is obtained as a corollary. The propositional mu-calculus is a propositional logic of programs which incorporates a least fixpoint operator and subsumes the Propositional Dynamic Logic of Fischer and Ladner, the infinite looping construct of Streett, and the Game Logic of Parikh. We give an elementary time decision procedure, using a reduction to the emptiness problem for automata on infinite trees. A small model theorem is obtained as a corollary. We consider automatic verification of finite state concurrent programs. The global state graph of such a program can be viewed as a finite (Kripke) structure, and a model checking algorithm can be given for determining if a given structure is a model of a specification expressed in a propositional temporal logic. In this paper, we present a unified approach for efficient model checking under a broad class of generalized fairness constraints in a branching time framework extending that of Clarke et al. (1983). Our method applies to any type of fairness expressed in a certain canonical form. Almost all \u2018practical\u2019 types of fairness from the literature, including the fundamental notions of impartiality, weak fairness, and strong fairness, can be succintly written in our canonical form. Moreover, our branching time approach can easily be adapted to handle types of fairness (such as fair reachability of a predicate) which cannot even be expressed in a linear temporal logic. We go on to argue that branching time logic is always better than linear time logic for model checking. We show that given any model checking algorithm for any system of linear time logic (in particular, for the usual system of linear time logic) there is a model checking algorithm of the same order of complexity (in both the structure and formula size) for the corresponding full branching time logic which trivially subsumes the linear time logic in expressive power (in particular, for the system of full branching time logic CTL * ). We also consider an application of our work to the theory of finite automata on infinite strings. The complexity of testing nonemptiness of finite state automata on infinite trees is investigated. It is shown that for tree automata with the pairs (or complemented pairs) acceptance condition having m states and n pairs, nonemptiness can be tested in deterministic time (mn)O(n); however, it is shown that the problem is in general NP-complete (or co-NP-complete, respectively). The new nonemptiness algorithm yields exponentially improved, essentially tight upper bounds for numerous important modal logics of programs, interpreted with the usual semantics over structures generated by binary relations. For example, it follows that satisfiability for the full branching time logic CTL* can be tested in deterministic double exponential time. Another consequence is that satisfiability for propositional dynamic logic (PDL) with a repetition construct (PDL-delta) and for the propositional Mu-calculus ($L\\mu$) can be tested in deterministic single exponential time."
  },
  {
    "id": "41596",
    "input": "Generate a title for the following abstract of a paper: We are investigating how to empower nonprofit community organizations to develop the information technology management practices required to carry out their civic goals. We highlight our methodology of working with nonprofit organizations through three case examples from the field. These examples illustrate that nonprofit organizations are able to and can indeed sustain their IT management practices through various methodological techniques. These techniques---such as scenario development, technology inventory assessment, and volunteer management practices---emphasize the importance of long-term critical planning and design skills. Based on our fieldwork, we enumerate lessons that may be valuable for community stakeholders, designers, researchers, and practitioners. Nonprofit organizations (NPOs) play a substantial role in the economies of many countries, in the delivery of social services, and in many quasi-government functions. But NPOs face many resource challenges; for example, they depend on volunteer labor that is often under-trained and has high turnover resulting in limited knowledge acquisition and decreased sustainability. Ethnographic data from a three-year multi-organizational analysis reveals the occurrence of social and technical patterns during informal technology learning. Construction of a pattern schema grounded in organizational learning and activity theories will enable the development of lightweight interventions in establishing information technology sustainability, self-directed learning, and management processes in NPOs. For 3 years, the authors of this article and several other colleagues have worked with 11 nonprofit community groups to help them take greater control of their information technology in terms of technology acceptance, adoption, and literacy through a research project. As part of this project, the authors explored informal learning methods that the groups could benefit from and practiced them with the community representatives who played key roles in the daily life of the organizations. In the present article, the authors reflect on the developmental trajectories observed for two individuals, each from a different nonprofit organization, with respect to information technology efficacy and ability. The authors analyze these trajectories as a sequence of the following four technology-related roles-technology consumers, technology planners, technology doers, and technology sustainers. The authors describe these roles, the methods used to promote informal learning, and implications for other researchers studying informal learning in communities. The under-representation of women in computer and infor-mation science (CIS) has created a crisis in availability of qualified CIS professionals and diversity of perspectives. Many interventions are being explored but these are primarily institutional programs like curriculum enhancements and mentoring. We describe wConnect, a developmental learning community that leverages social relations and social networking software to support women in CIS. This is a practical issue of some urgency that presents an opportunity for community informatics to impact the CIS profession. We report our progress and lessons learned, so that other organizations can initiate similar outreach activities."
  },
  {
    "id": "411149",
    "input": "Generate a title for the following abstract of a paper: Active Shape Models often require a considerable number of training samples and landmark points on each sample, in order to\n be efficient in practice. We introduce the Fractal Active Shape Models, an extension of Active Shape Models using fractal\n interpolation, in order to surmount these limitations. They require a considerably smaller number of landmark points to be\n determined and a smaller number of variables for describing a shape, especially for irregular ones. Moreover, they are shown\n to be efficient when few training samples are available.\n  Fractal interpolation provides an efficient way to describe data that have an irregular or self-similar structure. Fractal interpolation literature focuses mainly on functions, i.e. on data points linearly ordered with respect to their abscissa. In practice, however, it is often useful to model curves as well as functions using fractal intepolation techniques. After reviewing existing methods for curve fitting using fractal interpolation, we introduce a new method that provides a more economical representation of curves than the existing ones. Comparative results show that the proposed method provides smaller errors or better compression ratios. Fractal interpolation functions are very useful in capturing data that exhibit an irregular (non-smooth) structure. Two new methods to identify the vertical scaling factors of such functions are presented. In particular, they minimize the area of the symmetric difference between the bounding volumes of the data points and their transformed images. Comparative results with existing methods are given that establish the proposed ones as attractive alternatives. In general, they outperform existing methods for both low and high compression ratios. Moreover, lower and upper bounds for the vertical scaling factors that are computed by the first method are presented. A 3D landmark detection method for 3D facial scans is presented and thoroughly evaluated. The main contribution of the presented method is the automatic and pose-invariant detection of landmarks on 3D facial scans under large yaw variations (that often result in missing facial data), and its robustness against large facial expressions. Three-dimensional information is exploited by using 3D local shape descriptors to extract candidate landmark points. The shape descriptors include the shape index, a continuous map of principal curvature values of a 3D object's surface, and spin images, local descriptors of the object's 3D point distribution. The candidate landmarks are identified and labeled by matching them with a Facial Landmark Model (FLM) of facial anatomical landmarks. The presented method is extensively evaluated against a variety of 3D facial databases and achieves state-of-the-art accuracy (4.5-6.3 mm mean landmark localization error), considerably outperforming previous methods, even when tested with the most challenging data."
  },
  {
    "id": "411724",
    "input": "Generate a title for the following abstract of a paper: Modern relational database systems are beginning to support ad hoc queries on mining models. In this paper, we explore novel techniques for optimizing queries that apply mining models to relational data. For such queries, we use the internal structure of the mining model to automatically derive traditional database predicates. We present algorithms for deriving such predicates for some popular discrete mining models:decision trees, naive Bayes, and clustering.Our experiments on Microsoft SQL Server 2000 demonstrate that these derived predicates can signi?cantly reduce the cost of evaluating such queries. Modern relational database systems are beginning to support ad hoc queries on mining models. In this article, we explore novel techniques for optimizing queries that contain predicates on the results of application of mining models to relational data. For such queries, we use the internal structure of the mining model to automatically derive traditional database predicates. We present algorithms for deriving such predicates for a large class of popular discrete mining models: decision trees, naive Bayes, clustering and linear support vector machines. Our experiments on Microsoft SQL Server demonstrate that these derived predicates can significantly reduce the cost of evaluating such queries. Relational databases provide the ability to store user-defined functions and predicates which can be invoked in SQL queries. When evaluation of a user-defined predicate is relatively expensive, the traditional method of evaluating predicates as early as possible is no longer a sound heuristic. There are two previous approaches for optimizing such queries. However, neither is able to guarantee the optimal plan over the desired execution space. We present efficient techniques that are able to guarantee the choice of an optimal plan over the desired execution space. The optimization algorithm with complete rank-ordering improves upon the naive optimization algorithm by exploiting the nature of the cost formulas for join methods and is polynomial in the number of  user-defined predicates (for a given number of relations.) We also  propose pruning rules that significantly reduce the cost of searching the execution space for both the naive algorithm as well as for the optimization algorithm with complete rank-ordering, without compromising optimality. We also propose a conservative local heuristic that is simpler and has low optimization overhead. Although it is not always guaranteed to find the optimal plans, it produces close to optimal plans in most cases. We discuss how, depending on application requirements, to determine the algorithm of choice. It should be emphasized that our optimization algorithms handle user-defined selections as well as user-defined join predicates uniformly. We present complexity analysis  and experimental comparison of the algorithms. The ability to approximately answer aggregation queries accurately and efficiently is of great benefit for decision support and data mining tools. In contrast to previous sampling-based studies, we treat the problem as an optimization problem whose goal is to minimize the error in answering queries in the given workload. A key novelty of our approach is that we can tailor the choice of samples to be robust even for workloads that are \u201csimilar\u201d but not necessarily identical to the given workload. Finally, our techniques recognize the importance of taking into account the variance in the data distribution in a principled manner. We show how our solution can be implemented on a database system, and present results of extensive experiments on Microsoft SQL Server 2000 that demonstrate the superior quality of our method compared to previous work."
  },
  {
    "id": "411112",
    "input": "Generate a title for the following abstract of a paper: Accurate and effective state estimation is essential for nonlinear fractional system, since it can provide some vital operation information about the system. However, inevitably missing measurements and additive uncertainty in the gain will affect the performance of estimation result. Thus, in this paper, in order to deal with these problems, a novel robust extended fractional Kalman filter (REFKF) is developed for states estimation of nonlinear fractional system, by which the states can be estimated accurately even with missing measurements. Finally, simulation results are provided to demonstrate that the proposed method can achieve much better estimation performance than the conventional extended fractional Kalman filter (EFKF). The accurate input-output universe of discourse (UOD) on which membership functions are defined is hard to acquire, especially for nonlinear multi-input and multi-output (MIMO) systems, and control accuracy will reduce greatly in the steady state due to limited fuzzy control rules. This paper presents a design scheme for the adaptive fuzzy logic controllers to control a MIMO system with on-line changeable input-output UOD of the controller, which could solve two problems mentioned above to a great extent. We obtain fuzzy inference rules that are defined by the error and change of error to describe the variation law of the input-output UOD according to the current trend of the controlled process, so the proper widths of the UOD can be derived from the fuzzy inference rules dynamically in real-time. Simulation results for the nonlinear MIMO system show the effectiveness of the proposed fuzzy logic controller. Although most previous work in cache analysis for WCET estimation assumes the LRU replacement policy, in practise more processors use simpler non-LRU policies for lower cost, power consumption and thermal output. This paper focuses on the analysis of FIFO, one of the most widely used cache replacement policies. Previous analysis techniques for FIFO caches are based on the same framework as for LRU caches using qualitative always-hit/always-miss classifications. This approach, though works well for LRU caches, is not suitable to analyze FIFO and usually leads to poor WCET estimation quality. In this paper, we propose a quantitative approach for FIFO cache analysis. Roughly speaking, the proposed quantitative analysis derives an upper bound on the \"miss ratio\" of an instruction (set), which can better capture the FIFO cache behavior and support more accurate WCET estimations. Experiments with benchmarks show that our proposed quantitative FIFO analysis can drastically improve the WCET estimation accuracy over pervious techniques (the average overestimation ratio is reduced from around 70% to 10% under typical setting). Most previous work on cache analysis for WCET estimation assumes a particular replacement policy called LRU. In contrast, much less work has been done for non-LRU policies, since they are generally considered to be very unpredictable. However, most commercial processors are actually equipped with these non-LRU policies, since they are more efficient in terms of hardware cost, power consumption and thermal output, while still maintaining almost as good average-case performance as LRU. In this work, we study the analysis of MRU, a non-LRU replacement policy employed in mainstream processor architectures like Intel Nehalem. Our work shows that the predictability of MRU has been significantly underestimated before, mainly because the existing cache analysis techniques and metrics do not match MRU well. As our main technical contribution, we propose a new cache hit/miss classification, k-Miss, to better capture the MRU behavior, and develop formal conditions and efficient techniques to decide k-Miss memory accesses. A remarkable feature of our analysis is that the k-Miss classifications under MRU are derived by the analysis result of the same program under LRU. Therefore, our approach inherits the advantages in efficiency and precision of the state-of-the-art LRU analysis techniques based on abstract interpretation. Experiments with instruction caches show that our proposed MRU analysis has both good precision and high efficiency, and the obtained estimated WCET is rather close to (typically 1&percnt;\u223c8&percnt; more than) that obtained by the state-of-the-art LRU analysis, which indicates that MRU is also a good candidate for cache replacement policies in real-time systems."
  },
  {
    "id": "411475",
    "input": "Generate a title for the following abstract of a paper: Abstract: Perfect phylogeny is one of the fundamental models for studying evolution. We investigate thefollowing variant of the model: The input is a species-characters matrix. The characters are binary anddirected, i.e., a species can only gain characters. The dierence from standard perfect phylogeny is thatfor some species the states of some characters are unknown. The question is whether one can completethe missing states in a way that admits a perfect phylogeny. The problem arises in classical... Perfect phylogeny is one of the fundamental models for studying evolution. We investigate the following variant of the model: The input is a species-characters matrix. The characters are binary and directed; i.e., a species can only gain characters. The difference from standard perfect phylogeny is that for some species the states of some characters are unknown. The question is whether one can complete the missing states in a way that admits a perfect phylogeny. The problem arises in classical phylogenetic studies, when some states are missing or undetermined. Quite recently, studies that infer phylogenies using inserted repeat elements in DNA gave rise to the same problem. Extant solutions for it take time O(n2m) for n species and m characters. We provide a graph theoretic formulation of the problem as a graph sandwich problem, and give near-optimal $\\tilde{O}(nm)$-time algorithms for the problem. We also study the problem of finding a single, general solution tree, from which any other solution can be obtained by node splitting. We provide an algorithm to construct such a tree, or determine that none exists. We study a problem that arises in computational biology, when wishing to reconstruct the phylogeny of a set of species. In Incomplete Directed Perfect Phylogeny (IDP), the characters are binary and directed (i.e., species can only gain characters), and the states of some characters are unknown. The goal is to complete the missing states in a way consistent with a perfect phylogenetic tree. This problem arises in classical phylogenetic studies, when some states are missing or undetermined, and in recent phylogenetic studies based on repeat elements in DNA. The problem was recently shown to be polynomial. As different completions induce different trees, it is desirable to find a general solution tree. Such a solution is consistent with the data, and every other consistent solution can be obtained from it by node splitting. Unlike the situation for complete datasets, a general solution may not exist for IDP instances. We provide a polynomial algorithm to find a general solution for an IDP instance, or determine that none exists. A haplotype is an m-long binary vector. The xor-genotype of two haplotypes is the m-vector of their coordinate-wise xor. We study the following problem: Given a set of xor-genotypes, reconstruct their haplotypes so that the set of resulting haplotypes can be mapped onto a perfect phylogeny tree. The question is motivated by studying population evolution in human genetics, and is a variant of the perfect phylogeny haplotyping problem that has received intensive attention recently. Unlike the latter problem, in which the input is \"full\" genotypes, here we assume less informative input, and so may be more economical to obtain experimentally.Building on ideas of Gusfield, we show how to solve the problem in polynomial time, by a reduction to the graph realization problem. The actual haplotypes are not uniquely determined by that tree they map onto, and the tree itself may or may not be unique. We show that tree uniqueness implies uniquely determined haplotypes, up to inherent degrees of freedom, and give a sufficient condition for the uniqueness. To actually determine the haplotypes given the tree, additional information is necessary. We show that two or three full genotypes suffice to reconstruct all the haplotypes, and present a linear algorithm for identifying those genotypes."
  },
  {
    "id": "412163",
    "input": "Generate a title for the following abstract of a paper: In this paper, we present the development and validation of a Disaster Management Metamodel, a language that we develop specific for describing disaster management domain, as a foundational component to create a decision support system to unify, facilitate and expedite access to disaster management expertise. The metamodel which consists of four views based on disaster management phases including Mitigation, Preparedness, Response and Recovery-phase classes is developed by using seven (7) steps of metamodel creation process. To check the expressiveness and the completeness aspect of the metamodel, we validate this representational metamodel by analysing a validation over ten well-known disaster management metamodels which are chosen based on a Model Importance Factor criteria. The paper presents the synthesis process, the resulting metamodel and its validation. Generally software model developers use a general purpose language such as Unified Modelling Language (UML) in modelling their domain application models. But when they come to the situation in which the models they create do not perfectly fit the modelling needs as they desire, a more specific domain modelling language offers a better alternative approach. In this paper, we create a Disaster Management (DM) metamodel that can be used to create a disaster management language. It will serve as a representational layer of DM expertise leading to a DM decision support system based on combining and matching different DM activities according to the disaster on hand. A creation process of the metamodel is presented leading to the synthesis of initial metamodel, as a main component to create a decision support system to unify, facilitate and expedite access to DM expertise. Expertise in disaster management (DM) is scarce and often unavailable in a timely manner. Moreover, it is not timely shared as it is often perceived as too tied to kinds of events (floods, bushfires, tsunamis, pandemic or earthquake), leading to catastrophic consequences. In this paper, we lay out a framework to create a decision support system to unify, facilitate and expedite access to DM expertise. We observe that many DM activities are actually common even when the events vary. We provide ontology as a metamodel to describe the various DM activities and desired outcomes. This ontology will serve as a representational layer of DM expertise leading to a DM decision support system based on combining and matching different DM activities according to the disaster on hand. Disaster Management (DM) is a diffused area of knowledge. It has many complex features interconnecting the physical and the social views of the world. Many international and national bodies create knowledge models to allow knowledge sharing and effective DM activities. But these are often narrow in focus and deal with specified disaster types. We analyze thirty such models to uncover that many DM activities are actually common even when the events vary. We then create a unified view of DM in the form of a metamodel. We apply a metamodelling process to ensure that this metamodel is complete and consistent. We validate it and present a representational layer to unify and share knowledge as well as combine and match different DM activities according to different disaster situations."
  },
  {
    "id": "41885",
    "input": "Generate a title for the following abstract of a paper: We present a general framework for determining the number of solutions of constraint satisfaction problems (CSPs) with a high precision. Our first strategy uses additional binary variables for the CSP, and applies an XOR or parity constraint based method introduced previously for Boolean satisfiability (SAT) problems. In the CSP framework, in addition to the naive individual filtering of XOR constraints used in SAT, we are able to apply a global domain filtering algorithm by viewing these constraints as a collection of linear equalities over the field of two elements. Our most promising strategy extends this approach further to larger domains, and applies the so-called generalized XOR constraints directly to CSP variables. This allows us to reap the benefits of the compact and structured representation that CSPs offer. We demonstrate the effectiveness of our counting framework through experimental comparisons with the solution enumeration approach (which, we believe, is the current best generic solution counting method for CSPs), and with solution counting in the context of SAT and integer programming. Model counting is the classical problem of computing the number of solutions of a given propositional formula. It vastly generalizes the NP-complete problem of propositional satisfiability, and hence is both highly useful and extremely expensive to solve in practice. We present a new approach to model counting that is based on adding a carefully chosen number of so-called streamlining constraints to the input formula in order to cut down the size of its solution space in a controlled manner. Each of the additional constraints is a randomly chosen XOR or parity constraint on the problem variables, represented either directly or in the standard CNF form. Inspired by a related yet quite different theoretical study of the properties of XOR constraints, we provide a formal proof that with high probability, the number of XOR constraints added in order to bring the formula to the boundary of being unsatisfiable determines with high precision its model count. Experimentally, we demonstrate that this approach can be used to obtain good bounds on the model counts for formulas that are far beyond the reach of exact counting methods. In fact, we obtain the first non-trivial solution counts for very hard, highly structured combinatorial problem instances. Note that unlike other counting techniques, such as Markov Chain Monte Carlo methods, we are able to provide high-confidence guarantees on the quality of the counts obtained. A promising approach for model counting was recently introduced, which in theory requires the use of large random XOR or parity constraints to obtain near-exact counts of solutions to Boolean formulas. In practice, however, short XOR constraints are preferred as they allow better constraint propagation in SAT solvers. We narrow this gap between theory and practice by presenting experimental evidence that for structured problem domains, very short XOR constraints can lead to probabilistic variance as low as large XOR constraints, and thus provide the same correctness guarantees. We initiate an understanding of this phenomenon by relating it to structural properties of synthetic instances. Consider a combinatorial state space S, such as the set of all truth assignments to N Boolean variables. Given a partition of S, we consider the problem of estimating the size of all the subsets in which S is divided. This problem, also known as computing the density of states, is quite general and has many applications. For instance, if we consider a Boolean formula in CNF and we partition according to the number of violated constraints, computing the density of states is a generalization of both SAT, MAXSAT and model counting. We propose a novel Markov Chain Monte Carlo algorithm to compute the density of states of Boolean formulas that is based on a flat histogram approach. Our method represents a new approach to a variety of inference, learning, and counting problems. We demonstrate its practical effectiveness by showing that the method converges quickly to an accurate solution on a range of synthetic and real-world instances."
  },
  {
    "id": "411501",
    "input": "Generate a title for the following abstract of a paper: We present a method to index objects moving on the plane in order to efficiently answer range queries about their position in the future. This problem is motivated by real-life applications, like predicting future congestion areas in a highway system, or allocating more bandwidth for areas where high concentration of mobile phones is imminent. We consider the problem in the external memory model of computation and present a dynamic technique. An experimental evaluation is included that shows the applicability of our method. We present techniques to index mobile objects in order to effi ciently answer range queries about their future positions. This problem appears in real-life applic ations, such as predicting future congestion areas in a highway system, or allocating more bandwidth for c ells where high concentration of mobile phones is impending. We address the problem in external memo ry and present dynamic solutions, both for the one-dimensional, as well as the two-dimensional cas es. Our approach transforms the problem into a dual space that is easier to index. Finally we discuss a dvantages and disadvantages among the various schemes proposed in literature for indexing mobileobjects. We present a new approach for indexing animated objects and efficiently answering queries about their position in time and space. In particular, we consider an animated movie as a spatiotemporal evolution. A movie is viewed as an ordered sequence of frames, where each frame is a 2D space occupied by the objects that appear in that frame. The queries of interest are range queries of the form, \u9a74find the objects that appear in area $S$ between frames $f_i$ and $f_j$\u9a74 as well as nearest neighbor queries such as, \u9a74find the $q$ nearest objects to a given position $A$ between frames $f_i$ and $f_j$.\u9a74 The straightforward approach to index such objects considers the frame sequence as another dimension and uses a 3D access method (such as, an R-Tree or its variants). This, however, assigns long \u9a74lifetime\u9a74 intervals to objects that appear through many consecutive frames. Long intervals are difficult to cluster efficiently in a 3D index. Instead, we propose to reduce the problem to a partial-persistence problem. Namely, we use a 2D access method that is made partially persistent. We show that this approach leads to faster query performance while still using storage proportional to the total number of changes in the frame evolution. What differentiates this problem from traditional temporal indexing approaches is that objects are allowed to move and/or change their extent continuously between frames. We present novel methods to approximate such object evolutions. We formulate an optimization problem for which we provide an optimal solution for the case where objects move linearly. Finally, we present an extensive experimental study of the proposed methods. While we concentrate on animated movies, our approach is general and can be applied to other spatiotemporal applications as well. Nearest neighbor queries have received much interest in recent years due to their increased importance in advanced database applications. However, past work has addressed such queries in a static setting. In this paper we consider instead a dynamic setting where data objects move continuously. Such a mobile spatiotemporal environment is motivated by real life applications in traffic management, intelligent navigation and cellular communication systems. We consider two versions of nearest neighbor queries depending on whether the temporal predicate is a single time instant or an interval. For example: \"find the closest object to a given object o after 10 minutes from now\", or, \"find the object that will be the closest to object o between 10 and 15 minutes from now\". Since data objects move continuously it is inefficient to update the database about their position at each time instant. Instead our approach is to employ methods that store the motion function of each object and answer nearest neighbor queries by efficiently searching through these methods."
  },
  {
    "id": "41568",
    "input": "Generate a title for the following abstract of a paper: A network of language processors (an NLP system) consists of several language identifying devices (language processors) associated with nodes of a network (in particular case with nodes of a virtual complete graph). The processors rewrite strings (representing the current state of the nodes) according to some prescribed rewriting mode and communicate them along the network via input and output filter languages. In this paper we study properties of NLP systems with L systems in the nodes. A hybrid network of evolutionary processors (an HNEP) consists of several language processors which are located in the nodes of a virtual graph and able to perform only one type of point mutations (insertion, deletion, substitution) on the words found in that node, according to some predefined rules. Each node is associated with an input and an output filter, defined by some random-context conditions. After applying in parallel a point mutation to all the words existing in every node, the new words which are able to pass the output filter of the respective node navigate simultaneously through the network and enter those nodes whose input filter they are able to pass. We show that even the so-called elementary HNEPs are computationally complete. In this case every node is able to perform only one instance of the specified operation: either an insertion, or a deletion, or a substitution of a certain symbol. We also prove that in the case of non-elementary networks, any recursively enumerable language over a common alphabet can be obtained with an HNEP whose underlying structure is a fixed graph depending on the common alphabet only. A hybrid network of evolutionary processors (an HNEP) is a graph where each node is associated with an evolutionary processor (a special rewriting system), a set of words, an input filter and an output filter. Every evolutionary processor is given with a finite set of one type of point mutations (an insertion, a deletion or a substitution of a symbol) which can be applied to certain positions of a string over the domain of the set of these rewriting rules. The HNEP functions by rewriting the words that can be found at the nodes and then re-distributing the resulting strings according to a communication protocol based on a filtering mechanism. The filters are defined by certain variants of random-context conditions. HNEPs can be considered as both language generating devices (GHNEPs) and language accepting devices (AHNEPs). In this paper, by improving the previous results, we prove that any recursively enumerable language can be determined by a GHNEP and an AHNEP with 7 nodes. We also show that the families of GHNEPs and AHNEPs with 2 nodes are not computationally complete. A hybrid network of evolutionary processors (an HNEP) is a graph with a language processor, input and output filters associated to each node. A language processor performs one type of point mutations (insertion, deletion or substitution) on the words in that node. The filters are defined by certain variants of random-context conditions. In this paper, we present a universal complete HNEP with 10 nodes simulating circular Post machines and show that every recursively enumerable language can be generated by a complete HNEP with 10 nodes. Thus, we positively answer the question posed in [5] about the possibility to generate an arbitrary recursively enumerable language over an alphabet Vwith a complete HNEP of a size smaller than 27 + 3\u00b7card(V)."
  },
  {
    "id": "41566",
    "input": "Generate a title for the following abstract of a paper: We address the problem of merging qualitative constraint networks (QCNs) representing agents local preferences or beliefs on the relative position of spatial or temporal entities. Two classes of merging operators which, given a set of input QCNs defined on the same qualitative formalism, return a set of qualitative configurations representing a global view of these QCNs, are pointed out. These operators are based on local distances and aggregation functions. In contrast to QCN merging operators recently proposed in the literature, they take account for each constraint from the input QCNs within the merging process. Doing so, inconsistent QCNs do not need to be discarded at start, hence agents reporting locally consistent, yet globally inconsistent pieces of information (due to limited rationality) can be taken into consideration. We address the problem of merging qualitative constraints networks (QCNs). We point out a merging algorithm which computes a consistent QCN representing a global view of the input set of (possibly conflicting) QCNs. This algorithm is generic in the sense that it does not depend on a specific qualitative formalism. The efficiency of our method comes from the fact that it merges locally the constraints of the input QCNs bearing on the same pairs of variables. We define several constraint merging operators in a way to ensure that the induced QCNs merging operator satisfies some expected properties from a logical standpoint. This paper addresses the problem of merging qualitative constraint networks (QCNs) defined on different qualitative formalisms. Our model is restricted to formalisms where the entities and the relationships between these entities are defined on the same domain. The method is an upstream step to a previous framework dealing with a set of QCNs defined on the same formalism. It consists of translating the input QCNs into a well-chosen common formalism. Two approaches are investigated: in the first one, each input QCN is translated to an equivalent QCN; in the second one, the QCNs are translated to approximations. These approaches take advantage of two dual notions that we introduce, the ones of refinement and abstraction between qualitative formalisms. Spatial or temporal reasoning is an important task for many applications in Artificial Intelligence, such as space scheduling, navigation of robots, etc. Several qualitative approaches have been proposed to repre- sent spatial and temporal entities and their relations. These approaches consider the qualitative aspects of the space relations only, disregarding any quantitative mea- surement. In some applications, e. g. multi-agent sys- tems, spatial or temporal information concerning a set of objects may be conflicting. This paper highlights the problem of merging spatial or temporal qualitative constraints networks. We propose a merging operator which, starting from a set of possibly conflicting qual- itative constraints networks, returns a consistent set of spatial or temporal information representing the result of merging."
  },
  {
    "id": "41828",
    "input": "Generate a title for the following abstract of a paper: Classifiers are often used to detect miscreant activities. We study how an\nadversary can systematically query a classifier to elicit information that\nallows the adversary to evade detection while incurring a near-minimal cost of\nmodifying their intended malfeasance. We generalize the theory of Lowd and Meek\n(2005) to the family of convex-inducing classifiers that partition input space\ninto two sets one of which is convex. We present query algorithms for this\nfamily that construct undetected instances of approximately minimal cost using\nonly polynomially-many queries in the dimension of the space and in the level\nof approximation. Our results demonstrate that near-optimal evasion can be\naccomplished without reverse-engineering the classifier's decision boundary. We\nalso consider general lp costs and show that near-optimal evasion on the family\nof convex-inducing classifiers is generally efficient for both positive and\nnegative convexity for all levels of approximation if p=1.   Classifiers are often used to detect miscreant activities. We study how an adversary can efficiently query a classifier to elicit information that allows the adversary to evade detection at near-minimal cost. We generalize results of Lowd and Meek (2005) to convex-inducing classifiers. We present algorithms that construct undetected instances of near-minimal cost using only polynomially many queries in the dimension of the space and without reverse engineering the decision boundary.  As a growing number of software developers apply machine learning to make key decisions in their systems, adversaries are adapting and launching ever more sophisticated attacks against these systems. The near-optimal evasion problem considers an adversary that searches for a low-cost negative instance by submitting a minimal number of queries to a classifier, in order to effectively evade the classifier. In this position paper, we posit several open problems and alternative variants to the near-optimal evasion problem. Solutions to these problems would significantly advance the state-of-the-art in secure machine learning. Classifier evasion consists in finding for a given instance x the \\\"nearest\\\" instance x\u2032 such that the classifier predictions of x and x\u2032 are different. We present two novel algorithms for systematically computing evasions for tree ensembles such as boosted trees and random forests. Our first algorithm uses a Mixed Integer Linear Program solver and finds the optimal evading instance under an expressive set of constraints. Our second algorithm trades off optimality for speed by using symbolic prediction, a novel algorithm for fast finite differences on tree ensembles. On a digit recognition task, we demonstrate that both gradient boosted trees and random forests are extremely susceptible to evasions. Finally, we harden a boosted tree model without loss of predictive accuracy by augmenting the training set of each boosting round with evading instances, a technique we call adversarial boosting."
  },
  {
    "id": "411303",
    "input": "Generate a title for the following abstract of a paper: In this paper, we investigate the contextual characteristics of media architecture -- parameters that impact its integration in the existing social fabric -- from a socio-demographic (environment), technical (content) and architectural (carrier) perspective. Our analysis draws upon four real-world examples of media architecture, which have been specifically chosen to demonstrate a prototypical range of context-related symptoms, including a deliberate case of vandalism, the disconnection of a building-wide lighting installation, or the inappropriate integration of a screen on an existing architectural facade. In spite of its intrinsic 'dynamic' character, we conclude that media architecture seems not well prepared to adequately respond to changes in its context over time. As a result, we propose a set of guidelines that target all relevant stakeholders, ranging from architectural designers to content managers and public authorities, in an aim to improve media architecture's acceptance and credibility, towards its long-term sustainability in our urban fabric. FixMyStreet (FMS) is a web-based civic participation platform that allows inhabitants to report environmental defects like potholes and damaged pavements to the government. In this paper, we examine the use of FMS in Brussels, the capital city of Belgium. Analyzing a total of 30,041 reports since its inception in 2013, we demonstrate how civic participation on FMS varies between the ethnically diverse districts in Brussels. We compare FMS use to a range of sociodemographic indicators derived from official city statistics as well as geotagged social media data from Twitter. Our statistical analysis revealed several significant differences between the districts that suggested that crowdsourced civic participation platforms tend to marginalize low-income and ethnically diverse communities. In this respect, our findings provide timely evidence to inform the design of more inclusive crowdsourced, civic participation platforms in the future. In this paper, we investigate the true 'public' potential of public displays by shifting the responsibility to create or control content from the traditional central authority to the citizen. To evaluate the potential value of this concept, we have designed and deployed a set of small public displays behind the street-side windows of three separate houses, of which the households were each invited to provide their own content. During a three-week, in-the-wild field study, we have analyzed the impact of citizen-controlled public displays on both participants and community members, and have observed the relationships between the public display and the neighborhood. Our analysis shows how delegating the control over content on a public display to members of the community can influence social cohesion in the immediate environment as it offers an additional opportunity for discourse. Observations also highlight how the effectiveness of citizen-controlled public displays can be dependent on pre-existing social, cultural or linguistic issues. This experiment aims to illustrate the value of a more socially- and location-relevant integration of public displays in our urban neighborhoods as a multifaceted yet democratic medium of public communication. This paper presents the evaluation study of Street Infographics, an urban intervention that visually represent data that is contextually related to local issues, and is visualized through situated displays that are placed within the social and public context of an urban environment. Based on the design characteristics of urban visualization, we defined six specific design principles and applied these in the deployment of a low-fidelity prototype during an in-the-wild study. Designed to augment an existing street sign with socially- and locally-relevant information, the resulting urban visualization encourages people to gain local knowledge, reflect on their perception and even foster social interaction. We describe the design of Street Infographics and its effect on local residents, as measured before and after our intervention. Our case study should be considered one of the first steps towards a better understanding of the true potential of the use of data visualization in a public context, such as for engaging citizens in acting towards a more qualitative and sustainable neighborhood."
  },
  {
    "id": "41263",
    "input": "Generate a title for the following abstract of a paper: We present the IBM systems submitted and evaluated within the CLEAR'06 evaluation campaign for the tasks of single person visual 3D tracking (localization) and 2D face tracking on CHIL seminar data. The two systems are significantly inter-connected to justify their presentation within a single paper as a joint vision system for single person 2D-face and 3D-head tracking, suitable for smart room environments with multiple synchronized, calibrated, stationary cameras. Indeed, in the developed system, face detection plays a pivotal role in 3D person tracking, being employed both in system initialization as well as in detecting possible tracking drift. Similarly, 3D person tracking determines the 2D frame regions where a face detector is subsequently applied. The joint system consists of a number of components that employ detection and tracking algorithms, some of which operate on input from all four corner cameras of the CHIL smart rooms, while others select and utilize two out of the four available cameras. Main system highlights constitute the use of AdaBoost-like multi-pose face detectors, a spatio-temporal dynamic programming algorithm to initialize 3D location hypotheses, and an adaptive subspace learning based tracking scheme with a forgetting mechanism as a means to reduce tracking drift. The system is benchmarked on the CLEAR'06 CHIL seminar database, consisting of 26 lecture segments recorded inside the smart rooms of the UKA and ITC CHIL partners. Its resulting 3D single-person tracking performance is 86% accuracy with a precision of 88 mm, whereas the achieved face tracking score is 54% correct with 37% wrong detections and 19% misses. In terms of speed, an inefficient system implementation runs at about 2 fps on a P4 2.8 GHz desktop. We present a robust vision system for single person tracking inside a smart room using multiple synchronized, calibrated, stationary cameras. The system consists of two main components, namely initialization and tracking, assisted by an additional component that detects tracking drift. The main novelty lies in the adaptive tracking mechanism that is based on subspace learning of the tracked person appearance in selected two-dimensional camera views. The sub-space is learned on the fly, during tracking, but in contrast to the traditional literature approach, an additional \"forgetting\" mechanism is introduced, as a means to reduce drifting. The proposed algorithm replaces mean-shift tracking, previously employed in our work. By combining the proposed technique with a robust initialization component that is based on face detection and spatio-temporal dynamic programming, the resulting vision system significantly outperforms previously reported systems for the task of tracking the seminar presenter in data collected as part of the CHIL project The paper introduces a novel detection and tracking system that pro- vides both frame-view and world-coordinate human location infor- mation, based on video from multiple synchronized and calibrated cameras with overlapping fields of view. The system is developed and evaluated for the specific scenario of a seminar lecturer present- ing in front of an audience inside a \"smart room\", its aim being to track the lecturer's head centroid in the three-dimensional (3D) space and also yield two-dimensional (2D) face information in the avail- able camera views. The proposed approach is primarily based on a statistical appearance model of human faces by means of well- known AdaBoost-like face detectors, extended to address the head pose variation observed in the smart room scenario of interest. The appearance module is complemented by two novel components and assisted by a simple tracking drift detection mechanism. The first component of interest is the initialization module, which employs a spatio-temporal dynamic programming approach with appropriate penalty functions to obtain optimal 3D location hypotheses. The sec- ond is an adaptive subspace learning based 2D tracking scheme with a novel forgetting mechanism, introduced as a means to reduce track- ing drift and increase robustness to illumination and head pose vari- ation. System performance is benchmarked on an extensive database of realistic human interaction in the lecture smart room scenario, col- lected as part of the European integrated project \"CHIL\". The system consistently achieves excellent tracking precision, with a 3D mean tracking error of less than 16 cm, and is demonstrated to outperform four alternative tracking schemes. Furthermore, the proposed system performs relatively well in detecting frontal and near-frontal faces in the available frame views. Visual detection and tracking of humans in complex scenes is a challenging problem with a wide range of applications, for example surveillance and human-computer interaction. In many such applications, time-synchronous views from multiple calibrated cameras are available, and both frame-view and space-level human location information is desired. In such scenarios, efficiently combining the strengths of face detection and person tracking is a viable approach that can provide both levels of information required and improve robustness. In this paper, we propose a novel vision system that detects and tracks human faces automatically, using input from multiple calibrated cameras. The method uses an Adaboost algorithm variant combined with mean shift tracking applied on single camera views for face detection and tracking, and fuses the results on multiple camera views to check for consistency and obtain the three-dimensional head estimate. We apply the proposed system to a lecture scenario in a smart room, on a corpus collected as part of the CHIL European Union integrated project. We report results on both frame-level face detection and three-dimensional head tracking. For the latter, the proposed algorithm achieves similar results with the IBM \u201cPeopleVision\u201d system."
  },
  {
    "id": "41801",
    "input": "Generate a title for the following abstract of a paper: We present a new class of games, local-effect games (LEGs), which exploit structure in a differ- ent way from other compact game representations studied in AI. We show both theoretically and em- pirically that these games often (but not always) have pure-strategy Nash equilibria. Finding a po- tential function is a good technique for finding such equilibria. We give a complete characterization of which LEGs have potential functions and pro- vide the functions in each case; we also show a general case where pure-strategy equilibria exist in the absence of potential functions. In experiments, we show that myopic best-response dynamics con- verge quickly to pure strategy equilibria in games not covered by our positive theoretical results. We analyze the complexity of computing pure strategy Nash equilibria (PSNE) in symmetric games with a fixed number of actions. We restrict ourselves to \"compact\" representations, meaning that the number of players can be exponential in the representation size. We show that in the general case, where utility functions are represented as arbitrary circuits, the problem of deciding the existence of PSNE is NP-complete. For the special case of games with two actions, we show that there always exists a PSNE and give a polynomial-time algorithm for finding one. We then focus on a specific compact representation: piecewise-linear utility functions. We give polynomial-time algorithms for finding a sample PSNE, counting the number of PSNEs, and also provide an FPTAS for finding social-welfare-maximizing equilibria. We extend our piecewise-linear representation to achieve what we believe to be the first compact representation for parameterized families of (symmetric) games. We provide methods for answering questions about a parameterized family without needing to solve each game from the family separately. We analyze the complexity of computing pure strategy Nash equilibria (PSNE) inn symmetric games with a fixed number of actions, where the utilities are compactly represented. Such a representation is able to describe symmetric games whose number of players is exponential in the representation size. We show that in the general case, where utility functions are represented as arbitrary circuits, the problem of deciding the existence of PSNE is NP-complete. For the special case of games with two actions, there always exist a PSNE and we give a polynomial algorithm for finding one. We then focus on a natural representation of utility as piecewise-linear functions, and show that such a representation has nice computational properties. In particular, we give polynomial-time algorithms to count the number of PSNE (thus deciding if such an equilibrium exists) and to find a sample PSNE, when one exists. In many real-world systems, strategic agents' decisions can be understood as complex-i.e., consisting of multiple sub-decisions-and hence can give rise to an exponential number of pure strategies. Examples include network congestion games, simultaneous auctions, and security games. However, agents' sets of strategies are often structured, allowing them to be represented compactly. There currently exists no general modeling language that captures a wide range of commonly seen strategy structure and utility structure. We propose Resource Graph Games (RGGs), the first general compact representation for games with structured strategy spaces, which is able to represent a wide range of games studied in literature. We leverage recent results about multilinearity, a key property of games that allows us to represent the mixed strategies compactly, and, as a result, to compute various equilibrium concepts efficiently. While not all RGGs are multilinear, we provide a general method of converting RGGs to those that are multilinear, and identify subclasses of RGGs whose converted version allow efficient computation."
  },
  {
    "id": "411688",
    "input": "Generate a title for the following abstract of a paper: Computer based simulation of quantum mechanical reactive scattering is a CPU intensive process. Despite the small I/O traffic a single simulation requires several thousand CPU hours. The ABC program provides an impelementation for quantum mechanical reaction simulation in such a way that the code can be efficiently ported to parallel computing platforms. The Computational Chemistry and Application Porting Support groups of the EGEE project worked together to create a grid enabled version of the ABC code. The collaboration resulted a Grid application that is capable of using several clusters and storage servers of the EGEE Grid symultaneously, achieving significant speed-up. The application has been ported to EGEE Grid as a parameter study application with the P-GRADE Grid portal. The paper describes the application porting process, the technical analysis and performance of the local and the Grid enabled ABC application. This paper describes and discusses the implementation, in a high-throughput computing environment, of the ANSYS\u00ae commercial suite. ANSYS\u00ae implements the calculations in a way which can be ported onto parallel architectures efficiently and for this reason the User Support Unit of the Italian Grid Initiative (IGI) and the INFN-Legnaro National Laboratories (INFN-LNL) worked together to implement a Grid enabled version of the ANSYS\u00ae code using the IGI Portal, a powerful and easy to use gateway to distributed computing and storage resources. The collaboration focused on the porting of the code onto the EGI Grid environment for the benefit of the involved community and for those communities interested in exploiting production Grid infrastructures in the same way. The innovative architecture of GPUs has been exploited to the end of implementing an efficient version of the time independent quantum reactive scattering ABC code. The intensive usage of the code as a computational engine for several molecular calculations and crossed beams experiment simulations has prompted a detailed analysis of the utilization of the innovative features of the GPU architecture. ABC has shown to rely on a heavy usage of blocks of recursive sequences of linear algebra matrix operations whose performances vary significantly with the input and the section of the code. This has requested the evaluation of the suitability of different implementation strategies for the various parts of ABC. The outcomes of the related test runs are discussed in the paper. The paper describes the application porting process onto the EGEE grid of GROMACS package that was carried out using the P-GRADE Grid Portal tool implemented in COMPCHEM. For this purpose a new stategy to access local and distributed resources has been designed and a set of visualization tools has been implemented in order to help chemical insight."
  },
  {
    "id": "41186",
    "input": "Generate a title for the following abstract of a paper: Model-Driven Development (MDD) is an emerging paradigm that uses Domain-Specific Modelling Languages (DSMLs) to provide 'correct-by-construction' capabilities for many software development activities. This paper describes a MDD toolsuite called Component Synthesis using Model-Integrated Computing (CoSMIC), a collection of DSMLs that support the development, configuration, deployment, and validation of component-based DRE systems. We also describe how we have applied CoSMIC to an avionics mission computing application to resolve key component-based DRE system development challenges. Our results show that the design-, deployment- and Quality Assurance (QA)-time capabilities provided by CoSMIC help to eliminate key complexities associated with development of QoS-enabled component middleware applications. This paper provides two contributions to the study of developing and applying domain-specific modeling languages (DSMLS) to distributed real-time and embedded (DRE)systems - particularly those systems using standards-based QoS-enabled component middleware. First, it describes the Platform-Independent Component Modeling Language (PICML), which is a DSML that enables developers to define component interfaces, QoS parameters and software building rules, and also generates descriptor files that facilitatesystem deployment. Second, it applies PICML to an unmanned air vehicle (UAV) application portion of an emergency response system to show how PICML resolves keycomponent-based DRE system development challenges. Our results show that the capabilities provided by PICML - combined with its design-and deployment-time validationcapabilities - eliminates many common errors associated with conventional techniques, thereby increasing the effectiveness of applying QoS-enabled component middleware technologies to the DRE system domain. Distributed real-time and embedded (DRE) systems have become critical in domains such as avionics (e.g., flight mission computers), telecommunications (e.g., wireless phone services), tele-medicine (e.g., robotic surgery), and defense applications (e.g., total ship computing environments). These types of system are increasingly interconnected via wireless and wireline networks to form systems of systems. A challenging requirement for these DRE systems involves supporting a diverse set of quality of service (QoS) properties, such as predictable latency/jitter, throughput guarantees, scalability, 24x7 availability, dependability, and security that must be satisfied simultaneously in real-time. Although increasing portions of DRE systems are based on QoS-enabled commercial-off-the-shelf (COTS) hardware and software components, the complexity of managing long lifecycles (often ~15-30 years) remains a key challenge for DRE developers and system integrators. For example, substantial time and effort is spent retrofitting DRE applications when the underlying COTS technology infrastructure changes. This paper provides two contributions that help improve the development, validation, and integration of DRE systems throughout their lifecycles. First, we illustrate the challenges in creating and deploying QoS-enabled component middleware-based DRE applications and describe our approach to resolving these challenges based on a new software paradigm called Model Driven Middleware (MDM), which combines model-based software development techniques with QoS-enabled component middleware to address key challenges faced by developers of DRE systems - particularly composition, integration, and assured QoS for end-to-end operations. Second, we describe the structure and functionality of CoSMIC (Component Synthesis using Model Integrated Computing), which is an MDM toolsuite that addresses key DRE application and middleware lifecycle challenges, including partitioning the components to use distributed resources effectively, validating software configurations, assuring multiple simultaneous QoS properties in real-time, and safeguarding against rapidly changing technology. Enterprise distributed real-time and embedded (DRE) systems are increasingly being developed with the use of component-based software techniques. Unfortunately, commonly used component middleware platforms provide limited support for event-based publish/subscribe (pub/sub) mechanisms that meet both quality-of-service (QoS) and configurability requirements of DRE systems. On the other hand, although pub/sub technologies, such as OMG Data Distribution Service (DDS), support a wide range of QoS settings, the level of abstraction they provide make it hard to configure them due to the significant source-level configuration that must be hard-coded at compile time or tailored at run-time using proprietary, ad hoc configuration logic. Moreover, developers of applications using native pub/sub technologies must write large amounts of boilerplate \"glue\" code to support run-time configuration of QoS properties, which is tedious and error-prone. This paper describes a novel, generative approach that combines the strengths of QoS-enabled pub/sub middleware with component-based middleware technologies. In particular, this paper describes the design and implementation of DDS4CIAO which addresses a number of inherent and accidental complexities in the DDS4CCM standard. DDS4CIAO simplifies the development, deployment, and configuration of component-based DRE systems that leverage DDS's powerful QoS capabilities by provisioning DDS QoS policy settings and simplifying the development of DDS applications."
  },
  {
    "id": "411376",
    "input": "Generate a title for the following abstract of a paper: Entity search is an emerging research topic in Information Retrieval, where the goal is to rank not documents, but entities in response to a given query. A particularly challenging example of this search scenario is when a user's underlying information need is for a list of entities related to a given entity, represented in the query. In this paper, we propose to tackle this problem as a voting process, by considering the occurrence of an entity among the top ranked documents for a given query as a vote for the existence of a relationship between this and the entity in the query. Our proposed approach is evaluated using a large Web test collection, in the context of the TREC 2009 Entity track. The results attest the effectiveness of our approach when compared to the top participants at TREC, with unparalleled gains in terms of recall. Moreover, through a comprehensive failure analysis, we uncover important issues to be considered when tackling this new search scenario and draw valuable insights towards achieving an effective related entity search performance.\n\n When a Web user's underlying information need is not clearly specified from the initial query, an effective approach is to diversify the results retrieved for this query. In this paper, we introduce a novel probabilistic framework for Web search result diversification, which explicitly accounts for the various aspects associated to an underspecified query. In particular, we diversify a document ranking by estimating how well a given document satisfies each uncovered aspect and the extent to which different aspects are satisfied by the ranking as a whole. We thoroughly evaluate our framework in the context of the diversity task of the TREC 2009 Web track. Moreover, we exploit query reformulations provided by three major Web search engines (WSEs) as a means to uncover different query aspects. The results attest the effectiveness of our framework when compared to state-of-the-art diversification approaches in the literature. Additionally, by simulating an upper-bound query reformulation mechanism from official TREC data, we draw useful insights regarding the effectiveness of the query reformulations generated by the different WSEs in promoting diversity. In this paper, we study the emerging Information Retrieval (IR) task of contextual suggestion in location-based social networks. The aim of this task is to make personalised recommendations of venues for entertainments or activities whilst visiting a city, by appropriately representing the context of the user, such as their location and personal interests. Instead of only representing the specific low-level interests of a user, our approach is driven by estimates of the high-level categories of venues that the user may be interested in. Moreover, we argue that an effective model for contextual suggestion should not only promote the categories that the user is interested in, but it should also be capable of eliminating redundancy by diversifying the recommended venues in the sense that they should cover various categories of interest to the given user. Therefore, we adapt web search result diversification approaches to the task of contextual suggestion. For categorising the venues, we use the category classifications employed by location-based social networks such as FourSquare, urban guides such as Yelp, and a large collection of web pages, the ClueWeb12 corpus, to build a textual classifier that is capable of predicting the category distribution for a certain venue given its web page. We thoroughly evaluate our approach using the TREC 2013 Contextual Suggestion track. We conduct a number of experiments where we consider venues from the closed environments of both FourSquare and Yelp, and the general web using the ClueWeb12 corpus. Our empirical results suggest that category diversification consistently improves the effectiveness of the recommendation model over a reasonable baseline that only considers the similarity between the user's profile and venue. The results also give insights on the effectiveness of our approach with different types of users. Online Reputation Management (ORM) is concerned with the monitoring of public opinions on social media for entities such as commercial organisations. In particular, we investigate the task of reputation dimension classification, which aims to classify tweets that mention a business entity into different dimensions (e.g. financial performanceu0027u0027 or products and servicesu0027u0027). However, producing a general reputation dimension classification system that can be used across businesses of different types is challenging, due to the brief nature of tweets and the lack of terms in tweets that relate to specific reputation dimensions. To tackle these issues, we propose a robust and effective tweet enrichment approach that expands tweets with additional discriminative terms from a contemporary Web corpus. Using the RepLab 2014 test collection, we show that our tweet enrichment approach outperforms effective baselines including the top performing submission to RepLab 2014. Moreover, we show that the achieved accuracy scores are very close to the upper bound that our approach could achieve on this collection."
  },
  {
    "id": "41811",
    "input": "Generate a title for the following abstract of a paper: Many natural language researchers are currently turning their attention to treebank development and trying to achieve accuracy and corpus data coverage in their representation formats. This paper presents a data-driven annotation schema developed for an Italian treebank ensuring data coverage and consistency between annotation of linguistic phenomena. The schema is a dependency-based format centered upon the notion of predicate-argument structure augmented with traces to represent discontinuous constituents. The treebank development involves an annotation process performed by a human annotator helped by an interactive parsing tool that builds incrementally syntactic representation of the sentence. To increase the syntactic knowledge of this parser, a specific data-driven strategy has been applied. We describe the cyclical development of the annotation schema highlighting the richness and flexibility of the format, and we present some representational issues. This paper presents a relation-based schema for treebank annotation, and its application in the development of a corpus of Italian sentences. The annotation schema keeps arguments and modifiers distinct and allows for an accurate representation of predicate-argument structure and subcategorization. The accuracy strongly depends on methods adopted for defining the relations which axe tripartite feature structures that consist of a morpho-syntactic, a functional and a semantic component. We presents empirical evidence for these tripartite structures by illustrating phenomena faced in the development of an Italian treebank. The paper investigates the issue of portability of methods and results over treebanks in different languages and annotation formats. In particular, it addresses the problem of converting an Italian treebank, the Turin University Treebank (TUT), developed in dependency format, into the Penn Treebank format, in order to possibly exploit the tools and methods already developed and compare the adequacy of information encoding in the two formats. We describe the procedures for converting the two annotation formats and we present an experiment that evaluates some linguistic knowledge extracted from the two formats, namely sub-categorization frames. This paper explores the convergence between cognitive modeling and engineering solutions to the parsing problem in NLP Natural language presents many sources of ambiguity, and several theories of human parsing claim that ambiguity is resolved by using past (linguistic) experience. In this paper we analyze and refine a connectionist paradigm (Recursive Neural Networks) capable of processing acyclic graphs to perform supervised learning on syntactic trees extracted from a large corpus of parsed sentences. Following a widely accepted hypothesis in psycholinguistics, we assume an incremental parsing process (one word at a time) that keeps a connected partial parse tree at all times. By implementing a parsing simulation procedure, we collect a large amount of data that shows the viability of the RNN as informant of a disambiguation process. We analyze what kind of information is exploited by the connectionist system in order to resolve different sources of ambiguity, and we see how the generalization performance of the system is affected by the tree complexity and the frequency of specific subtrees. We finally propose some enhancements to the architecture in order to achieve a better prediction accuracy."
  },
  {
    "id": "412309",
    "input": "Generate a title for the following abstract of a paper: This paper concerns the message complexity of broadcast in arbitrary point-to-point communication networks. Broadcast is a task initiated by a single processor that wishes to convey a message to all processors in the network. The widely accepted model of communication networks, in which each processor initially knows the identity of its neighbors but does not know the entire network topology, is assumed. Although it seems obvious that the number of messages required for broadcast in this model equals the number of links, no proof of this basic fact has been given before.It is shown that the message complexity of broadcast depends on the exact complexity measure. If messages of unbounded length are counted at unit cost, then broadcast requires &THgr;(&uharl;V&uharr;) messages, where V is the set of processors in the network. It is proved that, if one counts messages of bounded length, then broadcast requires &THgr;(&uharl;E&uharr;) messages, where E is the set of edges in the network. Assuming an intermediate model in which each vertex knows the topology of the network in radius &rgr; \u2265 1 from itself, matching upper and lower bounds of &THgr;(min{&uharl;E&uharr;, &uharl;V&uharr;1+&THgr;(l)/&rgr;}) is proved on the number of messages of bounded length required for broadcast. Both the upper and lower bounds hold for both synchronous and asynchronous network models.The same results hold for the construction of spanning trees, and various other global tasks. In this article, we show that keeping track of history enables significant improvements in the communication complexity of dynamic network protocols. We present a communication optimal maintenance of a spanning tree in a dynamic network. The amortized (on the number of topological changes) message complexity is O(V), where V is the number of nodes in the network. The message size used by the algorithm is O(log &verbar;ID&verbar;) where &verbar;ID&verbar; is the size of the name space of the nodes. Typically, log &verbar;ID&verbar; &equals; O(log V). Previous algorithms that adapt to dynamic networks involved \u03a9 (E) messages per topological change\u2014inherently paying for re-computation of the tree from scratch. Spanning trees are essential components in many distributed algorithms. Some examples include broadcast (dissemination of messages to all network nodes), multicast, reset (general adaptation of static algorithms to dynamic networks), routing, termination detection, and more. Thus, our efficient maintenance of a spanning tree implies the improvement of algorithms for these tasks. Our results are obtained using a novel technique to save communication. A node uses information received in the past in order to deduce present information from the fact that certain messages were NOT sent by the node's neighbor. This technique is one of our main contributions. It is shown that keeping track of history allows significant improvements in the realistic model of communication complexity of dynamic network protocols. The communication complexity for solving an arbitrary graph problem is improved from Theta (E) to Theta (V), thus achieving the lower bound. Moreover, O(V) is also the amortized complexity of solving an arbitrary function (not only graph functions) defined on the local inputs of the nodes. As a corollary, it is found that amortized communication complexity, i.e. incremental cost of adapting to a single topology change, can be smaller than the communication complexity of solving the problem from scratch. The first stage in the solution is a communication-optimal maintenance of a spanning tree in a dynamic network. The second stage is the optimal maintenance of replicas of databases. An important example of this task is the problem of updating the description of the network's topology at every node. For this problem the message complexity is improved from O(EV) to Theta (V). The improvement for a general database is even larger if the size of the database is larger than E. This paper introduces a new distributed data object called Resource Controller that provides an abstraction for managing the consumption of a global resource in a distributed system. Examples of resources that may be managed by such an object include; number of messages sent, number of nodes participating in the protocol, and total CPU time consumed.The Resource Controller object is accessed through a procedure that can be invoked at any node in the network. Before consuming a unit of resource at some node, the controlled algorithm should invoke the procedure at this node, requesting a permit or a rejection.The key characteristics of the Resource Controller object are the constraints that it imposes on the global resource consumption. An (M, W)-Controller guarantees that the total number of permits granted is at most M; it also ensures that, if a request is rejected, then at least M\u2014W permits are eventually granted, even if no more requests are made after the rejected one.In this paper, we describe several message and space-efficient implementations of the Resource Controller object. In particular, we present an (M, W)-Controller whose message complexity is O(n log2n log(M/(W + 1)) where n is the total number of nodes. This is in contrast to the O(nM) message complexity of a fully centralized controller which maintains a global counter of the number of granted permits at some distinguished node and relays all the requests to the node."
  },
  {
    "id": "411541",
    "input": "Generate a title for the following abstract of a paper: We present an extremely fast graph drawing algorithm for very large graphs, which we term ACE ( for Algebraic multigrid Computation of Eigenvectors). ACE exhibits a vast improvement over the fastest algorithms we are currently aware of; using a serial PC, it draws graphs of millions of nodes in less than a minute. ACE finds an optimal drawing by minimizing a quadratic energy function. The minimization problem is expressed as a generalized eigenvalue problem, which is solved rapidly using a novel algebraic multigrid technique. The same generalized eigenvalue problem seems to come up also in other fields; hence ACE appears to be applicable outside graph drawing too. We present a multi-scale layout algorithm for the aesthetic drawing of undirected graphs with straight-line edges. The algorithm is extremely fast, and is capable of drawing graphs of substantially lar- ger size than any other algorithm we are aware of. For example, the algorithm achieves optimal drawings of 1000 vertex graphs in about 2 seconds. The paper contains graphs with over 6000 nodes. The propo- sed algorithm embodies a new multi-scale scheme for drawing graphs, which was motivated by the recently published multi-scale algorithm of Hadany and Harel (7). It can signicantly improve the speed of essen- tially any force-directed method (regardless of that method's ability of drawing weighted graphs or the continuity of its cost-function). We present an algorithm for drawing directed graphs, which is based on rapidly solving a unique one-dimensional optimization problem for each of the axes. The algorithm results in a clear description of the hierarchy structure of the graph. Nodes are not restricted to lie on fixed horizontal layers, resulting in layouts that convey the symmetries of the graph very naturally. The algorithm can be applied without change to cyclic or acyclic digraphs, and even to graphs containing both directed and undirected edges. We also derive a hierarchy index from the input digraph, which quantitatively measures its amount of hierarchy. Abstract- We present an algorithm for drawing directed graphs which is based on rapidly solving a unique one-dimensional optimization problem for each of the axes. The algorithm results in a clear description of the hierarchy structure of the graph. Nodes are not restricted to lie on fixed horizontal layers, resulting in layouts that convey the symmetries of the graph very naturally. The algorithm can be applied without change to cyclic or acyclic digraphs and even to graphs containing both directed and undirected edges. We also derive a hierarchy index from the input digraph, which quantitatively measures its amount of hierarchy."
  },
  {
    "id": "41311",
    "input": "Generate a title for the following abstract of a paper: We propose a new data-driven framework for novel object detection and segmentation, or \u00c2\u00bfobject pop-out\u00c2\u00bf. Traditionally, this task is approached via background subtraction, which requires continuous observation from a stationary camera. Instead, we consider this an image matching problem. We detect novel objects in the scene using an unordered, sparse database of previously captured images of the same general environment. The problem is formulated in a new image composition framework: 1) given an input image, we find a small set of similar matching images; 2) each of the matches is aligned with the input by proposing a set of homography transformations; 3) regions from different transformed matches are stitched together into a single composite image that best matches the input; 4) the difference between the input and the composite is used to \u00c2\u00bfpop-out\u00c2\u00bf new or changed objects. Object discovery algorithms group together image regions that originate from the same object. This process is effective when the input collection of images contains a large number of densely sampled views of each object, thereby creating strong connections between nearby views. However, existing approaches are less effective when the input data only provide sparse coverage of object views. We propose an approach for object discovery that addresses this problem. We collect a database of about 5 million product images that capture 1.2 million objects from multiple views. We represent each region in the input image by a \"bag\" of database object regions. We group input regions together if they share similar \"bags of regions.\" Our approach can correctly discover links between regions of the same object even if they are captured from dramatically different viewpoints. With the help from these added links, our proposed approach can robustly discover object instances even with sparse coverage of the viewpoints. In this paper, we propose a data driven approach to first- person vision. We propose a novel image matching algo- rithm, named Re-Search, that is designed to cope with self- repetitive structures and confusing patterns in the indoor environment. This algorithm uses state-of-art image search techniques, and it matches a query image with a two-pass strategy. In the first pass, a conventional image search al- gorithm is used to search for a small number of images that are most similar to the query image. In the second pass, the retrieval results from the first step are used to discover features that are more distinctive in the local context. We demonstrate and evaluate the Re-Search algorithm in the context of indoor localization, with the illustration of poten- tial applications in object pop-out and data-driven zoom-in. We address the problem of novel view synthesis: given an input image, synthesizing new images of the same object or scene observed from arbitrary viewpoints. We approach this as a learning task but, critically, instead of learning to synthesize pixels from scratch, we learn to copy them from the input image. Our approach exploits the observation that the visual appearance of different views of the same instance is highly correlated, and such correlation could be explicitly learned by training a convolutional neural network (CNN) to predict appearance flows - 2-D coordinate vectors specifying which pixels in the input view could be used to reconstruct the target view. Furthermore, the proposed framework easily generalizes to multiple input views by learning how to optimally combine single-view predictions. We show that for both objects and scenes, our approach is able to synthesize novel views of higher perceptual quality than previous CNN-based techniques."
  },
  {
    "id": "412295",
    "input": "Generate a title for the following abstract of a paper: This paper describes jcc, an integration of the timed default concurrent constraint programming framework [16] (Timed Default cc) into JAVA [7]. jcc is intended for use in education and research, for the programming of embedded reactive systems, for parallel/distributed simulation and modelling (particularly for space, robotics and systems biology applications), and to support the development of constraint-based program analysis and type-checking tools. In fully implementing the Timed Default cc framework, jcc supports the notion of (typed) logical variables (called \"promises\", after [5]), allows the programmer to add his/her own constraint system (an implementation of the Herbrand constraint system is provided), implements (instantaneous) defaults via backtracking, implements a complete renewal of the constraint-store at each time instant, and implements bounded-time execution of the Timed cc control constructs. jcc implements the notion of reactive vats [5] as single threads of execution within the JVM; a vat may be thought of as encapsulating a single synchronous, reactive Timed cc computation. A computation typically consists of a dynamically changing collection of interacting vats (some of which could potentially be located at different JVMs), with dynamically changing connectivity. jcc programs fully inter-operate with JAVA programs, and compile into standard JVM byte-code. jcc programs fully respect the JAVA type system; logical variables are typed. jcc is compatible with the Generic Java [3] extensions, thereby allowing the use of parameterized types. Indeed, jcc may be viewed as an extension of JAVA which replaces JAVA's notoriously difficult imperative thread-based concurrency with the notion of reactive vats interacting via constraints on logical variables. jcc source code is available under the Lesser GNU licence through SourceForge. Synchronous programming (Berry, 1989) is a powerful approach to programming reactive systems. Following the idea that \u201cprocesses are relations extended over time\u201d (Abramsky, 1993), we propose a simple but powerful model for timed, determinate computation, extending the closure-operator model for untimed concurrent constraint programming (CCP). In Saraswat et al . (1994a) we had proposed a model for this called tcc\u2014here we extend the model of tcc to express strong time-outs: if an event A does not happen through time t , cause event B to happen at time t . Such constructs arise naturally in practice (e.g. in modeling transistors) and are supported in synchronous programming languages. The fundamental conceptual difficulty posed by these operations is that they are non-monotonic. We provide compositional semantics to the non-monotonic version of concurrent constraint programming (Default cc) obtained by changing the underlying logic from intuitionistic logic to Reiter's default logic. This allows us to use the same construction (uniform extension through time) to develop Default cc as we had used to develop tcc from cc. Indeed the smooth embedding of cc processes into Default cc processes lifts to a smooth embedding of tcc processes into Default cc processes. We identify a basic set of combinators (that constitute the Default cc programming framework), and provide constructive operational semantics (implemented by us as an interpreter) for which the model is fully abstract. We show that the model is expressive by defining combinators from the synchronous languages. We show that Default cc is compositional and supports the properties of multiform time, orthogonal pre-emption and executable specifications. In addition, Default cc programs can be read as logical formulae (in an intuitionistic temporal logic)\u2014we show that this logic is sound and complete for reasoning about (in)equivalence of Default cc programs. Like the synchronous languages, Default cc programs can be compiled into finite state automata. In addition, the translation can be specified compositionally. This enables separate compilation of Default cc programs and run-time tradeoffs between partial compilation and interpretation. A preliminary version of this paper was published as Saraswat et al . (1995). Here we present a complete treatment of hiding, along with a detailed treatment of the model. We extend the model of [SJG94b] to express strong time-outs (and pre-emption): if an event A does not happen through time t, cause event B to happen at time t. Such constructs arise naturally in practice (e.g. in modeling transistors) and are supported in languages such as ESTEREL (through instantaneous watchdogs) and LUSTRE (through the \u201ccurrent\u201d operator).The fundamental conceptual difficulty posed by these operators is that they are non-monotonic. We provide a simple compositional semantics to the non-monotonic version of concurrent constraint programming (CCP) obtained by changing the underlying logic from intuitionist logic to Reiter's default logic. This allows us to use the same construction (uniform extension through time) to develop Default Timed CCP (Default tcc) as we had used to develop Timed CCP (tcc) from CCP. Indeed the smooth embedding of CCP processes into Default cc processes lifts to a smooth embedding of tcc processes into Default tcc processes. Interesting tcc properties such as determinacy, multiform time, a uniform pre-emption construct (\u201cclock\u201d), full-abstraction, and compositional compilation into automata are preserved.Default tcc thus provides a simple and natural (denotational) model capable of representing the full range of pre-emption constructs supported in ESTEREL, LUSTRE and other synchronous programming languages. This paper focuses on policy languages for (role-based) access control [14, 32], especially in their modern incarnations in the form of trust-management systems [9] and usage control [30, 31]. Any (declarative) approach to access control and trust management has to address the following issues: Explicit denial, inheritance, and overriding, and History-sensitive access control.Our main contribution is a policy algebra, in the timed concurrent constraint programming paradigm, that uses a form of default constraint programming to address the first issue, and reactive computing to address the second issue.The policy algebra is declarative --- programs can be viewed as imposing temporal constraints on the evolution of the system --- and supports equational reasoning. The validity of equations is established by coinductive proofs based on an operational semantics.The design of the policy algebra supports reasoning about policies by a systematic combination of constraint reasoning and model checking techniques based on linear time temporal-logic. Our framework permits us to perform security analysis with dynamic state-dependent restrictions."
  },
  {
    "id": "411587",
    "input": "Generate a title for the following abstract of a paper: This paper describes OpenTracker, an open software architecture that provides a generic solution to the different tasks involved in tracking input devices and processing tracking data for virtual environments. It combines a highly modular design with a configuration syntax based on XML, thus full advantage of this new technology. OpenTracker is a first attempt towards a \"write once, track anywhere\" approach to virtual reality application development. This article describes OpenTracker, an open software architecture that provides a framework for the different tasks involved in tracking input devices and processing multi-modal input data in virtual environments and augmented reality application. The OpenTracker framework eases the development and maintenance of hardware setups in a more flexible manner than what is typically offered by virtual reality development packages. This goal is achieved by using an object-oriented design based on XML, taking full advantage of this new technology by allowing to use standard XML tools for development, configuration and documentation. The OpenTracker engine is based on a data flow concept for multi-modal events. A multi-threaded execution model takes care of tunable performance. Transparent network access allows easy development of decoupled simulation models. Finally, the application developer's interface features both a time-based and an event based model, that can be used simultaneously, to serve a large range of applications. OpenTracker is a first attempt towards a \"'write once, input anywhere\"' approach to virtual reality application development. To support these claims, integration into an existing augmented reality system is demonstrated. We also show how a prototype tracking equipment for mobile augmented reality can be assembled from consumer input devices with the aid of OpenTracker. Once development is sufficiently mature, it is planned to make Open-Tracker available to the public under an open source software license. Tracking is an indispensable part of any virtual reality and augmented reality application. While the need for quality of tracking, in particular for high performance and fidelity, has led to a large body of past and current research, little attention is typically paid to software engineering aspects of tracking software. To address this issue we describe a software design and implementation that applies the pipes-and-filter architectural pattern to provide a customizable and flexible way of dealing with tracking data and configurations. The contribution of this work cumulates in the development of a generic data flow network library called OpenTracker to deal specifically with tracking data. The flexibility of the data flow network approach is demonstrated in a set of development scenarios and prototype applications in the area of mobile augmented reality. In this paper we present a natural feature tracking algorithm based on on-line boosting used for localizing a mobile computer. Mobile augmented reality requires highly accurate and fast six degrees of freedom tracking in order to provide registered graphical overlays to a mobile user. With advances in mobile computer hardware, vision-based tracking approaches have the potential to provide efficient solutions that are non-invasive in contrast to the currently dominating marker-based approaches. We propose to use a tracking approach which can use in an unknown environment, i.e. the target has not be known beforehand. The core of the tracker is an on-line learning algorithm, which updates the tracker as new data becomes available. This is suitable in many mobile augmented reality applications. We demonstrate the applicability of our approach on tasks where the target objects are not known beforehand, i.e. interactive planing."
  },
  {
    "id": "411592",
    "input": "Generate a title for the following abstract of a paper: The paper presents a new unique feature of the IN-METAFrame Service Definition Environment: the automatic generation of diagnostic\n location information as a consequence of detecting an error in the design phase of a Service Logic. Violations of constraints\n which express frame conditions for the design (concerning e.g. implementability, country specific standards, and network specific\n features) are detected by formal verification techniques. The subsequent error diagnosis and correction is now supported by\n a new kind of abstract views, which not only give hints on the possible source of trouble, tut additionally automatically locate the exact occurrence of the constraint violation in the Service Logic.\n  In today's business the availability of services is of central importance. To guarantee a higher availability of a service, like a Web-service, it can be installed on several machines, which are running in a hot-standby operation. In case of a fault one hot-standby service can take over the work of the faulty service. Novel to our approach is that this kind of redundancy can be applied to services, that normally do not support service availability concepts. The switching of one machine running in hot-standby mode to active can be done from an external system monitoring the service. MaTRICS is an architecture that allows the configuration of any service provided by a specific server. The specification is done by service logic graphs, which can be validated by model checking. In this paper we present the FormulaBuilder, a flexible tool for graph-based modelling and generation of formulae. The FormulaBuilder allows easy and intuitive creation of formulae by using basic components called Formula Building Blocks (FBBs) and arranging them as graphs according to the syntactic structure of a formula. Such a graph can then be validated and used to generate the corresponding formula on the basis of a specific syntax which is chosen from a list of syntaxes supported by the FormulaBuilder.An important application of the FormulaBuilder is the formal specification of properties that describe the requirements of a system. Such property specifications are usually needed by verification tools like model checkers, that help software engineers to detect errors in a specified system. The FormulaBuilder allows users to model property specifications as formula graphs by using commonly-occurring specification patterns. This paper presents a formal requirements engineering method capturing specification, synthesis, and verification. Being multi-paradigm, our approach integrates individual established formal methods: temporal logics are used to express abstract specifications in the form of loose global constraints, like ordering requirements, or abstract safety and liveness properties, whereas Statecharts are used to support the development of a detailed, hierarchical specification at the concrete level. The link between, these two specification layers is established by means of 1) a semi-automatic synthesis procedure, where sequential portions of Statecharts, automatically synthesized from abstract specifications, can be manually composed into structured Statecharts, and 2) by automatic formal verification via model checking, which validates the global constraints for the resulting overall Statechart specification. The method is illustrated along a detailed user session."
  },
  {
    "id": "41406",
    "input": "Generate a title for the following abstract of a paper: This paper presents new techniques for slant and slope removal in cursive handwritten words. Both methods require neither heuristics nor parameter tuning. This avoids the heavy experimental effort required to find the optimal configuration of a parameter set. A comparison between the new deslanting technique and the method proposed by Bozinovic and Srihari was made by measuring the performance of both methods within a word recognition system tested on different databases. The proposed technique is shown to improve the recognition rate by 10.8% relative to traditional normalization methods. Moreover, a long exploration of the parameter space is avoided. This paper presents a system for the offline recognition of large vocabulary unconstrained handwritten texts. The only assumption made about the data is that it is written in English. This allows the application of Statistical Language Models in order to improve the performance of our system. Several experiments have been performed using both single and multiple writer data. Lexica of variable size (from 10,000 to 50,000 words) have been used. The use of language models is shown to improve the accuracy of the system (when the lexicon contains 50,000 words, the error rate is reduced by approximately 50 percent for single writer data and by approximately 25 percent for multiple writer data). Our approach is described in detail and compared with other methods presented in the literature to deal with the same problem. An experimental setup to correctly deal with unconstrained text recognition is proposed. This paper presents a system for the offline recognitionof cursive handwritten lines of text. The system is based oncontinuous density HMMs and Statistical Language Models.The system recognizes data produced by a single writer.No a-priori knowledge is used about the content of the textto be recognized. Changes in the experimental setup withrespect to the recognition of single words are highlighted.The results show a recognition rate of ~85% with a lexiconcontaining 50'000 words. The experiments were performedover a publicly available database. This work presents an Offline Cursive Word Recognition System dealing with single writer samples. The system is based on a continuous density Hiddden Markov Model trained using either the raw data, or data transformed using Principal Component Analysis or Independent Component Analysis. Both techniques significantly improved the recognition rate of the system.Preprocessing, normalization and feature extraction are described as well as the training technique adopted. Several experiments were performed using a publicly available database. The accuracy obtained is the highest presented in the literature over the same data."
  },
  {
    "id": "411627",
    "input": "Generate a title for the following abstract of a paper: A considerable amount of our activities on the Web involves revisits to pages or sites. Reasons for revisiting include active monitoring of content, verification of information, regular use of online services, and reoccurring tasks. Browsers support for revisitation is mainly focused on frequently and recently visited pages. In this paper we present a dynamic browser toolbar that provides recommendations beyond these usual suspects, balancing diversity and relevance. The recommendation method used is a combination of ranking and propagation methods. Experimental outcomes show that this algorithm performs significantly better than the baseline method. Further experiments address the question whether it is more appropriate to recommend specific pages or rather (portal pages of) Web sites. We conducted two user studies with a dynamic toolbar that relies on our recommendation algorithm. In this context, the outcomes confirm that users appreciate and use the contextual recommendations provided by the toolbar. Web browsers provide only little support for users to revisit pages that they do not visit very often. We developed a browser toolbar that reminds users of visited pages related to the page that they currently viewing. The recommendation method combines ranking with propagation methods. A user evaluation shows that on average 22.7% of the revisits were triggered by the toolbar, a considerable change on the participants' revisitation routines. In this paper we discuss the value of the recommendations and the implications derived from the evaluation. In this article we update and extend on earlier long-term studies on user's page revisit behavior. Revisits are very common in web navigation, but not as predominant as reported in earlier studies. Backtracking is the most common type of page revisitation and is both used for finding new in- formation and relocating information visited be- fore. Search engines are mainly used for finding new information and users frequently backtrack to result pages. Visits to pages already visited in earlier sessions tend to occur in chunks, but it is not straightforward to create a list of most likely pages that will be revisited. We conclude with a short discussion on design implications for user- adaptive revisitation support. most recent study, carried out by Cockburn and McKenzie, were based on data from late 1999 and might need to be updated to better reflect current usage patterns. Also, the studies did not separate within-session page revisits from cross-session page revisits. As will be explained in more detail in the next section, it is useful to discern these two activities, both for the design and evaluation of novel or adaptive revisitation support. In this article we update and elaborate on findings from the earlier studies, based on a large amount of web usage data collected from 25 users in 2004 and 2005. In the next section we separate various motivations for revisiting pag es and explain how current browsers support these types of re- visits. We continue with a brief summary of the process of collecting and preparing the data. In section four we deal with the question whether page revisits really account for 81% of all navigation actions - as reported by (Cockburn and McKenzie, 2001) - is likely to be an overestimation. In section five we explore the well-known power laws of favored pages and recency of page revisits; more specifi- cally, we separate within-session revisits and cross-sess ion revisits and characterize the frequencies with which they occur. In section six we briefly describe the relation be- tween search activities and page revisits. In the second-la st section we attempt to exploit these characteristics to cre- ate lists of pages that are likely to be revisited. In the last section we discuss the results and design implications for more user-centric revisitation support. More than 45 % of the pages that we visit on the Web are pages that we have visited before. Browsers support revisits with various tools, including bookmarks, history views and URL auto-completion. However, these tools only support revisits to a small number of frequently and recently visited pages. Several browser plugins and extensions have been proposed to better support the long tail of less frequently visited pages, using recommendation and prediction techniques. In this article, we present a systematic overview of revisitation prediction techniques, distinguishing them into two main types and several subtypes. We also explain how the individual prediction techniques can be combined into comprehensive revisitation workflows that achieve higher accuracy. We investigate the performance of the most important workflows and provide a statistical analysis of the factors that affect their predictive accuracy. Further, we provide an upper bound for the accuracy of revisitation prediction using an \u2018oracle\u2019 that discards non-revisited pages."
  },
  {
    "id": "412099",
    "input": "Generate a title for the following abstract of a paper: We have developed a performance debugger for a parallel logic program to improve inherent parallelism of a program. This debugger helps programmers to find unexpected sequentiality visualizing execution history in a virtual environment. We demonstrated an example of performance debugging, and showed how the debugger helps to improve performance of a program on a parallel machine PIE64. A fine-grained highly parallel program has many threads of execution. The first task to debug it is comprehending the situation of the execution. For this task, it is important to visualize the execution. Our debugger HyperDEBU for a parallel logic programming language Fleng visualizes control/data flows of execution of a Fleng program according to a user's intention. Breakpoints are introduced as information which represents a user's intention or points of view. HyperDEBU uses this information to visualize execution of a program. HyperDEBU enables efficient debugging by its visual examining/manipulating facilities. In this paper, a multiwindow debugger HyperDEBU for finegrained highly parallel programs is presented. The target language\n of HyperDEBU is Fleng which is one of the committed choice languages. This debugger supports many kinds and levels of views\n of programs, and helps user to locate bugs efficiently.\n  For the programming environment of a parallel logic programming language, it is an important problem to develop a debugger. However it is difficult to debug parallel programs by observing their execution traces. Using the characteristics of logic programming languages, one solution to this problem is debugging using declarative semantics programs. However parallel logic programming languages (Concurrent Prolog, GHC, etc.) are not pure logic programming languages because of their new primitives for synchronization. We must consider the causality relation between input and output because some operational meaning must be added to the declarative semantics for parallel logic programs. Hence, in this paper, we introduce a communicating process model to represent execution of parallel logic programs. And we present extended algorithmic debugging using this model."
  },
  {
    "id": "41964",
    "input": "Generate a title for the following abstract of a paper: An electronic cash (e-cash) scheme lets a user withdraw money from a bank and then spend it anonymously. E-cash can be used only if it can be securely and fairly exchanged for electronic goods or services. In this paper, we introduce and realize endorsed e-cash. An endorsed e-coin consists of a lightweight endorsement x and the rest of the coin which is meaningless without x. We reduce the problem of exchanging e-cash to that of exchanging endorsements. We demonstrate the usefulness of endorsed e-cash by exhibiting simple and efficient solutions to two important problems: (1) optimistic and unlinkable fair exchange of e-cash for digital goods and services; and (2) onion routing with incentives and accountability for the routers. Finally, we show how to represent a set of n endorsements using just one endorsement; this means that the complexity of the fair exchange protocol for n coins is the same as for one coin, making e-cash all the more scalable and suitable for applications. Our fair exchange of multiple e-coins protocol can be applied to fair exchanges of (almost) any secrets. In an electronic cash (e-cash) system, a user can withdraw coins from the bank, and then spend each coin anonymously and unlinkably. For some applications, it is desirable to set a limit on the dollar amounts of anonymous transactions. For example, governments require that large transactions be reported for tax purposes. In this work, we present the first e-cash system that makes this possible without a trusted party. In our system, a user's anonymity is guaranteed so long as she does not: (1) double-spend a coin, or (2) exceed the publicly-known spending limit with any merchant. The spending limit may vary with the merchant. Violation of either condition can be detected, and can (optionally) lead to identification of the user and discovery of her other activities. While it is possible to balance accountability and privacy this way using e-cash, this is impossible to do using regular cash. Our scheme is based on our recent compact e-cash system. It is secure under the same complexity assumptions in the random-oracle model. We inherit its efficiency: 2\u2113 coins can be stored in O(\u2113+k) bits and the complexity of the withdrawal and spend protocols is O(\u2113+k), where k is the security parameter. Fairly exchanging digital content is an everyday problem. It has been shown that fair ex- change cannot be done without a trusted third party (called the Arbiter). Yet, even with a trusted party, it is still non-trivial to come up with an efficient solution, especially one that can be used in a p2p file sharing system with a high volume of data exchanged. We provide an efficient optimistic fair exchange mechanism for bartering digital files, where receiving a payment in return to a file (buying) is also considered fair. The exchange is op- timistic, removing the need for the Arbiter's involvement unless a dispute occurs. While the previous solutions employ costly cryptographic primitives for every file or block exchanged, our protocol employs them only once per peer, therefore achieving O(n) efficiency improvement when n blocks are exchanged between two peers. The rest of our protocol uses very efficient cryptography, making it perfectly suitable for a p2p file sharing system where tens of peers exchange thousands of blocks and they do not know beforehand which ones they will end up exchanging. Therefore, our system yields to one-two orders of magnitude improvement in terms of both computation and communication (80 seconds vs. 84 minutes, 1.6MB vs. 100MB). Thus, for the first time, a provably secure (and privacy respecting when payments are made using e-cash) fair exchange protocol is being used in real bartering applications (e.g., BitTorrent) (14) without sacrificing performance. Fair exchange is one of the most fundamental problems in secure distributed computation. Alice has something that Bob wants, and Bob has something that Alice wants. A fair exchange protocol would guarantee that, even if one of them maliciously deviates from the protocol, either both of them get the desired content, or neither of them do. It is known that no two-party protocol can guarantee fairness in general; therefore the presence of a trusted arbiter is necessary. In optimistic fair exchange, the arbiter only gets involved in case of faults, but needs to be trusted. To reduce the trust put in the arbiter, it is natural to consider employing multiple arbiters. Expensive techniques like byzantine agreement or secure multi-party computation with \u03a9(n2) communication can be applied to distribute arbiters in a non-autonomous way. Efficient protocols can be achieved by keeping the arbiters autonomous (non-communicating). Avoine and Vaudenay [5] employ multiple autonomous arbiters in their optimistic fair exchange protocol which uses global timeout mechanisms; all arbiters have access to loosely synchronized clocks. They left two open questions regarding the use of distributed autonomous arbiters: (1) Can an optimistic fair exchange protocol without timeouts provide fairness when employing multiple autonomous arbiters? (2) Can any other optimistic fair exchange protocol with timeouts achieve better bounds on the number of honest arbiters required? In this paper, we answer both questions negatively. To answer these questions, we define a general class of optimistic fair exchange protocols with multiple arbiters, called \"distributed arbiter fair exchange\" (DAFE) protocols. Informally, in a DAFE protocol, if a participant fails to send a correctly formed message, the other party must contact some subset of the arbiters and get correctly formed responses from them. The arbiters do not communicate with each other, but only to Alice and Bob. We prove that no DAFE protocol can meaningfully exist."
  },
  {
    "id": "41879",
    "input": "Generate a title for the following abstract of a paper: Wikipedia can be considered as an ex- treme form of a self-managing team, as a means of labour division. One could expect that this bottom-up approach, with the absense of top-down organisational control, would lead to a chaos, but our analysis shows that this is not the case. In the Dutch Wikipedia, an in- tegrated and coherent data structure is created, while at the same time users succeed in distributing roles by self- selection. Some usersfocus on an area of expertise, while others edit over the whole encyclopedic range. This con- stitutes our conclusion that Wikipedia, in general, is a successful example of a self-managing team. Dao is an attractive game to play, although it is solvable in a few seconds on a computer. The game is so small that the complete game graph can be kept in internal memory. At the same time, the number of nodes in the game graph of Dao is large enough to allow interesting analyses. In the game spectrum, Dao resides between on the one hand trivial games such as Tic-Tac-Toe and Do-Guti and on the other hand games, such as Connect-Four and Awari that are solved but of which the game graph cannot be kept in memory. In this paper we provide many detailed properties of Dao and its solution. Our conclusion is that a game like Dao can be used as a benchmark of search enhancements. As an illustration we provide an example concerning the size of transposition tables in \u03b1-\u03b2 search. In the law, it is generally acknowledged that there are intuitive differences between reasoning with rules and reasoning with principles. For instance, a rule seems to lead directly to its conclusion if its condition is satisfied, while a principle seems to lead merely to a reason for its conclusion. However, the implications of these intuitive differences for the logical status of rules and principles remain controversial.A radical opinion has been put forward by Dworkin (1978). The intuitive differences led him to argue for a strict logical distinction between rules and principles. Ever since, there has been a controversy whether the intuitive differences between rules and principles require a strict logical distinction between the two. For instance, Soeteman (1991) disagrees with Dworkin's opinion, and argues that rules and principles cannot be strictly distinguished, and do not have a different logical structure.In this paper, we claim that the differences between rules and principles are merely a matter of degree. We give an integrated view on rules and principles in which rules and principles have the same logical structure, but different behavior in reasoning. In this view, both rules and principles are considered to consist of a condition and a conclusion. The observed differences between rules and principles are, in our view, the result of different types of relations that they have with other rules and principles. In the integrated view, typical rules and typical principles are the extremes of a spectrum.We support our claim by giving an explicit formalization of our integrated view using the recently developed formal tools provided by Reason-Based Logic. As an application of our view on rules and principles, we give three ways of reconstructing reasoning by analogy. The retrieval of patient-related literature is hampered by the large size of medical literature. Various computer systems have been developed to assist physicians during information retrieval. However, in general, physicians lack the time and skills required to employ such systems effectively. Our goal is to investigate to what extent a physician can be provided with patient-related literature without spending extra time and without acquiring additional skills. In previous research we developed a method to formulate a physician's patient-related information needs automatically, without requiring any interaction between the physician and the system. The formulated information needs can be used as a starting point for literature retrieval. As a result we found that the number of information needs formulated per physician was quite high and had to be reduced to avoid a literature overload. In this paper we present four types of knowledge that may be used to accomplish a reduction in the number of information needs. The usefulness of each of these knowledge types depends heavily on the specific cause underlying the multitude of information needs. To determine the nature of the cause, we performed an experimental analysis. The results of the analysis led us to conclude that the knowledge types can be ordered according to their appropriateness as follows: (1) knowledge concerning temporal aspects, (2) knowledge concerning a physician's specialism, (3) domain knowledge, and (4) a user model. Further research has to be performed, in particular on precisely assessing the performance of each type of knowledge within our domain."
  },
  {
    "id": "41287",
    "input": "Generate a title for the following abstract of a paper: Sensor nodes in a network consume energy in a non-uniform manner. Designing clustering protocols that are heterogeneity aware is still an open issue in sensor networks. This work focuses on hierarchically clustered heterogeneous sensor networks. Sensor nodes organize themselves in self organized groups called clusters. Each cluster consists of a cluster head and some member nodes. The sensor network consists of nodes of two different energy levels. We show that current heterogeneity aware protocols (such as SEP [1], DEEC [2] etc.) are unable to distribute the usage of energy amongst the sensor nodes uniformly. We propose CRP, Cluster Re-election Protocol, that is heterogeneity aware and enhances the network stability period (period before which the first node dies) over current protocols. We show, by simulation, that CRP improves network stability by reducing the variation in the residual energy of sensor nodes. Therefore. CRP is able to exploit the heterogeneity present in a sensor network in an efficient manner. Wireless sensor networks aim at a special kind of ad hoc networks, exposing an energy-constrained distributed computing environment. Proposed protocols have tried to incorporate power management schemas of the likes of reduced duty cycles and active synchronization. However this has led to a significant loss in the latency of data delivery. In this work we have introduced low latency medium access control (LLM), as a mechanism to deliver data with low latency without compromising the energy efficiency of the network. To this end we exploit the data aggregating properties of a sensor network and introduce a pre-notification packet to keep potential forwarding nodes aware of a forthcoming data packet. Our simulations show that LLM does perform as per expectations Recent research in wireless sensor networks have shown in most of the WSN applications node positions are often known in priori, to be able to effectively assimilate data from the WSN deployment. Also expected lifetime of the network, ie. for how long the deployment should work, is often an important specification for a particular deployment. In this paper we proposed two novel protocols, we call, Location and expected Lifetime Biased Clustering (LeLBC) and a modification of it, with fully localized intra cluster chaining (LeLBC-ICC). Both the protocols utilize the location information and network lifetime requirement as the knowledge for scheduling cluster head selection expeditiously. Experiment results have shown that LeLBC outperforms widely quoted non deterministic cluster based protocol LEACH, while LeLBC-ICC gives comparable results with the near optimal solution PEGASIS. Both the protocols use only localized information and maximum numbers of nodes remain alive during entire lifetime of the network. The lifetime of sensor network depends on the efficient utilization of resource-constrained sensor nodes. Several MAC protocols like DMAC and its variants have been proposed to save critical sensor resources through sleep-wakeup scheduling over data gathering tree. For applications where data aggregation is not possible, the sleep duration decreases gradually from the leaves to the root of the data gathering tree. This results early failure of sensor nodes near the sink, and affects network connectivity and coverage. Deploying redundant sensors can solve this problem where a faulty node is replaced by a redundant node to maintain network connectivity and coverage. However, the amount of redundancy depends on the node failure pattern, and thus more number of redundant nodes required to be deployed near the sink. This paper proposes a gradient based sensor deployment scheme for energy-efficient data gathering exploring the trade-off among connectivity, coverage, fault-tolerance and redundancy. The density of deployment is estimated based on the distance of a node from the sink while dealing with connectivity, coverage and fault-tolerance. The effectiveness of the proposed scheme has been analyzed both theoretically and with the help of simulation."
  },
  {
    "id": "41553",
    "input": "Generate a title for the following abstract of a paper: We present a generalization of thin-plate splines for interpolation and approximation of manifold-valued data, and demonstrate its usefulness in computer graphics with several applications from different fields. The cornerstone of our theoretical framework is an energy functional for mappings between two Riemannian manifolds which is independent of parametrization and respects the geometry of both manifolds. If the manifolds are Euclidean, the energy functional reduces to the classical thin-plate spline energy. We show how the resulting optimization problems can be solved efficiently in many cases. Our example applications range from orientation interpolation and motion planning in animation over geometric modelling tasks to color interpolation. We study nonparametric regression between Riemannian manifolds based on regularized empirical risk minimization. Regularization functionals for mappings between manifolds should respect the geometry of input and output manifold and be independent of the chosen parametrization of the manifolds. We define and analyze the three most simple regularization functionals with these properties and present a rather general scheme for solving the resulting optimization problem. As application examples we discuss interpolation on the sphere, fingerprint processing, and correspondence computations between three-dimensional surfaces. We conclude with characterizing interesting and sometimes counterintuitive implications and new open problems that are specific to learning between Riemannian manifolds and are not encountered in multivariate regression in Euclidean space. Improvements in our ability to process large amounts of data have led to progress in many areas of science, not least artificial intelligence (AI). With advances in machine learning has come the development of machines that can learn intelligent behaviour directly from data, rather than being explicitly programmed to exhibit such behaviour. For instance, the advent of 'big data' has resulted in systems that can recognize objects or sounds with considerable precision. On page 529 of this issue, Mnih et al.1 describe an agent that uses large data sets to teach itself how to play 49 classic Atari 2600 computer games by looking at the pixels and learning actions that increase the game score. It beat a professional games player in many instances \u2014 a remarkable example of the progress being made in AI. In machine learning, systems are trained to infer patterns from observational data. A particularly simple type of pattern, a mapping between input and output, can be learnt through a process called supervised learning. A supervised-learning system is given training data consisting of example inputs and the corresponding outputs, and comes up with a model to explain those data (a process called function approximation). It does this by choosing from a class of model specified by the system's designer. Designing this class is an art: its size and complexity should reflect the amount of training data available, and its content should reflect 'prior knowledge' that the designer of the system considers useful for the problem at hand. If all this is done well, the inferred model will then apply not only for the training set, but also for other data that adhere to the same underlying pattern. The rapid growth of data sets means that machine learning can now use complex model classes and tackle highly non-trivial inference problems. Such problems are usually characterized by several factors: the data are multidimensional; the underlying pattern is complex (for instance, it might be nonlinear or changeable); and the designer has only weak prior knowledge about the problem \u2014 in particular, a mechanistic understanding is lacking. The human brain repeatedly solves non-trivial inference problems as we go about our daily lives, interpreting high-dimensional sensory data to determine how best to control all the muscles of the body. Simple supervised learning is clearly not the whole story, because we often learn without a 'supervisor' telling us the outputs of a hypothetical input\u2013output function. Here, 'reinforcement' has a central role in learning behaviours from weaker supervision. Machine learning adopted this idea to develop reinforcement-learning algorithms, in which supervision takes the form of a numerical reward signal2, and the goal is for the system to learn a policy that, given the current state, determines which action to pick to maximize an accumulated future reward. Mnih et al. use a form of reinforcement learning known as Q-learning3 to teach systems to play a set of 49 vintage video games, learning how to increase the game score as a numerical reward. In Q-learning, Q*(s,a) represents the accumulated future reward, Q*, if in state s the system first performs action a, and subsequently follows an optimal policy. The system tries to approximate Q* by using an artificial neural network \u2014 a function approximator loosely inspired by biological neural networks \u2014 called a deep Q-network (DQN). The DQN's input (the pixels from four consecutive game screens) is processed by connected 'hidden' layers of computations, which extract more and more specialized visual features to help approximate the complex nonlinear mapping between inputs and the value of possible actions \u2014 for instance, the value of a move in each possible direction when playing Space Invaders (Fig. 1). The system picks output actions on the basis of its current estimate of Q*, thereby exploiting its knowledge of a game's reward structure, and intersperses the predicted best action with random actions to explore uncharted territory. The game then responds with the next game screen and a reward signal equal to the change in the game score. Periodically, the network uses inputs and rewards to update the DQN parameters, attempting to move closer to Q*. Much thought went into how exactly to do this, given that the agent collects its own training data over time. As such, the data are not independent from a statistical point of view, implying that most of statistical theory does not apply. The authors store past experiences in the system's memory and subsequently re-train on them \u2014 a procedure they liken to hippocampal processes during sleep. They also report that the system benefits from randomly permuting these experiences. There are several interesting aspects of Mnih and colleagues' paper. First, the system performances are comparable to those of a human games tester. Second, the approach displays impressive adaptability. Although each system was trained using data from one game, the prior knowledge that went into the system design was essentially the same for all 49 games; the systems essentially differed only in the data they had been trained on. Finally, the main methods used have been around for several decades, making Mnih and colleagues' engineering feat all the more commendable. What is responsible for the impressive performance of Mnih and colleagues' system, also reported for another DQN4? It may be largely down to improved function approximation using deep networks. Even though the size of the game screens produced by the emulator is reduced by the system to 84 \u00d7 84 pixels, the problem's dimensionality is much higher than that of most previous applications of reinforcement learning. Also, Q* is highly nonlinear, which calls for a rich nonlinear function class to be used as an approximator. This type of approximation can be accurately made only using huge data sets (which the game emulator can produce), state-of-the-art function learning and considerable computing power. Some fundamental issues remain open, however. Can we mathematically understand reinforcement learning from dependent data, and develop algorithms that provably work? Is it sufficient to learn statistical associations, or do we need to take into account the underlying causal structure, describing, say, which pixels causally influence others? This may help in finding relevant parts of the state space (for example, identifying which sets of pixels form a relevant entity, such as an alien in Space Invaders); in avoiding 'superstitious' behaviour, in which statistical associations may be misinterpreted as causal; and in making systems more robust with respect to data-set shifts, such as changes in the behaviours or visual appearance of game characters3, 5, 6. And how should we handle latent learning \u2014 the fact that biological systems also learn when no rewards are present? Could this help us to handle cases in which the dimensionality is even higher and the key quantities are hidden in a sea of irrelevant information? In the early days of AI, beating a professional chess player was held by some to be the gold standard. This has now been achieved, and the target has shifted as we have grown to understand that other problems are much harder for computers, in particular problems involving high dimensionalities and noisy inputs. These are real-world problems, at which biological perception\u2013action systems excel and machine learning outperforms conventional engineering methods. Mnih and colleagues may have chosen the right tools for this job, and a set of video games may be a better model of the real world than chess, at least as far as AI is concerned. Download references Density estimation is a fundamental problem in statistical learning. This problem is especially challenging for complex high-dimensional data due to the curse of dimensionality. A promising solution to this problem is given here in an inference-free hierarchical framework that is built on score matching. We revisit the Bayesian interpretation of the score function and the Parzen score matching, and construct a multilayer perceptron with a scalable objective for learning the energy (i.e. the unnormalized log-density), which is then optimized with stochastic gradient descent. In addition, the resulting deep energy estimator network (DEEN) is designed as products of experts. We present the utility of DEEN in learning the energy, the score function, and in single-step denoising experiments for synthetic and high-dimensional data. We also diagnose stability problems in the direct estimation of the score function that had been observed for denoising autoencoders."
  },
  {
    "id": "41452",
    "input": "Generate a title for the following abstract of a paper: Dynamic textured sequences are characterized by the interactions be- tween many particles or objects in the scene. Based on earlier work the im- ages of the sequence are interpreted as the output of a linear autoregressive process driven by white Gaussian noise. We extend earlier work by increas- ing the amount temporal information included when learning the motion in the scene, allowing the models to capture complex motion patterns which ex- tend over multiple frames, thereby increasing the perceptual accuracy of the synthesized results. To overcome problems of dynamic model stability, we apply Burg's Maximum Entropy Spectral Analysis technique f or parameter estimation, which is found to be reliably stable on smaller samples of training data, even with higher-order dynamics. Learned, activity-specific motion models are useful for human pose and motion estimation. Nevertheless, while the use of activity-specific models simplifies monocular tracking, it leaves open the larger issues of how one learns models for multiple activities or stylistic variations, and how such models can be combined with natural transitions between activities. This paper extends the Gaussian process latent variable model (GP-LVM) to address some of these issues. We introduce a new approach to constraining the latent space that we refer to as the locally-linear Gaussian process latent variable model (LL-GPLVM). The LL-GPLVM allows for an explicit prior over the latent configurations that aims to preserve local topological structure in the training data. We reduce the computational complexity of the GPLVM by adapting sparse Gaussian process regression methods to the GP-LVM. By incorporating sparsification, dynamics and back-constraints within the LL-GPLVM we develop a general framework for learning smooth latent models of different activities within a shared latent space, allowing the learning of specific topologies and transitions between different activities. For model-based 3D human pose estimation, even simple models of the human body lead to high-dimensional state spaces. Where the class of activity is known a priori, low- dimensional activity models learned from training data make possible a thorough and efficient search for the best pose. Conversely, searching for solutions in the full state space places no restriction on the class of motion to be recovered, but is both difficult and expensive. This paper explores a potential middle ground between these approaches, using the hierarchical Gaussian process latent variable model to learn activity at differ- ent hierarchical scales within the human skeleton. We show that by training on full-body activity data then descending through the hierarchy in stages and exploring subtrees inde- pendently of one another, novel poses may be recovered. Experimental results on motion capture data and monocular video sequences demonstrate the utility of the approach, and comparisons are drawn with existing low-dimensional activity models. We introduce Gaussian process dynamical models (GPDM) for nonlinear time series analysis, with applications to learning models of human pose and motion from high-dimensionalmotion capture data. A GPDM is a latent variable model. It comprises a low-dimensional latent space with associated dynamics, and a map from the latent space to an observation space. We marginalize out the model parameters in closed-form, using Gaussian process priors for both the dynamics and the observation mappings. This results in a non-parametric model for dynamical systems that accounts for uncertainty in the model. We demonstrate the approach, and compare four learning algorithms on human motion capture data in which each pose is 50-dimensional. Despite the use of small data sets, the GPDM learns an effective representation of the nonlinear dynamics in these spaces."
  },
  {
    "id": "411831",
    "input": "Generate a title for the following abstract of a paper: Abstract: In the current literature on service-oriented computing, the relation- ship between services and web-services is not always clear .M uch research, notably in the area of service representation, discovery and composition, claims to address services whereas the ya ctually apply to web-services. In this paper , we use insights from Language/Action Perspective and from value modeling,to define services at an abstract (business) level. On that basis, we explore ap rag- matic approach to service discovery ,t he cornerstone of the Service Oriented Architecture, and sho wh ow it differs from web service discovery .I nt he course of the discussion, some differences between aS emantic We ba pproach and a Pragmatic We ba pproach become,apparant. 1I ntroduction Service oriented architectures are becoming increasingly important as enablers of exchange, coordination. and cooperation between organizations and individuals. Engineering and management of services raise a number of issues concerning the analysis, design, integration, bundling, and maintenance of services. These issues are notoriously difficult to resolve due to the abstractness of services as compared to other kinds of resources. In this paper, we analyze the concept of service based on a number of definitions from the literature and propose a conceptual service model based on the REA ontology. The model relates the service notion to the resource concept and shows how the abstractions offered by services can be represented using an encapsulation relationship. The use of the proposed service model is illustrated by means of an application oil marketing oriented representation and design of services. Interoperability between enterprise applications requires an understanding of the obstacles to interoperability in order to provide methods for overcoming these obstacles. We address the problem from different angles: first, we investigate the benefit of multi-lingual ontologies to overcome language problems between users of an enterprise application. Second, we propose a method to integrate legacy components into new enterprise applications. Third, we research the impact service-oriented computing to enterprise application integration. Finally, we present our results with agent-oriented platforms for integrating autonomous applications. 1. Ontologies for enterprise integration Matching interfaces by routines for data conversion between applications are not solving the problem of discovering the opportunity for application integration and for addressing semantic mismatches between data and services. One aspect of the semantic mismatch is multi-linguality and heterogeneous data representation. We have developed a scheme for representing data into multi-lingual ontologies that allow to find back information from heterogeneous data sources independently from the original data structures in the sources (1). The method is based on the notion of attribute concepts like size, color, price etc. These concepts are about properties of objects like products or even services. When a new data source (or a new service) is added to the system, the property description of the new item are linked to the abstract attribute concepts in the ontology. A user (or system) requiring to find the item can use the terms in the ontology to locate the item. Since the ontology concepts have surrogate keys, multiple natural language translations can be attached to them. The link between data sources to the ontology is enriched by meta data about the data source supplier. This allows tracing back any part of an answer to a location request back to the supplier of the original information. The precision is to the tuple level and loss-less: the original data sources can be reconstructed from the integrated representation and vice versa. The approach has been successfully employed in the MEMO project (2), which produced a portal for B2B commerce for vertically integrated industries. Specifically, the goal was to integrate product and service catalogs from companies of several countries belonging to the same vertical market. Those companies partially share ontologies on how to describe products and services. However, each role of a company in the vertical supply chain induces a specialized sub-ontology that can differ from country to country due to their cultural and legal heritage. This type of heterogeneity is a great obstacle to application integration. For example, the concept of a fire resistance of a floor cover is has different interpretations in different countries due to their national laws and standards. Our solution addressed this problem by allowing to let multiple ontologies co-exist in the integrating system. The user of the system has associated to her the ontologies that apply to her. Whenever she used the system, only her ontologies are active and she only gets information back that is classified into these ontologies. The approach has been realized using the ConceptBase system (3). ConceptBase is a multi-user repository system based on the Telos knowledge representation language (4). Its key features are \u2022 meta modeling: objects are instances of classes, classes are instances of meta classes, meta classes are instances of meta meta classes etc. The abstraction hierarchy is virtually unlimited allowing to model multiple representation schemes into a single uniform framework With the growing importance of services in the modern enterprise there is a need for innovating traditional management accounting practices. In the Service Science literature, some work has been devoted to service accounting but mainly on a conceptual level. To address this research challenge, we have built an integrated service accounting framework on top of the business ontology REA. The e3value model is a useful tool to give an overall picture of a service network, which can be combined smoothly with the more detailed REA model. The result has been evaluated on the basis of the requirements and with an example from online gaming. As far as we know, it is the first worked out service accounting framework in Service Science and allows giving often-used concepts as value-in-use a precise definition."
  },
  {
    "id": "412010",
    "input": "Generate a title for the following abstract of a paper: Hyperspectral images exhibit significant spectral correlation, whose exploitation is crucial for compression. In this paper, we investigate the problem of predicting a given band of a hyperspectral image using more than one previous band. We present an information-theoretic analysis based on the concept of conditional entropy, which is used to assess the available amount of correlation and the pot... A first attempt to exploit distributed source coding (DSC) principles for the lossless compression of hyperspectral images is presented. The DSC paradigm is exploited to design a very light coder which minimizes the exploitation of the correlation between the image bands. In this way we managed to move the computational complexity from the encoder to the decoder, thus matching the needs of classical acquisition system where compression is achieved on board of the aerial platform and decoding at the ground station. Though the encoder does not explicitly exploit inter-band correlation, the achieved bit rate is about 1 bit/pixel lower than classical 2D schemes such as JPEG-LS or CALID 2D, and only about 1 b/p higher than the best performing, and much more complex, 3D schemes. This paper deals with the application of distributed source coding (DSC) theory to remote sensing image compression. Although DSC exhibits a significant potential in many application fields, up till now the results obtained on real signals fall short of the theoretical bounds, and often impose additional system-level constraints. The objective of this paper is to assess the potential of DSC for lossless image compression carried out onboard a remote platform. We first provide a brief overview of DSC of correlated information sources. We then focus on onboard lossless image compression, and apply DSC techniques in order to reduce the complexity of the onboard encoder, at the expense of the decoder's, by exploiting the correlation of different bands of a hyperspectral dataset. Specifically, we propose two different compression schemes, one based on powerful binary error-correcting codes employed as source codes, and one based on simpler multilevel coset codes. The performance of both schemes is evaluated on a few AVIRIS scenes, and is compared with other state-of-the-art 2D and 3D coders. Both schemes turn out to achieve competitive compression performance, and one of them also has reduced complexity. Based on these results, we highlight the main issues that are still to be solved to further improve the performance of DSC-based remote sensing systems. In this paper we propose an algorithm for near-lossless com- pression of hyperspectral images based on distributed source coding (DSC). The encoding is based on syndrome coding of bit-planes of the quantized prediction error of each band, using the same information in the previous band as side in- formation. The practical scheme employs an array of low- density parity-check codes. Unlike other existing DSC techniques, the determination of the encoding rate for each data block is completely based on a statistical model, avoiding the need of inter-source com- munication, as well as of a feedback channel. Moreover, the statistical model allows to estimate the statistics of the cur- rently decoded bit-plane also using the information about the previously decoded ones in the same band; this boosts the performance of the DSC scheme towards the capacity of the conditional entropy of the multilevel (as opposed to binary) source. Experimental results have been worked out using AVIRIS data; a significant performance improvement is ob- tained with respect to existing DSC and classical techniques, although there is still a gap with respect to the theoretical coding bounds."
  },
  {
    "id": "411995",
    "input": "Generate a title for the following abstract of a paper: Analysing the representation of gender in news media has a long history within the fields of journalism, media and communication. Typically this can be performed by measuring how often people of each gender are mentioned within the textual content of news articles. In this paper, we adopt a different approach, classifying the faces in images of news articles into their respective gender. We present a study on $885{,}573$ news articles gathered from the web, covering a period of four months between 19th October 2014 and 19th January 2015 from $882$ news outlets. Findings show that gender bias differs by topic, with Fashion and the Arts showing the least bias. Comparisons of gender bias by outlet suggest that tabloid-style news outlets may be less gender-biased than broadsheet-style ones, supporting previous results from textual content analysis of news articles. In this article we present an application of text-analysis technologies to support social science research, in particular the analysis of patterns in news content. We describe a system that gathers and annotates large volumes of textual data in order to extract patterns and trends. We have examined 3.5 million news articles and show that their topic is related to the gender bias and readability of their content. This study is intended to illustrate how pattern analysis technology can be deployed to automate tasks commonly performed by humans in the social sciences, in order to enable large scale studies that would otherwise be impossible. We explore the problem of learning to predict the popularity of an article in online news media. By \"popular\" we mean an article that was among the \"most read\" articles of a given day in the news outlet that published it. We show that this cannot be modelled simply as the binary classification task of separating popular from unpopular articles, thereby assuming that popularity is an absolute property. Instead, we propose to view popularity in the perspective of a competitive situation where the popular articles are those which were the most appealing on that particular day. This leads to the notion of an \"appeal\" function, to model which we use a linear function in the bag of words representation. The parameters of this linear function are learnt from a training set formed by pairs of documents, one of which was popular and the other which appeared on the same page and date, without becoming popular. To learn the appeal function we use Ranking Support Vector Machines, using data collected from six different outlets over a period of 1 year. We show that our method can predict which articles will become popular, as well as extracting those keywords that mostly affect the appeal function. This also enables us to compare different outlets from the point of view of their readers' preference patterns. Remarkably, this is achieved using very limited information, namely the textual content of title and description of each article, the page and date of publication, and whether it became popular. We automatically assemble a big dataset to train a face gender classifier.This is formed by 4 million images and over 60,000 features.The resulting system significantly outperforms the previous state of the art without human annotation.This study lends support to the \\\"unreasonable effectiveness of data\\\" conjecture.This study is relevant to computer vision (LBP features, face classification), machine learning (large scale linear classifiers), and big data.This study can serve as a template for other \\\"web scale\\\" learning tasks. The application of learning algorithms to big datasets has been identified for a long time as an effective way to attack important tasks in pattern recognition, but the generation of large annotated datasets has a significant cost. We present a simple and effective method to generate a classifier of face images, by training a linear classification algorithm on a massive dataset entirely assembled and labelled by automated means. In doing so, we perform the largest experiment on face gender recognition so far published, reporting the highest performance yet. Four million images and more than 60,000 features are used to train online classifiers. By using an ensemble of linear classifiers, we achieve an accuracy of 96.86% on the most challenging public database, labelled faces in the wild (LFW), 2.05% higher than the previous best result on the same dataset (Shan, 2012). This result is relevant both for the machine learning community, addressing the role of large datasets, and the computer vision community, providing a way to make high quality face gender classifiers. Furthermore, we propose a general way to generate and exploit massive data without human annotation. Finally, we demonstrate a simple and effective adaptation of the Pegasos that makes it more robust."
  },
  {
    "id": "41890",
    "input": "Generate a title for the following abstract of a paper: We describe an approach for automated analysis of deformable objects which extracts structure information from groups of images containing different ex- amples of the object with a particular application to human imaging. The proposed analysis framework simultaneously segments and registers a set of images, incrementally constructing a model of the composition of the ob- ject. By fitting an appropriate intensity distribution mode l to the image we obtain a soft segmentation which allows us to explicitly model the construc- tion of each pixel from constituent image segments, rather than its expected intensity. This effectively decouples the model from the effects of the imag- ing system and varying statistics in different examples. When estimating the optimal deformation field for each example, the original ima ge is compared to a reconstruction, generated using the composition model and its intensity distribution parameters for each segment (i.e. an estimate of how the model would appear given the imaging conditions for that image). In the paper we describe the algorithm in detail and show results of applying it to two sets of medical images of different anatomies taken with different imaging modalities. We present quantitative results demonstrating that the proposed algorithm is more powerful than current state of the art methods at extract- ing structural information such as spatial correspondences across groups of images with varying statistics. We address the problem of extracting information from groups of medical images of the same anatomy. We describe an algorithm which simultaneously segments and registers a set of such images, incrementally constructing a model of their structure and the correspondences across the set. The framework explicitly models the fraction of each tissue type, rather than the expected intensity in each voxel, to decouple the model from details of the imaging sequence and modality. When estimating the optimal deformation field, the current image is compared to a reconstructed image, generated from the model tissue fractions and the current estimate of intensity distributions for each tissue type in the current image (i.e. an estimate of how the model would appear given the imaging conditions for that image). We describe the algorithm in detail and present results of applying it to a set of MR images of the brain Active Appearance Models (5) are widely used to match statistical models of shape and appearance to new images rapidly. They work by findi ng model pa- rameters which minimise the sum of squares of residual differences between model and target image. Their efficiency is achieved by pre-c omputing the Jacobian describing how the residuals are expected to change as the param- eters vary. This leads to a method of predicting the position of the minima based on a single measurement of the residuals (though in practise the algo- rithm is iterated to refine the estimate). However, the estimate of the Jacobian from the training set will only be an approximation for any given target image, and may be a poor one if the target image is significantly different from the training im ages. This paper describes a simple method of updating a representation of the Jacobian as the search progresses. This allows us to tune the AAM to the current example. Though useful for matching to a single image, it is particularly powerful when tracking objects through sequences, as it gives a method of tuning the AAM as the search progresses. We demonstrate the power of the technique on a variety of datasets. We present an efficient and accurate algorithm for face tracking using a set of Active Appearance Models (AAMs). We observe that a single AAM, trained at a particular model resolution and a particular range of displacements, has a \"sweet spot\" - a range of displacements for which it is most accurate. A common approach to increasing the range of convergence is to use a multi-resolution model, or a sequence of AAMs trained on smaller and smaller displacements. However, during tracking it is inefficient to run the whole sequence at every frame. If there has been little movement since the previous frame, it is sufficient to only run one step of a single higher resolution AAM. In this paper we show that we can use a non-linear regressor to estimate the magnitude of the displacement from the optimal position in the current frame, and use this to select a model which has been tuned to work well at that displacement. This is significantly more efficient than running a complete sequence of models at every frame. We describe the method in detail and demonstrate its performance on several datasets."
  },
  {
    "id": "411616",
    "input": "Generate a title for the following abstract of a paper: Outlier detection is the identification of points in a dataset that do not conform to the norm. Outlier detection is highly sensitive to the choice of the detection algorithm and the feature subspace used by the algorithm. Extracting domain-relevant insights from outliers needs systematic exploration of these choices since diverse outlier sets could lead to complementary insights. This challenge is especially acute in an interactive setting, where the choices must be explored in a time-constrained manner. In this work, we present REMIX, the first system to address the problem of outlier detection in an interactive setting. REMIX uses a novel mixed integer programming (MIP) formulation for automatically selecting and executing a diverse set of outlier detectors within a time limit. This formulation incorporates multiple aspects such as (i) an upper limit on the total execution time of detectors (ii) diversity in the space of algorithms and features, and (iii) meta-learning for evaluating the cost and utility of detectors. REMIX provides two distinct ways for the analyst to consume its results: (i) a partitioning of the detectors explored by REMIX into perspectives through low-rank non-negative matrix factorization; each perspective can be easily visualized as an intuitive heatmap of experiments versus outliers, and (ii) an ensembled set of outliers which combines outlier scores from all detectors. We demonstrate the benefits of REMIX through extensive empirical validation on real-world data. Bike sharing systems, aiming at providing the missing links in public transportation systems, are becoming popular in urban cities. A key to success for a bike sharing systems is the effectiveness of rebalancing operations, that is, the efforts of restoring the number of bikes in each station to its target value by routing vehicles through pick-up and drop-off operations. There are two major issues for this bike rebalancing problem: the determination of station inventory target level and the large scale multiple capacitated vehicle routing optimization with outlier stations. The key challenges include demand prediction accuracy for inventory target level determination, and an effective optimizer for vehicle routing with hundreds of stations. To this end, in this paper, we develop a Meteorology Similarity Weighted K-Nearest-Neighbor (MSWK) regressor to predict the station pick-up demand based on large-scale historic trip records. Based on further analysis on the station network constructed by station-station connections and the trip duration, we propose an inter station bike transition (ISBT) model to predict the station drop-off demand. Then, we provide a mixed integer nonlinear programming (MINLP) formulation of multiple capacitated bike routing problem with the objective of minimizing total travel distance. To solve it, we propose an Adaptive Capacity Constrained K-centers Clustering (AdaCCKC) algorithm to separate outlier stations (the demands of these stations are very large and make the optimization infeasible) and group the rest stations into clusters within which one vehicle is scheduled to redistribute bikes between stations. In this way, the large scale multiple vehicle routing problem is reduced to inner cluster one vehicle routing problem with guaranteed feasible solutions. Finally, the extensive experimental results on the NYC Citi Bike system show the advantages of our approach for bike demand prediction and large-scale bike rebalancing optimization. Multi-task learning aims at learning multiple related but different tasks. In general, there are two ways for multi-task learning. One is to exploit the small set of labeled data from all tasks to learn a shared feature space for knowledge sharing. In this way, the focus is on the labeled training samples while the large amount of unlabeled data is not sufficiently considered. Another way has a focus on how to share model parameters among multiple tasks based on the original features space. Here, the question is whether it is possible to combine the advantages of both approaches and develop a method, which can simultaneously learn a shared subspace for multiple tasks and learn the prediction models in this subspace? To this end, in this paper, we propose a feature representation learning framework, which has the ability in combining the autoencoders, an effective way to learn good representation by using large amount of unlabeled data, and model parameter regularization methods into a unified model for multi-task learning. Specifically, all the tasks share the same encoding and decoding weights to find their latent feature representations, based on which a regularized multi-task softmax regression method is used to find a distinct prediction model for each task. Also, some commonalities are considered in the prediction models according to the relatedness of multiple tasks. There are several advantages of the proposed model: 1) it can make full use of large amount of unlabeled data from all the tasks to learn satisfying representations, 2) the learning of distinct prediction models can benefit from the success of autoencoder, 3) since we incorporate the labeled information into the softmax regression method, so the learning of feature representation is indeed in a semi-supervised manner. Therefore, our model is a semi-supervised autoencoder for multi-task learning (SAML for short). Finally, extensive experiments on three real-world data sets demonstrate the effectiveness of the proposed framework. Moreover, the feature representation obtained in this model can be used by other methods to obtain improved results. Removing objects that are noisy is an important goal of data cleaning as noise hinders most types of data analysis. Most existing data cleaning methods focus on removing noise that is the product of low-level data errors that result from an imperfect data collection process, but data objects that are irrelevant or only weakly relevant can also significantly hinder data analysis. Thus, if the goal is to enhance the data analysis as much as possible, these objects should also be considered as noise, at least with respect to the underlying analysis. Consequently, there is a need for data cleaning techniques that remove both types of noise. Because data sets can contain large amounts of noise, these techniques also need to be able to discard a potentially large fraction of the data. This paper explores four techniques intended for noise removal to enhance data analysis in the presence of high noise levels. Three of these methods are based on traditional outlier detection techniques: distance-based, clustering-based, and an approach based on the local outlier factor (LOF) of an object. The other technique, which is a new method that we are proposing, is a hyperclique-based data cleaner (HCleaner). These techniques are evaluated in terms of their impact on the subsequent data analysis, specifically, clustering and association analysis. Our experimental results show that all of these methods can provide better clustering performance and higher quality association patterns as the amount of noise being removed increases, although HCleaner generally leads to better clustering performance and higher quality associations than the other three methods for binary data."
  },
  {
    "id": "41644",
    "input": "Generate a title for the following abstract of a paper: In this paper, we present a grasp representation in task space exploiting position information of the fingertips. We propose a new way for grasp representation in the task space, which provides a suitable basis for grasp imitation learning. Inspired by neuroscientific findings, finger movement synergies in the task space together with fingertip positions are used to derive a parametric low-dimensional grasp representation. Taking into account correlating finger movements, we describe grasps using a system of virtual springs to connect the fingers, where different grasp types are defined by parameterizing the spring constants. Based on such continuous parameterization, all instantiation of grasp types and all hand preshapes during a grasping action (reach, preshape, enclose, open) can be represented. We present experimental results, in which the spring constants are merely estimated from fingertip motion tracking using a stereo camera setup of a humanoid robot. The results show that the generated grasps based on the proposed representation are similar to the observed grasps. In this paper we present a framework for grasp planning with a humanoid robot arm and a five-fingered hand. The aim is to provide the humanoid robot with the ability of grasping objects that appear in a kitchen environment. Our approach is based on the use of an object model database that contains the description of all the objects that can appear in the robot workspace. This database is completed with two modules that make use of this object representation: An exhaustive offl ine grasp analysis system and a real-time stereo vision system. The offline grasp analysis system determines the best grasp for the objects by employing a simulation system, together with CAD models of the objects and the five-fingered hand. The results of this analysis are added to the object database using a description suited to the requirements of the grasp execution modules. A stereo camera system is used for a real-time object localization using a combination of appearance-based and model- based methods. The different components are integrated in a controller architecture to achieve manipulation task goals for the humanoid robot. Typical tasks of future service robots involve grasping and manipulating a large variety of objects differing in size and shape. Generating stable grasps on 3D objects is considered to be a hard problem, since many parameters such as hand kinematics, object geometry, material properties and forces have to be taken into account. This results in a high-dimensional space of possible grasps that cannot be searched exhaustively. We believe that the key to find stable grasps in an efficient manner is to use a special representation of the object geometry that can be easily analyzed. In this paper, we present a novel grasp planning method that evaluates local symmetry properties of objects to generate only candidate grasps that are likely to be of good quality. We achieve this by computing the medial axis which represents a 3D object as a union of balls. We analyze the symmetry information contained in the medial axis and use a set of heuristics to generate geometrically and kinematically reasonable candidate grasps. These candidate grasps are tested for force-closure. We present the algorithm and show experimental results on various object models using an anthropomorphic hand of a humanoid robot in simulation. The ability to grasp large objects with both hands enables bimanual robot systems to fully employ their capabilities in human-centered environments. Hence, algorithms are needed to precompute bimanual grasping configurations that can be used online to efficiently create whole body grasps. In this work we present a bimanual grasp planner that can be used to build a set of grasps together with manipulability information for a given object. For efficient grasp planning precomputed reachability information and a beneficial object representation, based on medial axis descriptions, are used. Since bimanual grasps may suffer from low manipulability, caused by a closed kinematic chain, we show how the manipulability of a bimanual grasp can be used as a quality measure. Therefore, manipulability clusters are introduced as an efficient way to approximatively describe the manipulability of a given bimanual grasp. The proposed approach is evaluated with a reference implementation, based on Simox [1], for the humanoid robot ARMAR-III [2]. Since the presented algorithms are robot-independent, there are no limitations for using this planner on other robot systems."
  },
  {
    "id": "411385",
    "input": "Generate a title for the following abstract of a paper: Stationary Subspace Analysis (SSA) is an unsupervised learning method that finds subspaces in which data distributions stay invariant over time. It has been shown to be very useful for studying non-stationarities in various applications. In this paper, we present the first SSA algorithm based on a full generative model of the data. This new derivation relates SSA to previous work on finding interesting subspaces from high-dimensional data in a similar way as the three easy routes to independent component analysis, and provides an information geometric view. The non-stationary nature of neurophysiological measurements, e.g. EEG, makes classification of motion intentions a demanding task. Variations in the underlying brain processes often lead to significant and unexpected changes in the feature distribution resulting in decreased classification accuracy in Brain Computer Interfacing (BCI). Several methods were developed to tackle this problem by either adapting to these changes or extracting features that are invariant. Recently, a method called Stationary Subspace Analysis (SSA) was proposed and applied to BCI data. It diminishes the influence of non-stationary changes as learning and classification is performed in a stationary subspace of the data which can be extracted by SSA. In this paper we extend this method in two ways. First we propose a variant of SSA that allows to extract stationary subspaces from labeled data without disregarding class-related variations or treating class-differences as non-stationarities. Second we propose a discriminant variant of SSA that trades-off stationarity and discriminativity, thus it allows to extract stationary subspaces without losing relevant information. We show that learning in a discriminative and stationary subspace is advantageous for BCI application and outperforms the standard SSA method. Modelling non-stationarities is an ubiquitous problem in neuroscience. Robust models help understand the underlying cause of the change observed in neuroscientific signals to bring new insights of brain functioning. A common neuroscientific signal to study the behaviour of the brain is electro-encephalography (EEG) because it is little intrusive, relatively cheap and easy to acquire. However, this signal is known to be highly non-stationary. In this paper we propose a robust method to visualize non-stationarities present in neuroscientific data. This method is unaffected by noise sources that are uninteresting to the cause of change, and therefore helps to better understand the neurological sources responsible for the observed non-stationarity. This technique exploits a robust version of the principal component analysis and we apply it as illustration to EEG data acquired using a brain-computer interface, which allows users to control an application through their brain activity. Nonstationarities in EEG cause a drop of performance during the operation of the brain-computer interface. Here we demonstrate how the proposed method can help to understand and design methods to deal with nonstationarities. In this article, we consider high-dimensional data which contains a low-dimensional non-Gaussian structure contaminated with Gaussian noise and propose a new method to identify the non-Gaussian subspace. A linear dimension reduction algorithm based on the fourth-order cumulant tensor was proposed in our previous work [4]. Although it works well for sub-Gaussian structures, the performance is not satisfactory for super-Gaussian data due to outliers. To overcome this problem, we construct an alternative by using Hessian of characteristic functions which was applied to (multidimensional) independent component analysis [10,11]. A numerical study demonstrates the validity of our method."
  },
  {
    "id": "411409",
    "input": "Generate a title for the following abstract of a paper: Large-scale distributed video surveillance systems pose new scalability challenges. Due to the large number of video sources in such systems, the amount of bandwidth required to transmit video streams for monitoring often strains the capability of the network. On the other hand, large-scale surveillance systems often rely on computer vision algorithms to automate surveillance tasks. We observe that these surveillance tasks present an opportunity for trade-off between the accuracy of the tasks and the bit rate of the video being sent. This paper shows that there exists a sweet spot, which we term critical video quality that can be used to reduce video bit rate without significantly affecting the accuracy of the surveillance tasks. We demonstrate this point by running extensive experiments on standard face detection and face tracking algorithms. Our experiments show that face detection works equally well even if the quality of compression is significantly reduced, and face tracking still works even if the frame rate is reduced to 6 frames per second. We further develop a prototype video surveillance system to demonstrate this idea. Our evaluation shows that we can achieve up to 29 times reduction in video bit rate when detecting faces and 16 times reduction when tracking faces. This paper also proposes a formal rate-accuracy optimization framework which can be used to determine appropriate encoding parameters in distributed video surveillance systems that are subjected to either bandwidth constraints or accuracy constraints. Many distributed multimedia applications rely on video analysis algorithms for automated video and image processing. Little is known, however, about the minimum video quality required to ensure an accurate performance of these algorithms. In an attempt to understand these requirements, we focus on a set of commonly used face analysis algorithms. Using standard datasets and live videos, we conducted experiments demonstrating that the algorithms show almost no decrease in accuracy until the input video is reduced to a certain critical quality, which amounts to significantly lower bitrate compared to the quality commonly acceptable for human vision. Since computer vision percepts video differently than human vision, existing video quality metrics, designed for human perception, cannot be used to reason about the effects of video quality reduction on accuracy of video analysis algorithms. We therefore investigate two alternate video quality metrics, blockiness and mutual information, and show how they can be used to estimate the critical video qualities for face analysis algorithms. Object tracking is commonly used in video surveillance, but typically video with full frame rate is sent. We previously have shown that full frame rate is not needed, but it is unclear what the appropriate frame rate to send or whether we can further reduce the frame rate. This paper answers these questions for two commonly used object tracking algorithms (frame-differencing-based blob tracking and CAMSHIFT tracking). The paper provides (i) an analytical framework to determine the critical frame rate to send a video for these algorithms without them losing the tracked object, given additional knowledge about the object and key design elements of the algorithms, and (ii) answers the questions of how we can modify the object tracking to further reduce the critical frame rate. Our results show that we can reduce the 30 fps rate by up to 7 times for blob tracking in the scenario of a single car moving across the camera view, and by up to 13 times for CAMSHIFT tracking in the scenario of a face moving in different directions. A video streaming server needs to adapt its source/channel encoding parameters (or configurations) to changes in network conditions and to differences in users' connection profiles. The adaptation can be achieved by adjusting parameters such as frame rate, error protection ratio, and resolution. Ideally, the server should adapt the serving configurations with respect to the current network and user conditions to improve received video quality. However, adaptations that optimize playable frame rate require intensive computation, and storing all possible configurations requires a tremendous amount of storage. This brings forth the issues of how to obtain good video quality and reduce server resources usage at the same time. We address this issue in this paper. Our approach is based on the observation that transcoding between certain configurations can be performed very efficiently. We propose a framework to compute a set of configurations to store on the server by considering two opposing goals: (a) maximizing expected received quality of the video, and (b) minimizing server resource usage by lowering transcoding cost and expected number of switches between configurations. The second objective also reduces the number of configurations, and therefore reduces the total storage required. Our framework models the relationship among different configurations in a partial order, formulates the search of a good set of configurations as an energy minimization problem, and we use techniques in image segmentation to solve the problem. Experimental results show that our framework relieves the server load and increases the number of clients served, while only slightly reducing the expected frame rate."
  },
  {
    "id": "41443",
    "input": "Generate a title for the following abstract of a paper: The investigation of genetic differences among humans has given evidence that mutations in DNA sequences are responsible for some genetic diseases. The most common mutation is the one that involves only a single nucleotide of the DNA sequence, which is called a single nucleotide polymorphism (SNP). As a consequence, computing a complete map of all SNPs occurring in the human populations is one of the primary goals of recent studies in human genomics. The construction of such a map requires to determine the DNA sequences that from all chromosomes. In diploid organisms like humans, each chromosome consists of two sequences called haplotypes. Distinguishing the information contained in both haplotypes when analyzing chromosome sequences poses several new computational issues which collectively form a new emerging topic of Computational Biology known as Haplotyping.This paper is a comprehensive study of some new combinatorial approaches proposed in this research area and it mainly focuses on the formulations and algorithmic solutions of some basic biological problems. Three statistical approaches are briefly discussed at the end of the paper. Producing spliced EST sequences is a fundamental task in the computational problem of reconstructing splice and transcript variants, a crucial step in the alternative splicing investigation. Now, given an EST sequence, there can be several spliced EST sequences associated to it, since the original EST sequences may have different alignments against wide genomic regions. In this paper we address a crucial issue arising from the above step: given a collection C of different spliced EST sequences that are associated to an initial set S of EST sequences, how can we extract a subset C\u2032 of C such that each EST sequence in S has a putative spliced EST in C\u2032 and C\u2032 agree on a common alignment region to the genome or gene structure? We introduce a new computational problem that models the above issue, and at the same time is also relevant in some more general settings, called Minimum Factorization Agreement (MFA). We investigate some algorithmic solutions of the MFA problem and their applicability to real data sets. We show that algorithms solving the MFA problem are able to find efficiently the correct spliced EST associated to an EST even when the splicing of sequences is obtained by a rough alignment process. Then we show that the MFA method could be used in producing or analyzing spliced EST libraries under various biological criteria. Alternative splicing (AS) is currently considered as one of the main mechanisms able to explain the huge gap between the number of predicted genes and the high complexity of the proteome in humans. The rapid growth of Expressed Sequence Tag (EST) data has encouraged the development of computational methods to predict alternative splicing from the analysis of EST alignment to genome sequences. EST data are also a valuable source to reconstruct the different transcript isoforms that derive from the same gene structure as a consequence of AS, as indeed EST sequences are obtained by fragmenting mRNAs from the same gene. The most recent studies on alternative splice sites detection have revealed that this topic is a quite challenging computational problem, far from a solution. The main computational issues related to the problem of detecting alternative splicing are investigated in this paper, and we analyze algorithmic solutions for this problem. We first formalize an optimization problem related to the prediction of constitutive and alternative splicing sites from EST sequences, the Minimum Exons ESTs Factorization problem (in short, MEF), and show that it is Np-hard, even for restricted instances. This problem leads us to define sets of spliced EST, that is, a set of EST factorized into their constitutive exons with respect to a gene. Then we investigate the computational problem of predicting transcript isoforms from spliced EST sequences. We propose a graph algorithm for the problem that is linear in the number of predicted isoforms and size of the graph. Finally, an experimental analysis of the method is performed to assess the reliability of the predictions. A challenging issue in designing computational methods for predicting the gene structure into exons and introns from a cluster of transcript (EST, mRNA) sequences, is guaranteeing accuracy as well as efficiency in time and space, when large clusters of more than 20,000 ESTs and genes longer than 1 Mb are processed. Traditionally, the problem has been faced by combining different tools, not specifically designed for this task.We propose a fast method based on ad hoc procedures for solving the problem. Our method combines two ideas: a novel algorithm of proved small time complexity for computing spliced alignments of a transcript against a genome, and an efficient algorithm that exploits the inherent redundancy of information in a cluster of transcripts to select, among all possible factorizations of EST sequences, those allowing to infer splice site junctions that are largely confirmed by the input data. The EST alignment procedure is based on the construction of maximal embeddings, that are sequences obtained from paths of a graph structure, called embedding graph, whose vertices are the maximal pairings of a genomic sequence T and an EST P. The procedure runs in time linear in the length of P and T and in the size of the output.The method was implemented into the PIntron package. PIntron requires as input a genomic sequence or region and a set of EST and/or mRNA sequences. Besides the prediction of the full-length transcript isoforms potentially expressed by the gene, the PIntron package includes a module for the CDS annotation of the predicted transcripts.PIntron, the software tool implementing our methodology, is available at http://www.algolab.eu/PIntron under GNU AGPL. PIntron has been shown to outperform state-of-the-art methods, and to quickly process some critical genes. At the same time, PIntron exhibits high accuracy (sensitivity and specificity) when benchmarked with ENCODE annotations."
  },
  {
    "id": "411283",
    "input": "Generate a title for the following abstract of a paper: Abstract. The paper proposes an epistemological analysis of the di-chotomy discovery/invention. In particular, we argue in favor o f the idea that science d oes not discover facts by induction but rather invents the-ories that are then checked against experience.  In artificial intelligence (AI), a number of criticisms were raised against the use of probability for dealing with uncertainty. All these criticisms, except what in this article we call the non-adequacy claim, have been eventually confuted. The non-adequacy claim is an exception because, unlike the other criticisms, it is exquisitely philosophical and, possibly for this reason, it was not discussed in the technical literature. A lack of clarity and understanding of this claim had a major impact on AI. Indeed, mostly leaning on this claim, some scientists developed an alternative research direction and, as a result, the AI community split in two schools: a probabilistic and an alternative one. In this article, we argue that the non-adequacy claim has a strongly metaphysical character and, as such, should not be accepted as a conclusive argument against the adequacy of probability. In this paper we propose an insect-based algorithm for tackling the Dynamic Task Allocation (DTA) problem, a fac- tory scheduling problem in which tasks are to be allocated to processing units. In previous works, multi-agent algorithms have been developed for the homogeneous case, that is, the case in which all agents (processing units) are identical. Here we investigate also the heterogeneous case in which agents can difier in their processing speed. Think for example of a factory with old and new machines or with difierent sets of machines each optimized for a difierent class of tasks. Most of the previously proposed algorithms use paradigms based on the specialization concept: Agents tend to specialize for one type of task in order to avoid unneces- sary reconflgurations. This typically increases the e-ciency of the whole system. Morley (13) has solved a painting prob- lem similar to the homogeneous version of the DTA problem. His market-based algorithm was adopted in a General Motors facility and reached a performance improvement of 10% over the previously adopted centralized scheduler. Furthermore, difierent insect-based algorithms have been successfully ap- plied to the homogeneous case of the DTA problem (4, 5). These algorithms are inspired by the division of labor in social insects and adopt the dynamic threshold model proposed by Theraulaz (15). In this paper we address two issues. First, we propose four modiflcations of an algorithm previously introduced by Cicirello and Smith (5). A detailed analysis of the impact of each modiflcation is given. Second, we propose a modi- flcation of the dynamic threshold model of Theraulaz et al. that was originally used for handling the case in which all booths are identical. We present a modifled version of the threshold model which takes into account the heterogeneous processing speeds of the agents and we show that the mod- ifled version obtains better results than the original version. Section 2 presents the problem using the example of a painting facility. Section 3 introduces related works, in par- ticular detailing the market-based and two insect-based al- gorithms. Then, Section 4 explains the algorithm proposed here, showing the modiflcations we applied to improve the performance. Section 5 proposes an experimental analysis that highlights the performance improvement obtained by the proposed algorithm over the ones previously presented in the literature. Section 6 concludes the paper. Task partitioning consists in dividing a task into sub-tasks that can be tackled separately. Partitioning a task might have both positive and negative effects: On the one hand, partitioning might reduce physical interference between workers, enhance exploitation of specialization, and increase efficiency. On the other hand, partitioning may introduce overheads due to coordination requirements. As a result, whether partitioning is advantageous or not has to be evaluated on a case-by-case basis. In this paper we consider the case in which a swarm of robots must decide whether to complete a given task as an unpartitioned task, or utilize task partitioning and tackle it as a sequence of two sub-tasks. We show that the problem of selecting between the two options can be formulated as a multi-armed bandit problem and tackled with algorithms that have been proposed in the reinforcement learning literature. Additionally, we study the implications of using explicit communication between the robots to tackle the studied task partitioning problem. We consider a foraging scenario as a testbed and we perform simulation-based experiments to evaluate the behavior of the system. The results confirm that existing multi-armed bandit algorithms can be employed in the context of task partitioning. The use of communication can result in better performance, but in may also hinder the flexibility of the system."
  },
  {
    "id": "411337",
    "input": "Generate a title for the following abstract of a paper: We investigate on-line strategies for money-making trading with brokerage, while competitive algorithms without considering the costs of exchanging are investigated in [R.El-Yaniv, A.Fiat, R.Karp, and G.Turpin, Proc. of FOCS, (1992) ]. A password-authenticated key exchange (PAKE) protocol in the three-party setting allows two users communicating over a public network to agree on a common session key by the help of a server. In the setting the users do not share a password between themselves, but only with the server. In this paper, we explore the possibility of designing a round-efficient three-party PAKE protocol with a method to protect against undetectable on-line dictionary attacks without using the random oracle. The protocol matches the most efficient three-party PAKE protocol secure against undetectable on-line dictionary attacks among those found in the literature while providing the same level of security. Finally, we indentify the relations between detectable on-line and undetectable on-line dictionary attacks by providing counter-examples to support the observed relations. Multivariate Public Key Cryptosystems (MPKC) are candidates for post-quantum cryptography. Rainbow is a digital signature scheme in MPKC, whose encryption and decryption are relatively efficient. However, the security of MPKC depends on the difficulty in solving a system of multivariate polynomials, and the key length of MPKC becomes substantially large compared with that of RSA cryptosystems for the same level of security. The size of the public key in MPKC has been reduced in previous research, but to the best of our knowledge, there are no algorithms to reduce the size of a private key . In this paper, we propose NC-Rainbow, a variation of Rainbow using non-commutative rings and we describe the ability of the proposed scheme to reduce the size of a private key in comparison with the ordinary Rainbow while maintaining the same level of security. In particular, using the proposed NC-Rainbow, the size of a private key is reduced by about 75% at the 80 bit security level. Moreover, the speed of signature generation is accelerated by about 34% at the 80 bit security level. Considering the fact that there exist information asymmetry (hidden information) in routing phase, and moral hazard (hidden action) in forwarding phase in autonomous Ad hoc network, this paper argues that economic-based mechanisms play both a signaling and a sanctioning role, which reveal the node's true forwarding cost in routing phase while provide incentives to nodes to exert reasonable effort in forwarding phase, that is, the role of economicinspired mechanisms in information asymmetry is to induce learning whereas the role of such mechanisms in moral hazard settings is to constrain behavior. Specifically, this paper conducts the following works: considering the mutually dependent link cost, we demonstrate that, for each participant, truth-telling is the risk dominant strategy in VCG-like routing mechanism based on analysis of extensive game form. Then, Individual rationality (IR) and Incentive Compatibility (IC) constraints are formally offered, which should be satisfied by any game theoretical routing and forwarding scheme. And different solution concepts are investigated to characterize the economic meanings of two kind forwarding approaches, that is, Nash equilibrium with no per-hop monitoring and dominant strategy equilibrium with per-hop monitoring."
  },
  {
    "id": "411156",
    "input": "Generate a title for the following abstract of a paper: We propose a system for retrieving human locomotion patterns from tracking data captured within a large geographical area, over a long period of time. A GPS receiver continuously captures data regarding the location of the person carrying it. A constrained agglomerative hierarchical clustering algorithm segments these data according to the person's navigational behavior. Sketches made on a map displayed on a computer screen are used for specifying queries regarding locomotion patterns. Two basic sketch primitives, selected based on a user study, are combined to form five different types of queries. We implement algorithms to analyze a sketch made by a user, identify the query, and retrieve results from the collection of data. A graphical user interface combines the user interaction strategy and algorithms, and allows hierarchical querying and visualization of intermediate results. We evaluate the system using a collection of data captured during nine months. The constrained hierarchical clustering algorithm is able to segment GPS data at an overall accuracy of 94% despite the presence of location-dependent noise. A user study was conducted to evaluate the proposed user interaction strategy and the usability of the overall system. The results of this study demonstrate that the proposed user interaction strategy facilitates fast querying, and efficient and accurate retrieval, in an intuitive manner. We propose a system for retrieving human locomotion patterns from tracking data captured within a large geographical area, over a long period of time. A GPS receiver continuously captures data regarding the location of the person carrying it. A constrained agglomerative hierarchical clustering algorithm segments these data according to the person's navigational behavior. Sketches made on a map displayed on a computer screen are used for specifying queries regarding locomotion patterns. Two basic sketch primitives, selected based on a user study, are combined to form five different types of queries. We implement algorithms to analyze a sketch made by a user, identify the query, and retrieve results from the collection of data. A graphical user interface combines the user interaction strategy and algorithms, and allows hierarchical querying and visualization of intermediate results. The sketch-based user interaction strategy facilitates querying for locomotion patterns in an intuitive and unambiguous manner. A system for retrieving video sequences created by tracking humans in a smart environment, by using spatial queries, is presented. Sketches made with a pointing device on the floor layout of the environment are used to formqueries corresponding to locomotion patterns. The sketches are analyzed to identify the type of the query. Directional search algorithms based on the minimum distance between points are applied for finding the best matches to the sketch. The results are ranked according to the similarity and presented to the user. The system was developed in two stages. An initial version of the system was implemented and evaluated by conducting a user study. Modifications were made where appropriate, according to the results and the feedback, to make the system more accurate and usable. We present the details of the initial system, the user study and the results, and the modifications thus made. The overall accuracy of retrieval for the initial system was approximately 93%, when tested on a collection of data from a real-life experiment. This is improved to approximately 97% after the modifications. The user interaction strategy and the search algorithms are usable in any environment for automated retrieval of locomotion patterns. The subjects who evaluated the system found it easy to learn and use. Their comments included several prospective applications for the user interaction strategy, providing valuable insight for future directions. We propose a system for retrieving multimedia related to a person's travel, using location data captured with a GPS receiver, mobile phone or a camera. The user makes simple sketches on a map displayed on a computer screen, to submit spatial, temporal or spatio-temporal queries regarding his travel. The system segments the location data and images, analyzes sketches made by a user, identifies the query, and retrieves relevant results. These results, combined with online maps and virtual tours rendered using street view panoramas, form multimedia travel stories. We present the system's current status and conclude with future directions."
  },
  {
    "id": "41329",
    "input": "Generate a title for the following abstract of a paper: A key prerequisite to make user instruction of work tasks by interactive demonstration effective and convenient is situated multi-modal interaction aiming at an enhancement of robot learning beyond simple low-level skill acquisition. We report the status of the Bielefeld GRAVIS-robot system that combines visual attention and gestural instruction with an intelligent interface for speech recognition and linguistic interpretation to allow multi-modal task-oriented instructions. With respect to this platform, we discuss the essential role of learning for robust functioning of the robot and sketch the concept of an integrated architecture for situated learning on the system level. It has the long-term goal to demonstrate speech-supported imitation learning of robot actions. We describe the current state of its realization to enable imitation of human hand postures for flexible grasping and give quantitative results for grasping a broad range of everyday objects. A major challenge for the realization of intelligent robots is to supply them with cognitive abilities in order to allow ordinary users to program them easily and intuitively. One approach to such programming is teaching work tasks by interactive demonstration. To make this effective and convenient for the user, the machine must be capable of establishing a common focus of attention and be able to use and integrate spoken instructions, visual perception, and non-verbal clues like gestural commands. We report progress in building a hybrid architecture that combines statistical methods, neural networks, and finite state machines into an integrated system for instructing grasping tasks by man-machine interaction. The system combines the GRAVIS-robot for visual attention and gestural instruction with an intelligent interface for speech recognition and linguistic interpretation, and a modality fusion module to allow multi-modal task-oriented man-machine communication with respect to dextrous robot manipulation of objects. We argue that direct experimental approaches to elucidate the architecture of higher brains may benefit from insights gained from exploring the possibilities and limits of artificial control architectures for robot systems. We present some of our recent work that has been motivated by that view and that is centered around the study of various aspects of hand actions since these are intimately linked with many higher cognitive abilities. As examples, we report on the development of a modular system for the recognition of continuous hand postures based on neural nets, the use of vision and tactile sensing for guiding prehensile movements of a multifingered hand, and the recognition and use of hand gestures for robot teaching. Regarding the issue of learning, we propose to view real-world learning from the perspective of data-mining and to focus more strongly on the imitation of observed actions instead of purely reinforcement-based exploration. As a concrete example of such an effort we report on the status of an ongoing project in our laboratory in which a robot equipped with an attention system with a neurally inspired architecture is taught actions by using hand gestures in conjunction with speech commands. We point out some of the lessons learnt from this system, and discuss how systems of this kind can contribute to the study of issues at the junction between natural and artificial cognitive systems. Grasping and manual interaction for robots so far has largely been approached with an emphasis on physics and control aspects.\n Given the richness of human manual interaction, we argue for the consideration of the wider field of \u201cmanual intelligence\u201d\n as a perspective for manual action research that brings the cognitive nature of human manual skills to the foreground. We\n briefly sketch part of a research agenda along these lines, argue for the creation of a manual interaction database as an\n important cornerstone of such an agenda, and describe the manual interaction lab recently set up at CITEC to realize this\n goal and to connect the efforts of robotics and cognitive science researchers towards making progress for a more integrated\n understanding of manual intelligence."
  },
  {
    "id": "412098",
    "input": "Generate a title for the following abstract of a paper: How hard to users to find interactive devices to use to achieve their goals, and how can we get this information early enough to influence design? We show that Markov modeling can obtain suitable measures, and we provide formulas that can be used for a large class of systems. We analyze and consider alternative designs for various real examples. We introduce a \u201cknowledege/usability graph,\u201d which shows the impact of even a smaller amount of knowledge for the user, and the extent to which designers' knowledge may bias their views of usability. Markov models can be built into design tools, and can therefore be made very convenient for designers to utilize. One would hope that in the future, design tools would include such mathematical analysis, and no new design skills would be required to evaluate devices. A particular concern of this paper is to make the approach accessible. Complete program code and all the underlying mathematics are provided in appendices to enable others to replicate and test all results shown. As interactive systems become increasingly entwined with architecture, and spaces become able to detect the presence of individuals, we argue that the control of visibility as a temporary personal state should be considered in the design of public spaces. This workshop will provide the opportunity for participants to engage hands-on with a computer vision tracking system (OpenCV) and explore how low-cost materials and tools can be used to render people invisible in monitored public space. We invite researchers and practitioners from the fields of art, design, HCI, architecture and social science to consider strategies for managing personal visibility and how these relate to design and the use of technologies. The intention of the workshop is not to produce implementable designs. Instead we prefer to make speculative design scenarios that might act as future inspiration or critique. By focusing on practical strategies for managing personal visibility we hope to extend designers thinking of presence in public space beyond the purely physical to include digital representations of inhabitation that are processed and archived remotely. Ten years ago, we were on the verge of having cameras built into our mobile phones, but knew very little about what to expect or how they would be used. Now we are faced with the same unknowns with mobile projector phones. This research seeks to explore how people will want to use such technology, how they will feel when using it, and what social effects we can expect to see. This paper describes our two-phase field investigation that uses a combination of methods to investigate how, when, and why mobile projections may be used. The first study used an experience sampling method to investigate responses to a range of different media types, and, for example, the choice of surfaces used in each case. The second study asked users to create video diary entries showing when, where, and why they would have wanted to project information. Together these studies provide complementary insights into the future use of mobile projector phones. Our results cover detailed responses to a range of media types from the first study, while the second identified which of the known mobile information needs were commonly recorded by participants. Both studies provide insights that may help shape the hardware, software, and interaction design of mobile projector phones as they become increasingly available. Swansea University | always@acm.orgmobiles employ a menu-based style of interac-tion with textual labels. In contrast, we used culturally sensitive icons developed with our vil-lage population to control simple multimedia and file-handling functions. StoryCreator was used to author short audio-photo narratives, comprising a storyboard of up to six still images synchro-nised to a voice-over of up to two minutes long (see Figure 3). Users are led through a story-creation process to fill media slots in a template, either image first or sound first. Once the media elements in each stream are recorded, users are prompted to synchronise the streams by replay-ing the sound clip and tabbing through the imag-es at the time they want them to appear. The only editing supported is to review and delete media elements or their synchronisation. Despite the creative limitations of this design and a very slow response time on some of the actions, rural Indian users were able to use it in a one-month trial to record a variety of story content with minimal training. One hundred and thirty-seven stories were recorded by 79 people, using 10 phones, on topics ranging from agricul-ture and health to education, self-help groups, and entertainment. The average number of imag-es used was 4.5, with a mean voice-over length of 66 seconds. A typical story is shown in Figure 1, with the local Kannada language voice-over translated and transcribed below the picture to which it relates. A young boy describes the chal-lenges of rearing cows in a short agricultural story lasting 1 minute 50 seconds; this plays back full-screen like a PowerPoint slideshow with spoken narration. A range of creative effects were demonstrated across the corpus, including the use of song during activities, the unfold-It is widely assumed that the Internet is a global information resource. This is not true. For many people in the poorest parts of the world, the Internet is both technically and psychologically inaccessible through lack of infrastructure, money, and the requisite forms of textual and computer literacy. The StoryBank project has been tackling some of these issues by using the fast-growing infrastructure of mobile telephony to support an alternative form of information sharing in pictures and sound. Situated in the Indian village of Budikote and inspired by developments in audiophotography and mobile imaging [1, 2], we have been exploring the possibility of semiliterate communities using the camera phone as a new kind of pen and paper for creating and sharing audio-visual stories. The system design has been described in a recent conference paper [3], and we are currently preparing a full write-up of the trial results. Here we want to promote the simple story format arrived at in the research, and point to some of the interaction design challenges of supporting it in this context. The mobile is undoubtedly a transformative technology for development work. Networking and power-management innovations and large-scale investment mean that even very remote rural locations are getting connected. But a word of caution: One cannot necessarily deploy in-built phone interfaces and applications for popu-lations that do not have our exposure to comput-ing or the levels of textual literacy we assume.Hence, three non-textual applications were written for the Nokia N80 camera phone: StoryCreator, StoryPlayer, and StorySender. This was a considerable challenge, since all existing"
  },
  {
    "id": "41621",
    "input": "Generate a title for the following abstract of a paper: In this survey, we deal with the problem how a universal computer can be constructed in a reversible environment. We discuss this problem based on the frameworks of reversible Turing machines, reversible logic circuits, and reversible cellular automata. We can see that in spite of the constraint of reversibility, there are several very simple reversible systems that have universal computing ability. Reversible computing is a paradigm where computing models are defined so that they reflect physical reversibility, one of the fundamental microscopic physical property of Nature. In this survey/tutorial paper, we discuss how computation can be carried out in a reversible system, how a universal reversible computer can be constructed by reversible logic elements, and how such logic elements are related to reversible physical phenomena. We shall see that, in reversible systems, computation can often be carried out in a very different manner from conventional (i.e., irreversible) computing systems, and even very simple reversible systems or logic elements have computation- or logical-universality. We discuss these problems based on reversible logic elements/circuits, reversible Turing machines, reversible cellular automata, and some other related models of reversible computing. Reversible computing is a paradigm of computation that reflects physical reversibility, and will become important when we develop future computing systems that directly utilize microscopic physical phenomena for logical operations. In this survey we discuss, from a theoretical point of view, how a reversible computer is implemented as a reversible logic circuit, how a reversible logic circuit is composed of reversible logic elements, and how a reversible logic element can be realized in a physically reversible system. We shall see that, in spite of the constraint of reversibility, universal reversible computers can be constructed by very simple reversible primitives, and that in these systems computation is often carried out in a very unique and different manner from conventional computing systems. Reversible computing is a paradigm of computation that reflects physical reversibility, and is considered to be important when designing a logical devices based on microscopic physical law in the near future. In this paper, we focus on a problem how universal computers can be built from primitive elements with very simple reversible rules. We introduce a new reversible logic element called a \"rotary element\", and show that any reversible Turing machine can be realized as a circuit composed only of them. Such reversible circuits work in a very different fashion from conventional ones. We also discuss a simple reversible cellular automaton in which a rotary element can be implemented."
  },
  {
    "id": "411848",
    "input": "Generate a title for the following abstract of a paper: We present a new algorithm to detect humans in still images utilizing covariance matrices as object descriptors. Since these descriptors do not lie on a vector space, well known machine learning techniques are not adequate to learn the classifiers. The space of d-dimensional nonsin- gular covariance matrices can be represented as a connected Riemannian manifold. We present a novel approach for classifying points lying on a Riemannian manifold by incorporating the a priori information about the geometry of the space. The algorithm is tested on INRIA human database where superior detection rates are observed over the previous approaches. We present a new algorithm to detect pedestrian in still images utilizing covariance matrices as object descriptors. Since the descriptors do not form a vector space, well known machine learning techniques are not well suited to learn the classifiers. The space of d-dimensional nonsingular covariance matrices can be represented as a connected Riemannian manifold. The main contribution of the paper is a novel approach for classifying points lying on a connected Riemannian manifold using the geometry of the space. The algorithm is tested on INRIA and DaimlerChrysler pedestrian datasets where superior detection rates are observed over the previous approaches. We describe a new region descriptor and apply it to two problems, object detection and texture classification. The covariance of d-features, e.g., the three-dimensional color vector, the norm of first and second derivatives of intensity with respect to x and y, etc., characterizes a region of interest. We describe a fast method for computation of covariances based on integral images. The idea presented here is more general than the image sums or histograms, which were already published before, and with a series of integral images the covariances are obtained by a few arithmetic operations. Covariance matrices do not lie on Euclidean space, therefore we use a distance metric involving generalized eigenvalues which also follows from the Lie group structure of positive definite matrices. Feature matching is a simple nearest neighbor search under the distance metric and performed extremely rapidly using the integral images. The performance of the covariance features is superior to other methods, as it is shown, and large rotations and illumination changes are also absorbed by the covariance matrix. The original mean shift algorithm is widely applied for nonparametric clustering in vector spaces. In this paper we generalize it to data points lying on Riemannian manifolds. This allows us to extend mean shift based clustering and filtering techniques to a large class of frequently occurring non-vector spaces in vision. We present an exact algorithm and prove its convergence properties as opposed to previous work which approximates the mean shift vector. The computational details of our algorithm are presented for frequently occurring classes of manifolds such as matrix Lie groups, Grassmann manifolds, essential matrices and symmetric positive definite matrices. Applications of the mean shift over these manifolds are shown."
  },
  {
    "id": "41811",
    "input": "Generate a title for the following abstract of a paper: Many natural language researchers are currently turning their attention to treebank development and trying to achieve accuracy and corpus data coverage in their representation formats. This paper presents a data-driven annotation schema developed for an Italian treebank ensuring data coverage and consistency between annotation of linguistic phenomena. The schema is a dependency-based format centered upon the notion of predicate-argument structure augmented with traces to represent discontinuous constituents. The treebank development involves an annotation process performed by a human annotator helped by an interactive parsing tool that builds incrementally syntactic representation of the sentence. To increase the syntactic knowledge of this parser, a specific data-driven strategy has been applied. We describe the cyclical development of the annotation schema highlighting the richness and flexibility of the format, and we present some representational issues. This paper presents a relation-based schema for treebank annotation, and its application in the development of a corpus of Italian sentences. The annotation schema keeps arguments and modifiers distinct and allows for an accurate representation of predicate-argument structure and subcategorization. The accuracy strongly depends on methods adopted for defining the relations which axe tripartite feature structures that consist of a morpho-syntactic, a functional and a semantic component. We presents empirical evidence for these tripartite structures by illustrating phenomena faced in the development of an Italian treebank. The paper investigates the issue of portability of methods and results over treebanks in different languages and annotation formats. In particular, it addresses the problem of converting an Italian treebank, the Turin University Treebank (TUT), developed in dependency format, into the Penn Treebank format, in order to possibly exploit the tools and methods already developed and compare the adequacy of information encoding in the two formats. We describe the procedures for converting the two annotation formats and we present an experiment that evaluates some linguistic knowledge extracted from the two formats, namely sub-categorization frames. This paper explores the convergence between cognitive modeling and engineering solutions to the parsing problem in NLP Natural language presents many sources of ambiguity, and several theories of human parsing claim that ambiguity is resolved by using past (linguistic) experience. In this paper we analyze and refine a connectionist paradigm (Recursive Neural Networks) capable of processing acyclic graphs to perform supervised learning on syntactic trees extracted from a large corpus of parsed sentences. Following a widely accepted hypothesis in psycholinguistics, we assume an incremental parsing process (one word at a time) that keeps a connected partial parse tree at all times. By implementing a parsing simulation procedure, we collect a large amount of data that shows the viability of the RNN as informant of a disambiguation process. We analyze what kind of information is exploited by the connectionist system in order to resolve different sources of ambiguity, and we see how the generalization performance of the system is affected by the tree complexity and the frequency of specific subtrees. We finally propose some enhancements to the architecture in order to achieve a better prediction accuracy."
  },
  {
    "id": "411166",
    "input": "Generate a title for the following abstract of a paper: In this paper we prove lower bounds on randomized multiparty communication complexity, both in the blackboard model (where each message is written on a blackboard for all players to see) and (mainly) in the message-passing model, where messages are sent player-to-player. We introduce a new technique for proving such bounds, called symmetrization, which is natural, intuitive, and often easy to use. For example, for the problem where each of k players gets a bit-vector of length n, and the goal is to compute the coordinate-wise XOR of these vectors, we prove a tight lower bounds of \u03a9(nk) in the blackboard model. For the same problem with AND instead of XOR, we prove a lower bounds of roughly \u03a9(nk) in the message-passing model (assuming k \u2264 n/3200) and \u03a9(n log k) in the blackboard model. We also prove lower bounds for bit-wise majority, for a graphconnectivity problem, and for other problems; the technique seems applicable to a wide range of other problems as well. The obtained communication lower bounds imply new lower bounds in the functional monitoring model [11] (also called the distributed streaming model). All of our lower bounds allow randomized communication protocols with two-sided error. We also use the symmetrization technique to prove several direct-sum-like results for multiparty communication. With the popularity of portable wireless devices it is important to model and predict how information or contagions spread by natural human mobility - for understanding the spreading of deadly infectious diseases and for improving delay tolerant communication schemes. Formally, we model this problem by considering M moving agents, where each agent initially carries a distinct bit of information. When two agents are at the same location or in close proximity to one another, they share all their information with each other. We would like to know the time it takes until all bits of information reach all agents, called the flood time, and how it depends on the way agents move, the size and shape of the network and the number of agents moving in the network.\n\nWe provide rigorous analysis for the Manhattan Random Way-point model (which takes paths with minimum number of turns), a convenient model used previously to analyze mobile agents, and find that with high probability the flood time is bounded by O (N log M[(N/M) log(NM)]), where M agents move on an N \u00d7 N grid. In addition to extensive simulations, we use a data set of taxi trajectories to show that our method can successfully predict flood times in both experimental settings and the real world.\n\n Tracking and approximating data matrices in streaming fashion is a fundamental challenge. The problem requires more care and attention when data comes from multiple distributed sites, each receiving a stream of data. This paper considers the problem of \\\"tracking approximations to a matrix\\\" in the distributed streaming model. In this model, there are m distributed sites each observing a distinct stream of data (where each element is a row of a distributed matrix) and has a communication channel with a coordinator, and the goal is to track an \u03b5-approximation to the norm of the matrix along any direction. To that end, we present novel algorithms to address the matrix approximation problem. Our algorithms maintain a smaller matrix B, as an approximation to a distributed streaming matrix A, such that for any unit vector x: |||Ax||2 \u2212 ||Bx||2| \u2264 \u03b5||A||2F. Our algorithms work in streaming fashion and incur small communication, which is critical for distributed computation. Our best method is deterministic and uses only O((m/\u03b5) log(\u03b2N)) communication, where N is the size of stream (at the time of the query) and \u03b2 is an upperbound on the squared norm of any row of the matrix. In addition to proving all algorithmic properties theoretically, extensive experiments with real large datasets demonstrate the efficiency of these protocols. A recent paper [1] proposes a general model for distributed learning that bounds the communication required for learning classifiers with \u03b5 error on linearly separable data adversarially distributed across nodes. In this work, we develop key improvements and extensions to this basic model. Our first result is a two-party multiplicative-weight-update based protocol that uses O(d2 log1/\u03b5) words of communication to classify distributed data in arbitrary dimension d, \u03b5-optimally. This extends to classification over k nodes with O(kd2 log1/\u03b5) words of communication. Our proposed protocol is simple to implement and is considerably more efficient than baselines compared, as demonstrated by our empirical results. In addition, we show how to solve fixed-dimensional and high-dimensional linear programming with small communication in a distributed setting where constraints may be distributed across nodes. Our techniques make use of a novel connection from multipass streaming, as well as adapting the multiplicative- weight-update framework more generally to a distributed setting."
  },
  {
    "id": "411498",
    "input": "Generate a title for the following abstract of a paper: Abstract: In this paper we present a transitionfunction based characterization of actionsin a realistic environment. Ourlanguage allows for the specification ofactions with duration, continuous effects,delayed effects, dependency onnon-sharable resources, and accounts forparallel and overlapping execution of actions. This paper presents an Answer Set Programming based approach to multiagent planning. The proposed methodology extends the action language B in [12] to represent and reason about plans with cooperative actions of an individual agent operating in a multiagent environment. This language is used to formalize multiagent planning problems and the notion of a joint plan for multi-agent in the presence of cooperative actions. Finally, the paper presents a system for computing joint plans based on the ASP-Prolog system. In this paper, we investigate the multiagent planning problem in the presence of cooperative actions and agents, which have their own goals and are willing to cooperate. To this end, we extend the action language $\\mathcal A$ in [12] to represent and reason about plans with cooperative actions of an individual agent operating in a multiagent environment. We then use the proposed language to formalize the multiagent planning problem and the notion of a joint plan for multiagents in this setting. We discuss a method for computing joint plans using answer set programming and provide arguments for the soundness and completeness of the implementation. This paper describes our methodology for building conformant planners, which is based on recent advances in the theory of action and change and answer set programming. The development of a planner for a given dynamic domain starts with encoding the knowledge about fluents and actions of the domain as an action theory D of some action language. Our choice in this paper is AL - an action language with dynamic and static causal laws and executability conditions. An action theory D of AL defines a transition diagram T(D) containing all the possible trajectories of the domain. A transition belongs to T(D) iff the execution of the action a in the state s may move the domain to the state s^'. The second step in the planner development consists in finding a deterministic transition diagram T^l^p(D) such that nodes of T^l^p(D) are partial states of D, its arcs are labeled by actions, and a path in T^l^p(D) from an initial partial state @d^0 to a partial state satisfying the goal @d^f corresponds to a conformant plan for @d^0 and @d^f in T(D). The transition diagram T^l^p(D) is called an 'approximation' of T(D). We claim that a concise description of an approximation of T(D) can often be given by a logic program @p(D) under the answer sets semantics. Moreover, complex initial situations and constraints on plans can be also expressed by logic programming rules and included in @p(D). If this is possible then the problem of finding a parallel or sequential conformant plan can be reduced to computing answer sets of @p(D). This can be done by general purpose answer set solvers. If plans are sequential and long then this method can be too time consuming. In this case, @p(D) is used as a specification for a procedural graph searching conformant planning algorithm. The paper illustrates this methodology by building several conformant planners which work for domains with complex relationship between the fluents. The efficiency of the planners is experimentally evaluated on a number of new and old benchmarks. In addition we show that for a subclass of action theories of AL our planners are complete, i.e., if in T^l^p(D) we cannot get from @d^0 to a state satisfying the goal @d^f then there is no conformant plan for @d^0 and @d^f in T(D)."
  },
  {
    "id": "411662",
    "input": "Generate a title for the following abstract of a paper: Although competition is regarded as a powerful motivator in game-based learning, it might have a negative influence, such as damage to confidence, on students who lose the competition. In this paper, we propose an indirect approach, substitutive competition, to alleviate such negative influences. The approach is used to develop a My-Pet v3 system, in which pupils master subject materials to make their pets stronger, and compete against each other. Specifically, pupils learn Chinese idioms in a pet-training game scenario, and their mastery of the material is related to the pets' strength to win the competition. The result of the competition is influenced by whether pupils spend enough effort on the learning tasks. This intention is expected to alleviate the negative influence that results from direct competition. A within-subject experiment was conducted to examine the influence of substitutive competition. The results indicated that substitutive competition seems a promising scheme to maximise the power of competition. However, there were no apparent evidences in this study to demonstrate its effect to alleviate pupils' sense of failure, as compared with other two direct competition conditions. This paper reports the results of an experiment that used qualitative and quantitative methods to investigate the effect of competition on students' use of game-based deliberate practice. We hypothesized that the results of the experiment would show that competition has a positive effect on performance outcomes, but it also increases students' tendency to game the system. The actual results of the experiment showed only very modest support for these hypotheses but have other implications for improving the design of educational games. To support self-regulated learning (SRL), computer-based learning environments (CBLEs) are often designed to be open-ended and multidimensional. These systems incorporate diverse features that allow students to enact and reveal their SRL strategies via the choices they make. However, research shows that students' use of such features is limited; students often neglect SRL-supportive tools in CBLEs. In this study, we examined middle school students' feature use and strategy development over time using a teachable agent system called Betty's Brain. Students learned about climate change and thermoregulation in two units spanning several weeks. Learning was assessed using a pretest-posttest design, and students' interactions with the system were logged. Results indicated that use of SRL-supportive tools was positively correlated with learning outcomes. However, promising strategy patterns weakened over time due to shallow strategy development, which also negatively impacted the efficacy of the system. Although students seemed to acquire one beneficial strategy, they did so at the cost of other beneficial strategies. Understanding this phenomenon may be a key avenue for future research on SRL-supportive CBLEs. We consider two hypotheses for explaining and perhaps reducing shallow strategy development: a student-centered hypothesis related to \\\"gaming the system,\\\" and a design-centered hypothesis regarding how students are scaffolded via the system. Highlights\u00bf We examine feature use in a teachable-agent system - Betty's Brain. \u00bf Middle school students learn about science processes by constructing concept maps. \u00bf Learning gains are associated with use of SRL-supportive features of the system. \u00bf Promising strategy patterns weaken over time due to shallow strategy development. \u00bf Students focus on a limited set of strategies at the cost of other beneficial strategies. The idea that teaching others is a powerful way to learn is intuitively compelling and supported in the research literature. We have developed computer-based, domain-independent Teachable Agents that students can teach using a visual representation. The students query their agent to monitor their learning and problem solving behavior. This motivates the students to learn more so they can teach their agent to perform better. This paper presents a teachable agent called Betty's Brain that combines learning by teaching with self-regulated learning feedback to promote deep learning and understanding in science domains. A study conducted in a 5th grade science classroom compared three versions of the system: a version where the students were taught by an agent, a baseline learning by teaching version, and a learning by teaching version where students received feedback on self-regulated learning strategies and some domain content. In the other two systems, students received feedback primarily on domain content. Our results indicate that all three groups showed learning gains during a main study where students learnt about river ecosystems, but the two learning by teaching groups performed better than the group that was taught. These differences persisted in the transfer study, but the gap between the baseline learning by teaching and self-regulated learning group decreased. However, there are indications that self-regulated learning feedback better prepared students to learn in new domains, even when they no longer had access to the self-regulation environment."
  },
  {
    "id": "411022",
    "input": "Generate a title for the following abstract of a paper: In this paper, we propose a genetic algorithm that generates and assesses assembly plans. An appropriately modified version of the well-known partially matched crossover, and purposely defined mutation operators allow the algorithm to produce near-optimal assembly plans starting from a randomly initialised population of (possibly non-feasible) assembly sequences. The quality of a feasible assembly sequence is evaluated based on the following three optimisation criteria: (i) minimising the orientation changes of the product; (ii) minimising the gripper replacements; and (iii) grouping technologically similar assembly operations. Two examples that endorse the soundness of our approach are also included. Context adaptation is certainly a promising approach in the development of fuzzy rule based systems (FRBSs). First, an initial rule base is extracted from heuristic knowledge of the application domain. Meanings of linguistic terms are defined so as to guarantee high interpretability of the FRBSs. Then, meanings are adapted to a specific context through the use of operators that, using a set of known input\u2013output patterns, appropriately modify the corresponding fuzzy sets. The choice of the specific operators and their parameters is context based and optimized so as to obtain a good interpretability\u2013accuracy trade-off. In this paper, we propose a set of operators that, starting from a given FRBS, adapt the FRBS to the specific context by adjusting the universes of the input and output variables, and modifying the core, the support and the shape of the fuzzy sets which compose the partitions of these universes. The operators are defined so as to preserve ordering of the linguistic terms, universality of rules, and interpretability of partitions. The choice of the parameters used in the operators is performed by a genetic optimization process aimed at maximizing the accuracy and preserving the interpretability of the FRBS. We finally describe the application of our context adaptation approach to two Mamdani fuzzy systems developed, respectively, for two different domains, namely, regression and data modeling. \u00a9 2008 Wiley Periodicals, Inc. In the last few years, several papers have exploited multi-objective evolutionary algorithms (MOEAs) to generate Mamdani fuzzy rule-based systems (MFRBSs) with different trade-offs between interpretability and accuracy. In this framework, a common approach is to distinguish between interpretability of the rule base (RB), also known as complexity, and interpretability of fuzzy partitions, also known as integrity of the database (DB). Typically, complexity has been used as one of the objectives of the MOEAs, while partition integrity has been ensured by enforcing constraints on the membership function (MF) parameters. In this paper, we propose to adopt partition integrity as an objective of the evolutionary process. To this aim, we first discuss how partition integrity can be measured by using a purposely defined index based on the similarity between the partitions learned during the evolutionary process and the initial interpretable partitions defined by an expert. Then, we introduce a three-objective evolutionary algorithm which generates a set of MFRBSs with different trade-offs between complexity, accuracy and partition integrity by concurrently learning the RB and the MF parameters of the linguistic variables. Accuracy is assessed in terms of mean squared error between the actual and the predicted values, complexity is calculated as the total number of conditions in the antecedents of the rules and integrity is measured by using the purposely defined index. The proposed approach has been experimented on six real-world regression problems. The results have been compared with those obtained by applying the same MOEA, but with only accuracy and complexity as objectives, both to learn only RBs, and to concurrently learn RBs and MF parameters, with and without constraints on the parameter tuning. We show that our approach achieves the best trade-offs between interpretability and accuracy. Finally, we compare our approach with a similar MOEA recently proposed in the literature. In the last years, the numerous successful applications of fuzzy rule-based systems (FRBSs) to several different domains have produced a considerable interest in methods to generate FRBSs from data. Most of the methods proposed in the literature, however, focus on performance maximization and omit to consider FRBS comprehensibility. Only recently, the problem of finding the right trade-off between performance and comprehensibility, in spite of the original nature of fuzzy logic, has arisen a growing interest in methods which take both the aspects into account. In this paper, we propose a Pareto-based multi-objective evolutionary approach to generate a set of Mamdani fuzzy systems from numerical data. We adopt a variant of the well-known (2+2) Pareto Archived Evolutionary Strategy ((2+2)PAES), which adopts the one-point crossover and two appropriately defined mutation operators. (2+2)PAES determines an approximation of the optimal Pareto front by concurrently minimizing the root mean squared error and the complexity. Complexity is measured as sum of the conditions which compose the antecedents of the rules included in the FRBS. Thus, low values of complexity correspond to Mamdani fuzzy systems characterized by a low number of rules and a low number of input variables really used in each rule. This ensures a high comprehensibility of the systems. We tested our version of (2+2)PAES on three well-known regression benchmarks, namely the Box and Jenkins Gas Furnace, the Mackey-Glass chaotic time series and Lorenz attractor time series datasets. To show the good characteristics of our approach, we compare the Pareto fronts produced by the (2+2)PAES with the ones obtained by applying a heuristic approach based on SVD-QR decomposition and four different multi-objective evolutionary algorithms."
  },
  {
    "id": "41864",
    "input": "Generate a title for the following abstract of a paper: We present a method for learning bilingual translation lexicons from monolingual cor- pora. Word types in each language are charac- terized by purely monolingual features, such as context counts and orthographic substrings. Translations are induced using a generative model based on canonical correlation analy- sis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a va- riety of language pairs and from a range of corpus types. Mixture modeling is a general technique for making any simple model more expressive through weighted combination. This generality and simplicity in part explains the success of the Expectation Maximization (EM) algorithm, in which updates are easy to derive for a wide class of mixture models. However, the likelihood of a mixture model is non-convex, so EM has no known global convergence guarantees. Recently, method of moments approaches offer global guarantees for some mixture models, but they do not extend easily to the range of mixture models that exist. In this work, we present Polymom, an unifying framework based on method of moments in which estimation procedures are easily derivable, just as in EM. Polymom is applicable when the moments of a single mixture component are polynomials of the parameters. Our key observation is that the moments of the mixture model are a mixture of these polynomials, which allows us to cast estimation as a Generalized Moment Problem. We solve its relaxations using semidefinite optimization, and then extract parameters using ideas from computer algebra. This framework allows us to draw insights and apply tools from convex optimization, computer algebra and the theory of moments to study problems in statistical estimation. Simulations show good empirical performance on several models. Our goal is to create a convenient natural language interface for performing well-specified but complex actions such as analyzing data, manipulating text, and querying databases. However, existing natural language interfaces for such tasks are quite primitive compared to the power one wields with a programming language. To bridge this gap, we start with a core programming language and allow users to \"naturalize\" the core language incrementally by defining alternative, more natural syntax and increasingly complex concepts in terms of compositions of simpler ones. In a voxel world, we show that a community of users can simultaneously teach a common system a diverse language and use it to build hundreds of complex voxel structures. Over the course of three days, these users went from using only the core language to using the naturalized language in 85.9% of the last 10K utterances. While simulated game environments have greatly accelerated research in reinforcement learning, existing environments lack the open-domain realism of tasks in computer vision or natural language processing, which operate on artifacts created by humans in natural, organic settings. To foster reinforcement learning research in such settings, we introduce the World of Bits (WoB), a platform in which agents complete tasks on the Internet by performing low-level keyboard and mouse actions. The two main challenges are: (i) to curate a large, diverse set of interesting web-based tasks, and (ii) to ensure that these tasks have a well-defined reward structure and are reproducible despite the transience of the web. To do this, we develop a methodology in which crowdworkers create tasks defined by natural language questions and provide demonstrations of how to answer the question on real websites using keyboard and mouse; HTTP traffic is cached to create a reproducible offline approximation of the web site. Finally, we show that agents trained via behavioral cloning and reinforcement learning can successfully complete a range of our web-based tasks."
  },
  {
    "id": "412232",
    "input": "Generate a title for the following abstract of a paper: The problem of high sensitivity in modeling is well known. Small perturbations in the model parameters may result in large, undesired changes in the model behavior. A number of authors have considered the issue of sensitivity in feedforward neural networks from a probabilistic perspective. Less attention has been given to such issues in recurrent neural networks. In this article, we present a new recurrent neural network architecture, that is capable of significantly improved parameter sensitivity properties compared to existing recurrent neural networks. The new recurrent neural network generalizes previous architectures by employing alternative discrete-time operators in place of the shift operator normally used. An analysis of the model demonstrates the existence of parameter sensitivity in recurrent neural networks and supports the proposed architecture. The new architecture performs significantly better than previous recurrent neural networks, as shown by a series of simple numerical experiments. We consider the issue of parameter sensitivity in models based on alternative discrete-time operators (ADTOs). A generic first-order ADTO is proposed that encompasses all the known first-order ADTOs introduced so far in the literature. New bounds on the operator parameters are derived, and a new algorithm is given for optimally selecting the parameters to give minimum parameter sensitivity. The Vapnik\u2013Chervonenkis dimension (VC-dim) characterizes the sample learning complexity of a classification model and it is often used as an indicator for the generalization capability of a learning method. The VC-dim has been studied on common feed-forward neural networks, but it has yet to be studied on Graph Neural Networks (GNNs) and Recursive Neural Networks (RecNNs). This paper provides upper bounds on the order of growth of the VC-dim of GNNs and RecNNs. GNNs and RecNNs are from a new class of neural network models which are capable of processing inputs that are given as graphs. A graph is a data structure that generalizes the representational power of vectors and sequences, via the ability to represent dependencies or relationships between feature vectors. It was shown previously that the ability of recurrent neural networks to process sequences increases the VC-dim when compared to the VC-dim of Neural Networks, which are limited to processing vectors. Since graphs are a more general form than sequences, the question arises how this will affect the VC-dimension of GNNs and RecNNs. A main finding in this paper is that the upper bounds on the VC-dim for GNNs and RecNNs are comparable to the upper bounds for recurrent neural networks. The result also suggests that the generalization capability of such models increases with the number of connected nodes. Financial forecasting is an example of a signal processing problem which is challenging due to small sample sizes, high noise, non-stationarity, and non-linearity. Neural networks have been very successful in a number of signal processing applications. We discuss fundamental limitations and inherent difficulties when using neural networks for the processing of high noise, small sample size signals. We introduce a new intelligent signal processing method which addresses the difficulties. The method proposed uses conversion into a symbolic representation with a self-organizing map, and grammatical inference with recurrent neural networks. We apply the method to the prediction of daily foreign exchange rates, addressing difficulties with non-stationarity, overfitting, and unequal a priori class probabilities, and we find significant predictability in comprehensive experiments covering 5 different foreign exchange rates. The method correctly predicts the direction of change for the next day with an error rate of 47.1%. The error rate reduces to around 40% when rejecting examples where the system has low confidence in its prediction. We show that the symbolic representation aids the extraction of symbolic knowledge from the trained recurrent neural networks in the form of deterministic finite state automata. These automata explain the operation of the system and are often relatively simple. Automata rules related to well known behavior such as tr end following and mean reversal are extracted."
  },
  {
    "id": "41465",
    "input": "Generate a title for the following abstract of a paper: Detailed knowledge of a circuit's timing is essential for performance optimization, timing closure, and generation of test patterns to detect small-delay defects. When an input transition is applied to the circuit's inputs, the resulting delay is not only determined by the propagation path, but also influenced by the power-supply noise. We introduce a path-sensitization procedure which precisely controls the switching activity in the circuit region surrounding the path. The procedure can maximize or minimize switching activity, or set it to a user-specified value. We study the accuracy-vs.-efficiency trade-offs for a hierarchy of timing models, from coarse zero-delay assumption to a waveform-accurate approach with sub-cycle resolution. For the first time, we present a MaxSAT formulation which guarantees maximization or minimization of switching activity, stemming from transitions and from glitches, simultaneously with path sensitization. We validate the quality of the generated test patterns using a mixed-mode IR-drop-aware timing simulator. IR-drop effects are increasingly relevant in context of both design and test. We introduce the event-driven simulator MIRID that calculates the impact of IR-drop to the circuit timing. MIRID performs the simulation on two abstraction levels: timing effects in the gate-level net-list, current and voltage waveform propagation in the electrical model of the power-distribution network (PDN). Switching events at the logic gates are forwarded to the electrical model, where induced currents and their impact on the neighboring PDN nodes are computed. From this information, values of voltages at the Vdd and ground terminals of logic gates are determined, which in turn are used to calculate accurate switching delays of the gates. MIRID supports a generic interface to electrical models, allowing for a seamless integration of arbitrary models of PDN and gate timing. We report experiments based on a simple PDN model that was introduced previously and incorporates a pre-characterized library. The simulation accuracy is validated by matching the results from MIRID and SPICE. The detection of small-delay faults is traditionally performed by sensitizing transitions on a path of sufficient length from an input to an output of the circuit going through the fault site. While this approach allows efficient test generation algorithms, it may result in false positives and false negatives as well, i.e. undetected faults are classified as detected or detectable faults are classified as undetectable. We present an automatic test pattern generation algorithm which considers waveforms and their propagation on each relevant line of the circuit. The model incorporates individual delays for each gate and filtering of small glitches. The algorithm is based on an optimized encoding of the test generation problem by a Boolean satisfiability (SAT) instance and is implemented in the tool WaveSAT. Experimental results for ISCAS-85, ITC-99 and industrial circuits show that no known definition of path sensitization can eliminate false positives and false negatives at the same time, thus resulting in inadequate small-delay fault detection. WaveSAT generates a test if the fault is testable and is also capable of automatically generating a formal redundancy proof for undetectable small-delay faults; to the best of our knowledge this is the first such algorithm that is both scalable and complete. Hardening a circuit against soft errors should be performed in early design steps before the circuit is laid out. A viable approach to achieve soft error rate (SER) reduction at a reasonable cost is to harden only parts of a circuit. When selecting which locations in the circuit to harden, priority should be given to critical spots for which an error is likely to cause a system malfunction. The criticality of the spots depends on parameters not all available in early design steps. We employ a selection strategy which takes only gate-level information into account and does not use any low-level electrical or timing information. We validate the quality of the solution using an accurate SER estimator based on the new UGC particle strike model. Although only partial information is utilized for hardening, the exact validation shows that the susceptibility of a circuit to soft errors is reduced significantly. The results of the hardening strategy presented are also superior to known purely topological strategies in terms of both hardware overhead and protection."
  },
  {
    "id": "411663",
    "input": "Generate a title for the following abstract of a paper: Container loading problems consist of finding an appropriate way to load objects into a container. A few alternative algorithms have been defined based on different optimization approaches. Different algorithms also depend on the types of objects considered. In most of the cases, however, the shape of the objects is restricted to be orthogonal (i.e., standard boxes). In this paper, we consider the case where the shape of the objects can be any polyhedron. We propose an algorithm using local search and simulated annealing, and we show that for standard boxes, it has a similar performance than previously established methods. We consider a two-dimensional packing problem of non-convex polygons of different shapes on a sheet. The problem is thus how to determine positions for placing objects such that the waste is minimized. In order to solve this problem, we propose an adaptive meta-heuristics using partial optimization. This partial optimization algorithm makes subgroups of objects. Moreover, theses groups and non-grouped objects are allocated using adaptive meta-heuristics. This partial optimization algorithm determines the groups dynamically; therefore meta-heuristic search efficiency is improved. The performance of the proposed method is tested using simulation experiments. The result shows that this method is superior to the case when adaptive meta-heuristics alone are used An optimal packing problem of non-convex polygons on a sheet is considered and applied to a textile nesting problem. We propose\n a procedure of grouping objects and adaptive meta-heuristics. The grouping procedure classifies objects into groups which\n are then allocated on the sheet using adaptive meta-heuristics. As a result, effectiveness of the meta-heuristics is improved.\n The performance of the proposed method is compared with other meta-heuristics using simulation experiments. The result shows\n that the present method is superior to cases where adaptive meta-heuristics without grouping are used. The aim of the paper is to show two methods of optimal clustering based on general dissimilarity measures. Since these methods use objects and the pairwise dissimilarity alone and not the intrinsic properties of the space in which the objects are put, a network of objects are under consideration and therefore clustering is studied on this network. The first method uses the meta-heuristic techniques for optimization, which include local search, simulated annealing, genetic algorithms, and tabu search, have been applied to many real problems. The paper investigates their usefulness in optimal clustering. The above four meta-heuristic methods are applied and which of the various options in those methods are appropriate is investigated by numerical experiments. Moreover a parameterized objective function is considered and a suitable parameter is found. The second method is fuzzy clustering that is a variation of fuzzy c-means for Euclidean space. A relatively simple algorithm is developed and numerical examples are also discussed"
  },
  {
    "id": "411594",
    "input": "Generate a title for the following abstract of a paper: Several emerging wireline and wireless access network technologies exhibit asymmetry in their network characteristics. For instance, cable modem networks exhibit significant bandwidth asymmetry, while packet radio networks exhibit media access asymmetry. A high degree of asymmetry can have an adverse effect on the performance of feedback-based transport protocols. We study the effects of bandwidth and media access asymmetry on the performance of the TCP protocol. We identify the fundamental reasons for the mismatch between TCP and asymmetric networks, and then present several techniques to address this problem In this paper, we study the effects of network asymmetry on end\u2010to\u2010end TCP performance and suggest techniques to improve it. The networks investigated in this study include a wireless cable modem network and a packet radio network, both of which can form an important part of a mobile ad hoc network. In recent literature (e.g., [18]), asymmetry has been considered in terms of a mismatch in bandwidths in the two directions of a data transfer. We generalize this notion of bandwidth asymmetry to other aspects of asymmetry, such as latency and media\u2010access, and packet error rate, which are common in wide\u2010area wireless networks. Using a combination of experiments on real networks and simulation, we analyze TCP performance in such networks where the throughput achieved is not solely a function of the link and traffic characteristics in the direction of data transfer (the forward direction), but depends significantly on the reverse direction as well. We focus on bandwidth and latency asymmetries, and propose and evaluate several techniques to improve end\u2010to\u2010end performance. These include techniques to decrease the rate of acknowledgments on the constrained reverse channel (ack congestion control and ack filtering), techniques to reduce source burstiness when acknowledgments are infrequent (TCP sender adaptation), and algorithms at the reverse bottleneck router to schedule data and acks differently from FIFO (acks\u2010first scheduling). This document describes TCP performance problems that arise because of asymmetric effects. These problems arise in several access networks, including bandwidth-asymmetric networks and packet radio subnetworks, for different underlying reasons. However, the end result on TCP performance is the same in both cases: performance often degrades significantly because of imperfection and variability in the ACK feedback from the receiver to the sender. There has been much work on developing techniques for estimating the capacity and the available bandwidth of network paths based on end-point measurements. The focus has primarily been on settings where the constrained link can be modeled as a point-to-point link with a well-defined bandwidth, serving packets in FIFO order. In this paper, we point out that broadband access networks, such as cable modem and 802.11-based wireless networks, break this model in various ways. The constrained link could (a) employ mechanisms such as token bucket rate regulation, (b) schedule packets in a non-FIFO manner, and (c) support multiple distinct rates. We study how these characteristics impede the operation of the various existing methods and tools for capacity and available bandwidth estimation, and present a new available bandwidth estimation technique, Probe- Gap, that overcomes some of these difficulties. Our evaluation is based on experiments with actual 802.11a and cable modem links."
  },
  {
    "id": "411805",
    "input": "Generate a title for the following abstract of a paper: \u2022A new Harmony Search algorithm has been defined.\u2022Definition of a new improvisation phase involving a selection procedure.\u2022New memory consideration process based on a recombination operator.\u2022Large experimental phase using a suite of benchmark problems.\u2022Better results than the ones produced by other well-known HS algorithms. Reaction systems is a formalism inspired by chemical reactions introduced by Rozenberg and Ehrenfeucht. Recently, an evolutionary algorithm based on this formalism, called Evolutionary Reaction Systems, has been presented. This new algorithm proved to have comparable performances to other well-established machine learning methods, like genetic programming, neural networks and support vector machines on both artificial and real-life problems. Even if the results are encouraging, to make Evolutionary Reaction Systems an established evolutionary algorithm, an in depth analysis of the effect of its parameters on the search process is needed, with particular focus on those parameters that are typical of Evolutionary Reaction Systems and do not have a counterpart in traditional evolutionary algorithms. Here we address this problem for the first time. The results we present show that one particular parameter, between the ones tested, has a great influence on the performances of Evolutionary Reaction Systems, and thus its setting deserves practitioners' particular attention: the number of symbols used to represent the reactions that compose the system. Furthermore, this work represents a first step towards the definition of a set of default parameter values for Evolutionary Reaction Systems, that should facilitate their use for beginners or inexpert practitioners. This paper presents a new genetic programming framework called Evolutionary Reaction Systems. It is based on a recently defined computational formalism, inspired by chemical reactions, called Reaction Systems, and it has several properties that distinguish it from other existing genetic programming frameworks, making it interesting and worthy of investigation. For instance, it allows us to express complex constructs in a simple and intuitive way, and it lightens the final user from the task of defining the set of primitive functions used to build up the evolved programs. Given that Evolutionary Reaction Systems is new and it has small similarities with other existing genetic programming frameworks, a first phase of this work is dedicated to a study of some important parameters and their influence on the algorithm\u2019s performance. Successively, we use the best parameter setting found to compare Evolutionary Reaction Systems with other well established machine learning methods, including standard tree-based genetic programming. The presented results show that Evolutionary Reaction Systems are competitive with, and in some cases even better than, the other studied methods on a wide set of benchmarks. The relationship between generalization and solutions functional complexity in genetic programming (GP) has been recently investigated. Three main contributions are contained in this paper: (1) a new measure of functional complexity for GP solutions, called Graph Based Complexity (GBC) is defined and we show that it has a higher correlation with GP performance on out-of-sample data than another complexity measure introduced in a recent publication. (2) A new measure is presented, called Graph Based Learning Ability (GBLA). It is inspired by the GBC and its goal is to quantify the ability of GP to learn \"difficult\" training points; we show that GBLA is negatively correlated with the performance of GP on out-of-sample data. (3) Finally, we use the ideas that have inspired the definition of GBC and GBLA to define a new fitness function, whose suitability is empirically demonstrated. The experimental results reported in this paper have been obtained using three real-life multidimensional regression problems."
  },
  {
    "id": "412035",
    "input": "Generate a title for the following abstract of a paper: Estimating the position of a 3-dimensional world point given its 2-dimensional projections in a set of images is a key component in numerous computer vision systems. There are several methods dealing with this problem, ranging from sub-optimal, linear least square triangulation in two views, to finding the world point that minimized the L2-reprojection error in three views. This leads to the statistically optimal estimate under the assumption of Gaussian noise. In this paper we present a solution to the optimal triangulation in three views. The standard approach for solving the three-view triangulation problem is to find a closed-form solution. In contrast to this, we propose a new method based on an iterative scheme. The method is rigorously tested on both synthetic and real image data with corresponding ground truth, on a midrange desktop PC and a Raspberry Pi, a low-end mobile platform. We are able to improve the precision achieved by the closed-form solvers and reach a speed-up of two orders of magnitude compared to the current state-of-the-art solver. In numbers, this amounts to around 300K triangulations per second on the PC and 30K triangulations per second on Raspberry Pi. We propose a novel method for iterative learning of point correspondences between image sequences. Points moving on surfaces in 3D space are projected into two images. Given a point in either view, the considered problem is to determine the corresponding location in the other view. The geometry and distortions of the projections are unknown, as is the shape of the surface. Given several pairs of point sets but no access to the 3D scene, correspondence mappings can be found by excessive global optimization or by the fundamental matrix if a perspective projective model is assumed. However, an iterative solution on sequences of point-set pairs with general imaging geometry is preferable. We derive such a method that optimizes the mapping based on Neyman's chi-square divergence between the densities representing the uncertainties of the estimated and the actual locations. The densities are represented as channel vectors computed with a basis function approach. The mapping between these vectors is updated with each new pair of images such that fast convergence and high accuracy are achieved. The resulting algorithm runs in real time and is superior to state-of-the-art methods in terms of convergence and accuracy in a number of experiments. This paper describes a system for structure-and-motion estimation for real-time navigation and obstacle avoidance. We demonstrate a technique to increase the efficiency of the 5-point solution to the relative pose problem. This is achieved by a novel sampling scheme, where we add a distance constraint on the sampled points inside the RANSAC loop, before calculating the 5-point solution. Our setup uses the KLT tracker to establish point correspondences across time in live video. We also demonstrate how an early outlier rejection in the tracker improves performance in scenes with plenty of occlusions. This outlier rejection scheme is well suited to implementation on graphics hardware. We evaluate the proposed algorithms using real camera sequences with fine-tuned bundle adjusted data as ground truth. To strenghten our results we also evaluate using sequences generated by a state-of-the-art rendering software. On average we are able to reduce the number of RANSAC iterations by half and thereby double the speed. In this article, we deal with fast algorithms for the quaternionic Fourier transform (QFT). Our aim is to give a guideline for choosing algorithms in practical cases. Hence, we are not only interested in the theoretic complexity but in the real execution time of the implementation of an algorithm. This includes floating point multiplications, additions, index computations and the memory accesses. We mainly consider two cases: the QFT of a real signal and the QFT of a quaternionic signal. For both cases it follows that the row-column method yields very fast algorithms. Additionally, these algorithms are easy to implement since one can fall back on standard algorithms for the fast Fourier transform and the fast Hartley transform. The latter is the optimal choice for real signals since there is no redundancy in the transform. We take advantage of the fact that each complete transform can be converted into another complete transform. In the case of the complex Fourier transform, the Hartley transform, and the QFT, the conversions are of low complexity. Hence, the QFT of a real signal is optimally calculated using the Hartley transform."
  },
  {
    "id": "412386",
    "input": "Generate a title for the following abstract of a paper: We develop an orthogonal forward selection (OFS) approach to construct radial basis function (RBF) network classifiers for two-class problems. Our approach integrates several concepts in probabilistic modelling, including cross validation, mutual information and Bayesian hyperparameter fitting. At each stage of the OFS procedure, one model term is selected by maximising the leave-one-out mutual information (LOOMI) between the classifier's predicted class labels and the true class labels. We derive the formula of LOOMI within the OFS framework so that the LOOMI can be evaluated efficiently for model term selection. Furthermore, a Bayesian procedure of hyperparameter fitting is also integrated into the each stage of the OFS to infer the l^2-norm based local regularisation parameter from the data. Since each forward stage is effectively fitting of a one-variable model, this task is very fast. The classifier construction procedure is automatically terminated without the need of using additional stopping criterion to yield very sparse RBF classifiers with excellent classification generalisation performance, which is particular useful for the noisy data sets with highly overlapping class distribution. A number of benchmark examples are employed to demonstrate the effectiveness of our proposed approach. A novel two-stage construction algorithm for linear-in-the-parameters classifier is proposed, aiming at noisy two-class classification problems. The purpose of the first stage is to produce a prefiltered signal that is used as the desired output for the second stage to construct a sparse linear-in-the-parameters classifier. For the first stage learning of generating the prefiltered signal, a two-level algorithm is introduced to maximise the model's generalisation capability, in which an elastic net model identification algorithm using singular value decomposition is employed at the lower level while the two regularisation parameters are selected by maximising the Bayesian evidence using a particle swarm optimization algorithm. Analysis is provided to demonstrate how ''Occam's razor'' is embodied in this approach. The second stage of sparse classifier construction is based on an orthogonal forward regression with the D-optimality algorithm. Extensive experimental results demonstrate that the proposed approach is effective and yields competitive results for noisy data sets. An efficient data based-modeling algorithm for nonlinear system identification is introduced for radial basis function (RBF) neural networks with the aim of maximizing generalization capability based on the concept of leave-one-out (LOO) cross validation. Each of the RBF kernels has its own kernel width parameter and the basic idea is to optimize the multiple pairs of regularization parameters and kernel widths, each of which is associated with a kernel, one at a time within the orthogonal forward regression (OFR) procedure. Thus, each OFR step consists of one model term selection based on the LOO mean square error (LOOMSE), followed by the optimization of the associated kernel width and regularization parameter, also based on the LOOMSE. Since like our previous state-of-the-art local regularization assisted orthogonal least squares (LROLS) algorithm, the same LOOMSE is adopted for model selection, our proposed new OFR algorithm is also capable of producing a very sparse RBF model with excellent generalization performance. Unlike our previous LROLS algorithm which requires an additional iterative loop to optimize the regularization parameters as well as an additional procedure to optimize the kernel width, the proposed new OFR algorithm optimizes both the kernel widths and regularization parameters within the single OFR procedure, and consequently the required computational complexity is dramatically reduced. Nonlinear system identification examples are included to demonstrate the effectiveness of this new approach in comparison to the well-known approaches of support vector machine and least absolute shrinkage and selection operator as well as the LROLS algorithm. We propose a unified data modeling approach that is equally applicable to supervised regression and classification applications, as well as to unsupervised probability density function estimation. A particle swarm optimization (PSO) aided orthogonal forward regression (OFR) algorithm based on leave-one-out (LOO) criteria is developed to construct parsimonious radial basis function (RBF) networks with tunable nodes. Each stage of the construction process determines the center vector and diagonal covariance matrix of one RBF node by minimizing the LOO statistics. For regression applications, the LOO criterion is chosen to be the LOO mean square error, while the LOO misclassification rate is adopted in two-class classification applications. By adopting the Parzen window estimate as the desired response, the unsupervised density estimation problem is transformed into a constrained regression problem. This PSO aided OFR algorithm for tunable-node RBF networks is capable of constructing very parsimonious RBF models that generalize well, and our analysis and experimental results demonstrate that the algorithm is computationally even simpler than the efficient regularization assisted orthogonal least square algorithm based on LOO criteria for selecting fixed-node RBF models. Another significant advantage of the proposed learning procedure is that it does not have learning hyperparameters that have to be tuned using costly cross validation. The effectiveness of the proposed PSO aided OFR construction procedure is illustrated using several examples taken from regression and classification, as well as density estimation applications."
  },
  {
    "id": "41582",
    "input": "Generate a title for the following abstract of a paper: Application Specific Programmable Processors (ASPP) provide efficient implementation for any of $m$ specified functionalities. Due to their flexibility and convenient performance-cost trade-offs, ASPPs are being developed by DSP, video, multimedia, and embedded IC manufacturers. In this paper, we present two low-cost approaches to graceful degradation-based permanent fault tolerance of ASPPs. ASPP fault tolerance constraints are incorporated during scheduling, allocation, and assignment phases of behavioral synthesis. Graceful degradation is supported by implementing multiple schedules of the ASPP applications, each with a different throughput constraint. In this paper, we do not consider concurrent error detection. The first ASPP fault tolerance technique minimizes the hardware resources while guaranteeing that the ASPP remains operational in the presence of all k-unit faults. On the other hand, the second fault tolerance technique maximizes the ASPP fault tolerance subject to constraints on the hardware resources. These ASPP fault tolerance techniques impose several unique tasks, such as fault-tolerant scheduling, hardware allocation, and application-to-faulty-unit assignment. We address each of them and demonstrate the effectiveness of the overall approach, the synthesis algorithms, and software implementations on a number of industrial-strength designs. In this paper, behavioral-level synthesis techniques are presented for the design of reconfigurable hardware. The techniques are applicable for synthesis of several classes of designs, including 1) design for fault tolerance against permanent faults, 2) design for improved manufacturability, and 3) design of application specific programmable processors (ASPP's)--processors designed to perform any computation from a specified set on a single implementation platform. This paper focuses on design techniques for efficient built-in self-repair (BISR), and thus directly addresses the former two applications. Previous BISR techniques have been based on replacing a failed module with a backup of the same type. We present new heterogeneous BISR methodologies which remove this constraint and enable replacement of a module with a spare of a different type. The approach is based on the flexibility of behavioral-level synthesis to explore the design space. Two behavioral synthesis techniques are developed; the first method is through assignment and scheduling, and the second utilizes transformations. Experimental results verify the effectiveness of the approaches. High level synthesis techniques for the synthesis of restructurable datapaths are introduced. The techniques can be used in applications such as design for fault tolerance against permanent faults, design for yield improvement, and design of application specific programmable processors. The paper focuses on design techniques for built in self repair (BISR), which addresses the first two of these applications. The new BISR methodology consists of two approaches which exploit the design space exploration abilities of high level synthesis. The first method uses resource allocation, assignment, and scheduling, and the second uses transformations. The effectiveness of the approaches are verified on a set of benchmark examples. Fault tolerance (FT) is essential in many Internet of Things (IoT) applications, in particular in the domains such as medical devices and automotive systems where a single fault in the system can lead to serious consequences. Non-volatile memory (NVM), on the other hand, is commonly used to improve system reliability due to its unique properties to retain data even if the power supply is lost. However, one of the most important drawbacks of NVM is that it imposes significant overhead regarding timing and energy. In this paper, we have proposed a unique technique with the use of NVM to create FT application specific architecture with almost no timing overhead and low energy overhead. We address the implementation of applications that are specified using synchronous data flow model of computation. We combine the use of NVM and classical CMOS transistors so that NVM judiciously stores selected complete states of the pertinent program. It allows the program to resume from the saved state in NVM when faults occur. The frequency of the state selection can be flexibly adjusted for an arbitrarily specified FT timing/energy overhead. Moreover, to find an optimal state selection (with low overhead), we have applied an improved min-cut max-flow algorithm. On a variety of typical benchmarks, the simulation results indicate that our approach incurs only a small overhead over lower bounds. It is also generic in a sense that it can be applied to a wide spectrum of underlying IoT architectures and computational models."
  },
  {
    "id": "411125",
    "input": "Generate a title for the following abstract of a paper: Balancing privacy and security concerns in biometric systems is an area of growing importance. While important work has gone on in template protection and revocable biometric tokens, these avenues of research address only one aspect of the problem. Such research does not address a critical issue: balancing the need government and anti-fraud programs to do deduplication (ensure one identity per person) against the potential for abuse using that data. Any existing system capable of deduplication, even if using a template protection scheme, would allow function creep or abuse by searching with latent prints. This paper introduces the concept of id-privacy, requiring at least i items (e.g. fingers) to be provided to resolve identity to better than d above random chance. We show how using cross-finger representation on unsegmented fingerprint slap data, we can address what may be the single most important \u201cprivacy\u201d issue in biometrics, privacy enhanced deduplication. We prove we can achieve (2,0)-id-privacy for fingerprint-based deduplication while preventing searching with a latent print. We introduce the Forest Finger algorithm - an approach for matching unsegmented slaps and cross-finger representations. Our results on the largest public slap database shows superior accuracy when compared with existing NIST Bozorth matcher when tested on unsegmented slaps, segmented prints or fused rolled prints. This paper reviews the biometric dilemma, the pending threat that may limit the long-term value of biometrics in security applications. Unlike passwords, if a biometric database is ever compromised or improperly shared, the underlying biometric data cannot be changed. The concept of revocable or cancelable biometric-based identity tokens (biotokens), if properly implemented, can provide significant enhancements in both privacy and security and address the biometric dilemma.The key to effective revocable biotokens is the need to support the highly accurate approximate matching needed in any biometric system as well as protecting privacy/security of the underlying data. We briefly review prior work and show why it is insufficient in both accuracy and security.This paper adapts a recently introduced approach that separates each datum into two fields, one of which is encoded and one which is left to support the approximate matching. Previously applied to faces, this paper uses this approach to enhance an existing fingerprint system. Unlike previous work in privacy-enhanced biometrics, our approach improves the accuracy of the underlying system! The security analysis of these biotokens includes addressing the crucial issue of protection of small fields.The resulting algorithm is tested on three different fingerprint verification challenge datasets and shows an average decrease in the Equal Error Rate of over 30% - providing improved security and improved privacy. This paper examines a novel security model for voice biometrics that decomposes the overall problem into bits of \u201cbiometric identity security,\u201d bits of \u201cknowledge security,\u201d and bits of \u201ctraditional encryption security.\u201d This is the first paper to examine balancing security gained from text-dependent and text-independent voice biometrics under this model. Our formulation allows for text-dependent voice biometrics to address both what you know and who you are. A text-independent component is added to defeat replay attacks. Further, we experimentally examine an extension of the recently introduced Vaulted Voice Verification protocol and the security tradeoffs of adding these elements. We show that by mixing text-dependent with text-independent voice verification and by expanding the challenge-response protocol, Vaulted Voice Verification can preserve privacy while addressing the problematic issues of voice as a remote/mobile biometric identifier. The resulting model supports both authentication and key release with the matching taking place client side, where a mobile device may be used. This novel security model addresses a real and crucial problem: that of security on a mobile device. While modern research in face recognition has focused on new feature representations, alternate learning methods for fusion of features, most have ignored the issue of unmodeled correlations in face data when combining diverse features such as similar visual regions, attributes, appearance frequency, etc. Conventional wisdom is that by using sufficient data and machine, one can learn the systematic correlations and use the data to form a more robust basis for core recognition tasks like verification, identification, and clustering. This however, takes large amounts of training data which is not really available for personal consumer photo collections. We address the fusion/correlation issue differently by proposing an ensemble-based approach that is built on different information sources such as facial appearance, visual context, and social (or co-occurrence) information of samples in a dataset, to provide higher classification accuracy for lace recognition in consumer photo collections, To evaluate the utility of our ensembles and simultaneously generate stronger generic features, we perform two experiments - (i) a verification experiment on the standard unconstrained LFW (Labeled Faces in the Wild) dataset where by using an ensemble of appearance related features we report comparable results with recently reported state-of-the-art results and 2.9% better classification accuracy than the previous best method, and (ii) experiment on the Gallagher personal photo collection where we demonstrate at least 17% relative performance gain using visual context and social co-occurrence ensembles."
  },
  {
    "id": "41238",
    "input": "Generate a title for the following abstract of a paper: Consider the situation that you have a data model, a functional model and a process model of a system, perhaps made by different analysts at different times. Are these models consistent with each other? A relevant question in practice - and therefore we think it should also be addressed in our courses. However, UML modelling textbooks don't discuss it, so we developed our own teaching materials. In this position paper we explain why and how. Software architects are responsible for designing an architectural solution that satisfies the functional and non-functional requirements of the system to the fullest extent possible. However, the details they need to make informed architectural decisions are often missing from the requirements specification. An earlier study we conducted indicated that architects intuitively recognize architecturally significant requirements in a project, and often seek out relevant stakeholders in order to ask Probing Questions (PQs) that help them acquire the information they need. This paper presents results from a qualitative interview study aimed at identifying architecturally significant functional requirements' categories from various business domains, exploring relevant PQs for each category, and then grouping PQs by type. Using interview data from 14 software architects in three countries, we identified 15 categories of architecturally significant functional requirements and 6 types of PQs. We found that the domain knowledge of the architect and her experience influence the choice of PQs significantly. A preliminary quantitative evaluation of the results against real-life software requirements specification documents indicated that software specifications in our sample largely do not contain the crucial architectural differentiators that may impact architectural choices and that PQs are a necessary mechanism to unearth them. Further, our findings provide the initial list of PQs which could be used to prompt business analysts to elicit architecturally significant functional requirements that the architects need. Failure to identify and analyze architecturally significant functional and non-functional requirements (NFRs) early on in the life cycle of a project can result in costly rework in later stages of software development. While NFRs indicate an explicit architectural impact, the impact that functional requirements may have on architecture is often implicit. The skills needed for capturing functional requirements are different than those needed for making architectural decisions. As a result, these two activities are often conducted by different teams in a project. Therefore it becomes necessary to integrate the knowledge gathered by people with different expertise to make informed architectural decisions. We present a study to bring out that functional requirements often have implicit architectural impact and do not always contain comprehensive information to aid architectural decisions. Further, we present our initial work on automating the identification of architecturally significant functional requirements from requirements documents and their classification into categories based on the different kinds of architectural impact they can have. We believe this to be a crucial precursor for recommending specific design decisions. We envisage ArcheR, a tool that (a) automates the identification of architecturally significant functional requirements from requirement specification documents, (b) classify them into categories based on the different kinds of architectural impact they can have, (c) recommend probing questions the business analyst should ask in order to produce a more complete requirements specification, and (d) recommend possible architectural solutions in response to the architectural impact.\n\n Software requirements specifications (SRSs) often lack the detail needed to make informed architectural decisions. Architects therefore either make assumptions, which can lead to incorrect decisions, or conduct additional stakeholder interviews, resulting in potential project delays. We previously observed that software architects ask Probing Questions (PQs) to gather information crucial to architectural decision-making. Our goal is to equip Business Analysts with appropriate PQs so that they can ask these questions themselves. We report a new study with over 40 experienced architects to identify reusable PQs for five areas of functionality and organize them into structured flows. These PQ-flows can be used by Business Analysts to elicit and specify architecturally relevant information. Additionally, we leverage machine learning techniques to determine when a PQ-flow is appropriate for use in a project, and to annotate individual PQs with relevant information extracted from the existing SRS. We trained and evaluated our approach on over 8,000 individual requirements from 114 requirements specifications and also conducted a pilot study to validate its usefulness.\n\n"
  },
  {
    "id": "411012",
    "input": "Generate a title for the following abstract of a paper: In 2003, Brewka, Niemela and Truszczynski introduced answer-set optimization problems. They consist of two components: a logic program and a set of preference rules. Answer sets of the program represent possible outcomes; preferences determine a preorder on them. Of interest are answer sets of the program that are optimal with respect to the preferences. In this work, we consider computational problems concerning optimal answer sets. We implement and study several methods for the problems of computing an optimal answer set; computing another one, once the first one is found; and computing an optimal answer set that is similar to (respectively, dissimilar from) a given interpretation. For the problems of the existence of similar and dissimilar optimal answer set we establish their computational complexity. In this paper we introduce the notion of anF-program, whereF is a collection of formulas. We then study the complexity of computing withF-programs.F-programs can be regarded as a generalization of standard logic programs. Clauses (or rules) ofF-programs are built of formulas fromF. In particular, formulas other than atoms are allowed as \u201cbuilding blocks\u201d ofF-program rules. Typical examples ofF are the set of all atoms (in which case the class of ordinary logic programs is obtained), the set of all literals (in this case, we get the class of logic programs with classical negation [9]), the set of all Horn clauses, the set of all clauses, the set of all clauses with at most two literals, the set of all clauses with at least three literals, etc. The notions of minimal and stable models [16, 1, 7] of a logic program have natural generalizations to the case ofF-programs. The resulting notions are called in this paperminimal andstable answer sets. We study the complexity of reasoning involving these notions. In particular, we establish the complexity of determining the existence of a stable answer set, and the complexity of determining the membership of a formula in some (or all) stable answer sets. We study the complexity of the existence of minimal answer sets, and that of determining the membership of a formula in all minimal answer sets. We also list several open problems. We propose and study a technique to improve the performance of those local-search SAT solvers that proceed by executing a prespecified number of tries, each starting with an element of the space of all truth assignments and performing a prespecified number of local-search steps (flips). Based on the input theory T, our method first constructs a collection of its relaxations, that is, theories whose models are easy to compute and \"almost\" satisfy T. It then uses a local-search algorithm to compute models of the relaxed theories and, finally, uses these models as starting points for tries when execut- ing the local search algorithm on T. To construct relaxations our method takes advantage of high-level representation of search problems, which separate the specification of a search problem from the de- scription of its particular instances. The method is general. We applied it to WSAT, a local-search SAT solver for CNF theories, and to WSAT(cc), a local-search SAT algorithm for theories in the language of propositional logic with cardinality constraints. Experimental results demonstrate its effectiveness for both local-search algorithms we studied. We propose and study a general technique to improve the performance of certain classes of local-search SAT solvers in computing solutions to instances of search problems. Our method takes advantage of high- level representations of search problems, which separate the specification of a search problem from the description of its particular instances. Typically, in order to solve a search problem for an instance I, we construct a propositional theory T so that models of T determine solutions. We then use a SAT solver to compute models ofT and reconstruct from them the corresponding solutions. However, once a propositional theory representing a search problem and its instance is formed, the connection to the search problem and the structure of its instances is no longer clear. Consequently, SAT solvers view these theories as if they were arbitrary propositional theories. They do not take advantage of the specification of the search problem and properties of the structure of its instances to fine-tune the way they compute models. Recently, researchers recognized that and proposed formalisms for representing search problems and their instances based on predicate logic (5,6) and the existential fragment of second-order logic (2). A search problem (more precisely, a set of constraints specifying it) is represented by a collection of clauses (program) in the logic P , and an instance I (input data to the problem ) is given as a collection DI of ground atoms, that is, an instance of some prespecified relational schema. To obtain a propositional representation of the search problem and its instance I, we ground the program P with respect to constants specified in the data set DI . This approach to building propositional representations of instances of search problems makes explicit the constraints of a search problem and the data instance schema. Consequently, it opens a possibility to design SAT solvers that can exploit properties of the problem statement and of the structure of the data instances. Two such solvers were proposed recently. (2) described a method that delays certain constraints based on the analysis of the problem specification. (6) introduced an approach to compute models directly from a predicate-logic specification of constraints and a description of a data instance. In this paper, we present a method that takes advantage of high-level program-data representations of search problems as data-program pairs in the logic PS+ (5) to improve the performance of local-search SAT solvers for propositional theories, possibly extended with boolean combinations of pseudo-boolean constraints. Local-search satisfiability methods (we refer to (8) for more details on that topic) that we are interested in proceed by executing a prespecified number, nt , of tries. A try starts with a random element of the space of all truth assignments and performs a prespecified number, nf , of flips . Each flip modifies the current truth assignment by changing truth values of a small number of atoms so that to optimize some objective function, for instance to minimize the number of clauses that become unsatisfied after the flip. If In this paper, we consider the question of skeptical reasoning for an impor- tant nonmonotonic reasoning system \u2014 the autoepistemic logic of Moore. Autoepistemic logic is a method of reasoning which assigns to a set of for- mulas the collection of theories called stable expansions. A naive method to perform skeptical autoepistemic reasoning \u2014 deciding whether a given formula ' belongs to all expansions of a theory \u2014 is to compute first all ex- pansions and then check whether ' belongs to each of them. This approach to skeptical autoepistemic reasoning is however prohibitively inefficient. The goal of this paper is to propose a different approach to computing intersection of all expansions of a theory. Our approach does not require us to compute any expansion of a theory. It reduces the question of membership in the intersection of all expansions to the question of propositional provability. More precisely, we describe a method that assigns to a modal theory I a propositional theory PI and to a modal-free formula ' another formula '' in such a manner that ' is in the intersection of all expansions of I if and only if PI \u22a2 ''. In general, the theory PI is much larger than the original theory I. We have found, however, several cases when it is not so and the size of the theory PI is a polynomial in the size ofI. These classes of theories are closely related to logic programs and disjunctive logic programs. Consequently, we obtain methods to check whether an atom is in the intersection of all supported (or stable) models of a (disjunctive) logic program, as well as numerous complexity results."
  },
  {
    "id": "4149",
    "input": "Generate a title for the following abstract of a paper: External difference families (EDFs) are a type of new combinatorial designs originated from cryptography. In this paper, some earlier ideas of recursive and cyclotomic constructions of combinatorial designs are extended, and a number of classes of EDFs and disjoint difference families are presented. A link between a subclass of EDFs and a special type of (almost) difference sets is set up. Almost difference families (ADFs) are a useful generalization of almost difference sets (ADSs). In this paper, we present some constructive techniques to obtain ADFs and establish a number of infinite classes of ADFs. Our results can be regarded as a generalization of the known difference families. It is clear that ADFs give partially balance incomplete block designs which arise in a natural way in many combinatorial and statistical problems. Codebooks (also called signal sets) meeting the Welch bounds are desirable in code-division multiple-access (CDMA) systems. In 2003, binary codebooks meeting Welch's bounds were constructed using difference sets by Ding, Golin, and Kloslashve. Recently, a generic construction of codebooks with cyclic difference sets meeting Welch's bound on the maximum cross-correlation amplitude was developed by Xia In this correspondence, the idea of Xia is extended, and related constructions of optimal codebooks with both cyclic and noncyclic difference sets are presented. These codebooks are optimal in the sense that they also meet this Welch bound. In addition, complex codebooks that almost meet Welch's bound on the maximum cross-correlation amplitude are also constructed with almost difference sets AbstractA classical method of constructing a linear code over GF(q) with a t-design is to use the incidence matrix of the t-design as a generator matrix over GF(q) of the code. This approach has been extensively investigated in the literature. In this paper, a different method of constructing linear codes using specific classes of 2-designs is studied, and linear codes with a few weights are obtained from almost difference sets, difference sets, and a type of 2-designs associated to semibent functions. Two families of the codes obtained in this paper are optimal. The linear codes presented in this paper have applications in secret sharing and authentication schemes, in addition to their applications in consumer electronics, communication and data storage systems. A coding-theory approach to the characterization of highly nonlinear Boolean functions is presented."
  },
  {
    "id": "41637",
    "input": "Generate a title for the following abstract of a paper: This paper addresses the problem of finding glass objects in images. Visual cues obtained by combining the systematic distortions in background texture occurring at the boundaries of transparent objects with the strong highlights typical of glass surfaces are used to train a hierarchy of classifiers, identify glass edges, and find consistent support regions for these edges. Qualitative and quantitative experiments involving a number of different classifiers and real images are presented. This paper addresses the problem of finding objects made of glass (or other transparent materials) in images. Since the appearance of glass objects depends for the most part on what lies behind them, we propose to use binary criteria (\"are these two regions made of the same material?\") rather than unary ones (\"is this glass?\") to guide the segmentation process. Concretely, we combine two complementary measures of affinity between regions made of the same material and discrepancy between regions made of different ones into a single objective function, and use the geodesic active contour framework to minimize this function over pixel labels. The proposed approach has been implemented, and qualitative and quantitative experimental results are presented. This paper addresses the problem of estimating the motion of a camera as it observes the outline (or apparent contour) of a solid bounded by a smooth surface in successive image frames. In this context, the surface points that project onto the outline of an object depend on the viewpoint and the only true correspondences between two outlines of the same object are the projections of frontier points where the viewing rays intersect in the tangent plane of the surface. In turn, the epipolar geometry is easily estimated once these correspondences have been identified. Given the apparent contours detected in an image sequence, a robust procedure based on RANSAC and a voting strategy is proposed to simultaneously estimate the camera configurations and a consistent set of frontier point projections by enforcing the redundancy of multiview epipolar geometry. The proposed approach is, in principle, applicable to orthographic, weak-perspective, and affine projection models. Experiments with nine real image sequences are presented for the orthographic projection case, including a quantitative comparison with the ground-truth data for the six data sets for which the latter information is available. Sample visual hulls have been computed from all image sequences for qualitative evaluation. This paper addresses the problem of recognizing three-dimensional objects bounded by smooth curved surfaces from image contours found in a single photograph. The proposed approach is based on a viewpoint-invariant relationship between object geometry and certain image features under weak perspective projection. The image features themselves are viewpoint-dependent. Concretely, the set of all possible silhouette bitangents, along with the contour points sharing the same tangent direction, is the projection of a one-dimensional set of surface points where each point lies on the occluding contour for a five-parameter family of viewpoints. These image features form a one-parameter family of equivalence classes, and it is shown that each class can be characterized by a set of numerical attributes that remain constant across the corresponding five-dimensional set of viewpoints. This is the basis for describing objects by \u201cinvariant\u201d curves embedded in high-dimensional spaces. Modeling is achieved by moving an object in front of a camera and does not require knowing the object-to-camera transformation; nor does it involve implicit or explicit three-dimensional shape reconstruction. At recognition time, attributes computed from a single image are used to index the model database, and both qualitative and quantitative verification procedures eliminate potential false matches. The approach has been implemented and examples are presented."
  },
  {
    "id": "412387",
    "input": "Generate a title for the following abstract of a paper: Artificial Intelligence (AI) has long dealt with the issue of finding a suitable formalization for commonsense reasoning. Defeasible argumentation has proven to be a successful approach in many respects, proving to be a con- fluence point for many alternative logical frameworks. Dierent formalisms have been developed, most of them sharing the common notions of argument and warrant. In defeasible argumentation, an argument is a tentative (de- feasible) proof for reaching a conclusion. An argument is warranted when it ultimately prevails over other conflicting arguments. In this context, defeasi- ble consequence relationships for modelling argument and warrant as well as their logical properties have gained particular attention. This article analyzes two non-monotonic inference operators Carg and Cwar intended for modelling argument construction and dialectical analysis (warrant), respectively. As a basis for such analysis we will use the LDSar framework, a unifying approach to computational models of argument using Labelled Deductive Systems (LDS). In the context of this logical framework, we show how labels can be used to represent arguments as well as argument trees, facilitating the definition and study of non-monotonic inference op- erators, whose associated logical properties are studied and contrasted. We contend that this analysis provides useful comparison criteria that can be extended and applied to other argumentation frameworks. Mathematics Subject Classification (2000). Primary 03B22; Secondary 03B42. Possibilistic Defeasible Logic Programming (P-DeLP) is a logic programming language which combines features from argumentation theory and logic programming, incorporating as well the treatment of possibilistic uncertainty and fuzzy knowledge at object-language level. Defeasible argumentation in general and P-DeLP in particular provide a way of modelling non-monotonic inference. From a logical viewpoint, capturing defeasible inference relationships for modelling argument and warrant is particularly important, as well as the study of their logical properties. This paper analyzes two non-monotonic operators for P-DeLP which model the expansion of a given program $\\mathcal{P}$ by adding new weighed facts associated with argument conclusions and warranted literals, resp. Different logical properties for the proposed expansion operators are studied and contrasted with a traditional SLD-based Horn logic. We will show that this analysis provides useful comparison criteria that can be extended and applied to other argumentation frameworks. Possibilistic Defeasible Logic Programming (P-DeLP) is a logic programming language which combines features from argumentation theory and logic programming, incorporating as well the treatment of possibilistic uncertainty and fuzzy knowledge at object-language level. Defeasible argumentation in general and P-DeLP in particular provide a way of modelling non-monotonic inference. From a logical viewpoint, capturing defeasible inference relationships for modelling argument and warrant is particularly important, as well as the study of their logical properties. This paper analyzes a non-monotonic operator for P-DeLP which models the expansion of a given program P by adding new weighed facts associated with warranted literals. Different logical properties are studied and contrasted with a traditional SLD-based Horn logic, providing useful comparison criteria that can be extended and applied to other argumentation frameworks. In the last decade defeasible argumentation frameworks have evolved to become a sound setting to formalize commonsense, qualitative reasoning. The logic programming paradigm has shown to be particularly useful for developing different argument-based frameworks on the basis of different variants of logic programming which incorporate defeasible rules. Most of such frameworks, however, are unable to deal with both explicit uncertainty and vague knowledge, as defeasibility is directly encoded in the object language. This paper presents possibilistic defeasible logic programming (P-DeLP), a new logic programming language which combines features from argumentation theory and logic programming, incorporating as well the treatment of possibilistic uncertainty. Such features are formalized on the basis of PGL, a possibilistic logic based on Godel fuzzy logic. One of the applications of P-DeLP is providing an intelligent agent with non-monotonic, argumentative inference capabilities. In this paper we also provide a better understanding of such capabilities by defining two non-monotonic operators which model the expansion of a given program by adding new weighed facts associated with argument conclusions and warranted literals, respectively. Different logical properties for the proposed operators are studied."
  },
  {
    "id": "412013",
    "input": "Generate a title for the following abstract of a paper: Suppose that we are given two independent sets I-b and I-r of a graph such that vertical bar I-b vertical bar = vertical bar I-r vertical bar, and imagine that a token is placed on each vertex in I-b. Then, the sliding token problem is to determine whether there exists a sequence of independent sets which transforms I-b and I-r so that each independent set in the sequence results from the previous one by sliding exactly one token along an edge in the graph. This problem is known to be PSPACE-complete even for planar graphs, and also for bounded treewidth graphs. In this paper, we show that the problem is solvable for trees in quadratic time. Our proof is constructive: for a yes-instance, we can find an actual sequence of independent sets between I-b and I-r whose length (i. e., the number of token-slides) is quadratic. We note that there exists an infinite family of instances on paths for which any sequence requires quadratic length. Suppose that we are given two independent sets I-0 and I-r of a graph such that vertical bar I-0 vertical bar = vertical bar I-r vertical bar, and imagine that a token is placed on each vertex in I-0. Then, the token jumping problem is to determine whether there exists a sequence of independent sets which transforms I-0 into I-r so that each independent set in the sequence results from the previous one by moving exactly one token to another vertex. Therefore, all independent sets in the sequence must be of the same cardinality. This problem is PSPACE-complete even for planar graphs with maximum degree three. In this paper, we first show that the problem is W[1]-hard when parameterized only by the number of tokens. We then give an FPT algorithm for general graphs when parameterized by both the number of tokens and the maximum degree. Our FPT algorithm can be modified so that it finds an actual sequence of independent sets between I-0 and I-r with the minimum number of token movements. Suppose that we are given two independent sets I-0 and I-r of a graph such that vertical bar I-0 vertical bar = vertical bar I-r vertical bar, and imagine that a token is placed on each vertex in I0. The token jumping problem is to determine whether there exists a sequence of independent sets of the same cardinality which transforms I-0 into Ir so that each independent set in the sequence results from the previous one by moving exactly one token to another vertex. This problem is known to be PSPACE-complete even for planar graphs of maximum degree three, and W[1]-hard for general graphs when parameterized by the number of tokens. In this paper, we present a fixedparameter algorithm for token jumping on planar graphs, where the parameter is only the number of tokens. Furthermore, the algorithm can be modified so that it finds a shortest sequence for a yes-instance. The same scheme of the algorithms can be applied to a wider class of graphs which forbid a complete bipartite graph K-3,K- t as a subgraph for a fixed integer t >= 3. Suppose that we are given two dominating sets D s and D t of a graph G whose cardinalities are at most a given threshold k. Then, we are asked whether there exists a sequence of dominating sets of G between D s and D t such that each dominating set in the sequence is of cardinality at most k and can be obtained from the previous one by either adding or deleting exactly one vertex. This decision problem is known to be PSPACE-complete in general. In this paper, we study the complexity of this problem from the viewpoint of graph classes. We first prove that the problem remains PSPACE-complete even for planar graphs, bounded bandwidth graphs, split graphs, and bipartite graphs. We then give a general scheme to construct linear-time algorithms and show that the problem can be solved in linear time for cographs, forests, and interval graphs. Furthermore, for these tractable cases, we can obtain a desired sequence if it exists such that the number of additions and deletions is bounded by O ( n ) , where n is the number of vertices in the input graph."
  },
  {
    "id": "411389",
    "input": "Generate a title for the following abstract of a paper: Let G be a 2-edge-connected simple graph on n vertices, let A denote an abelian group with the identity element 0, and let D be an orientation of G. The boundary of a function f:E(G)->A is the function @?f:V(G)->A given by @?f(v)[email protected]?\"e\"@?\"E\"^\"+\"(\"v\")f(e)[email protected]?\"e\"@?\"E\"^\"-\"(\"v\")f(e), where E^+(v) is the set of edges with tail v and E^-(v) is the set of edges with head v. A graph G is A-connected if for every b:V(G)->A with @?\"v\"@?\"V\"(\"G\")b(v)=0, there is a function f:E(G)->A-{0} such that @?f=b. In this paper, we prove that if d(x)+d(y)>=n for each [email protected]?E(G), then G is not Z\"3-connected if and only if G is either one of 15 specific graphs or one of K\"2\",\"n\"-\"2,K\"3\",\"n\"-\"3,K\"2\",\"n\"-\"2^+ or K\"3\",\"n\"-\"3^+ for n>=6, where K\"r\",\"s^+ denotes the graph obtained from K\"r\",\"s by adding an edge joining two vertices of maximum degree. This result generalizes the result in [G. Fan, C. Zhou, Degree sum and Nowhere-zero 3-flows, Discrete Math. 308 (2008) 6233-6240] by Fan and Zhou. Let G be a 2-edge-connected simple graph on n=3 vertices and A an abelian group with |A|=3. If a graph G^* is obtained by repeatedly contracting nontrivial A-connected subgraphs of G until no such a subgraph left, we say G can be A-reduced to G^*. Let G\\\"5 be the graph obtained from K\\\"4 by adding a new vertex v and two edges joining v to two distinct vertices of K\\\"4. In this paper, we prove that for every graph G satisfying max{d(u),d(v)}=n2 where uv@?E(G), G is not Z\\\"3-connected if and only if G is isomorphic to one of twenty two graphs or G can be Z\\\"3-reduced to K\\\"3, K\\\"4 or K\\\"4^- or G\\\"5. Our result generalizes the former results in [R. Luo, R. Xu, J. Yin, G. Yu, Ore-condition and Z\\\"3-connectivity, European J. Combin. 29 (2008) 1587-1595] by Luo et al., and in [G. Fan, C. Zhou, Ore condition and nowhere zero 3-flows, SIAM J. Discrete Math. 22 (2008) 288-294] by Fan and Zhou. Let G be a 2-edge-connected undirected graph, A be an (additive) Abelian group, and A\u2217=A\u2212{0}. A graph G is A-connected if G has an orientation D(G) such that for every mapping b:V(G)\u21a6A satisfying \u2211v\u2208V(G)b(v)=0, there is a function f:E(G)\u21a6A\u2217 such that for each vertex v\u2208V(G), the sum of f over the edges directed out from v minus the sum of f over the edges directed into v equals b(v). For a 2-edge-connected graph G, define \u039bg(G)=min{k: for any Abelian group A with |A|\u2265k, G is A-connected }. Let P denote a path in G, let \u03b2G(P) be the minimum length of a circuit containing P, and let \u03b2i(G) be the maximum of \u03b2G(P) over paths of length i in G. We show that \u039bg(G)\u2264\u03b2i(G)+1 for any integer i>0 and for any 2-connected graph G. Partial solutions toward determining the graphs for which equality holds were obtained by Fan et al. in [G. Fan, H.-J. Lai, R. Xu, C.-Q. Zhang, C. Zhou, Nowhere-zero 3-flows in triangularly connected graphs, Journal of Combinatorial Theory, Series B 98 (6) (2008) 1325\u20131336], among others. In this paper, we completely determine all graphs G with \u039bg(G)=\u03b22(G)+1. Let G be an undirected graph, A be an (additive) abelian group and A* = A - {0}. A graph G is A-connected if G has an orientation D(G) such that for every function b : V(G) \u21a6A satisfying \u03a3v \u2208 V(G) b(v) = 0, there is a function f: E(G)\u21a6A* such that at each vertex v \u2208 V(G), the amount of f values on the edges directed out from v minus the amount of f values on the edges directed into v equals b(v). In this paper, we investigate, for a 2-edge-connected graph G with diameter at most 2, the group connectivity number \u039b g(G) = min{n: G is A-connected for every abelian group A with |A| \u2265 n}, and show that any such graph G satisfies \u039b g (G) \u2264 6. Furthermore, we show that if G is such a 2-edge-connected diameter 2 graph, then \u039b g(G) = 6 if and only if G is the 5-cycle; and when G is not the 5-cycle, then \u039b g (G) = 5 if and only if G is the Petersen graph or G belongs to two infinite families of well characterized graphs."
  },
  {
    "id": "412055",
    "input": "Generate a title for the following abstract of a paper: In this paper we prove that testing the $textit{cyclic level planarity}$ of a cyclic level graph is a polynomial-time solvable problem. This is achieved by introducing and studying a generalization of this problem, which we call $textit{cyclic }{cal T}textit{-level planarity}$. Moreover, we show a complexity dichotomy for testing the $textit{simultaneous level planarity}$ of a set of level graphs, with respect to both the number of level graphs and the number of levels. In this paper we study the clustered graphs whose underlying graph is a cycle. This is a simple family of clustered graphs that are \u201chighly non connected\u201d. We start by studying 3-cluster cycles, that are clustered graphs such that the underlying graph is a simple cycle and there are three clusters all at the same level. We show that in this case testing the c-planarity can be done efficiently and give an efficient drawing algorithm. Also, we characterize 3-cluster cycles in terms of formal grammars. Finally, we generalize the results on 3-cluster cycles considering clustered graphs that at each level of the inclusion tree have a cycle structure. Even in this case we show efficient c-planarity testing and drawing algorithms. In a drawing of a clustered graph vertices and edges are drawn as points and curves, respectively, while clusters are represented by simple closed regions. A drawing of a clustered graph is c-planar if it has no edge-edge, edge-region, or region-region crossings. Determining the complexity of testing whether a clustered graph admits a c-planar drawing is a long-standing open problem in the Graph Drawing research area. An obvious necessary condition for c-planarity is the planarity of the graph underlying the clustered graph. However, this condition is not sufficient and the consequences on the problem due to the requirement of not having edge-region and region-region crossings are not yet fully understood.In order to shed light on the c-planarity problem, we consider a relaxed version of it, where some kinds of crossings (either edge-edge, edge-region, or region-region) are allowed even if the underlying graph is planar. We investigate the relationships among the minimum number of edge-edge, edge-region, and region-region crossings for drawings of the same clustered graph. Also, we consider drawings in which only crossings of one kind are admitted. In this setting, we prove that drawings with only edge-edge or with only edge-region crossings always exist, while drawings with only region-region crossings may not. Further, we provide upper and lower bounds for the number of such crossings. Finally, we give a polynomial-time algorithm to test whether a drawing with only region-region crossings exists for biconnected graphs, hence identifying a first non-trivial necessary condition for c-planarity that can be tested in polynomial time for a noticeable class of graphs. We study the following problem: given a planar graph  G  and a planar drawing (embedding) of a subgraph of  G , can such a drawing be extended to a planar drawing of the entire graph  G q This problem fits the paradigm of extending a partial solution for a problem to a complete one, which has been studied before in many different settings. Unlike many cases, in which the presence of a partial solution in the input makes an otherwise easy problem hard, we show that the planarity question remains polynomial-time solvable. Our algorithm is based on several combinatorial lemmas, which show that the planarity of partially embedded graphs exhibits the \u2018TONCAS\u2019 behavior \u201cthe obvious necessary conditions for planarity are also sufficient.\u201d These conditions are expressed in terms of the interplay between (1) the rotation system and containment relationships between cycles and (2) the decomposition of a graph into its connected, biconnected, and triconnected components. This implies that no dynamic programming is needed for a decision algorithm and that the elements of the decomposition can be processed independently.   Further, by equipping the components of the decomposition with suitable data structures and by carefully splitting the problem into simpler subproblems, we make our algorithm run in linear time.   Finally, we consider several generalizations of the problem, such as minimizing the number of edges of the partial embedding that need to be rerouted to extend it, and argue that they are NP-hard. We also apply our algorithm to the simultaneous graph drawing problem  Simultaneous Embedding with Fixed Edges (Sefe) . There we obtain a linear-time algorithm for the case that one of the input graphs or the common graph has a fixed planar embedding."
  },
  {
    "id": "411910",
    "input": "Generate a title for the following abstract of a paper: .\u00a0\u00a0 Morphological shared-weight neural networks (MSNN) combine the feature extraction capability of mathematical morphology with\n the function-mapping capability of neural networks in a single trainable architecture. The MSNN method has been previously\n demonstrated using a variety of imaging sensors, including TV, forward-looking infrared (FLIR) and synthetic aperture radar\n (SAR). In this paper, we provide experimental results with laser radar (LADAR). We present three sets of experiments. In the\n first set of experiments, we use the MSNN to detect different types of targets simultaneously. In the second set, we use the\n MSNN to detect only a particular type of target. In the third set, we test a novel scenario, referred to as the Sims scenario:\n we train the MSNN to recognize a particular type of target using very few examples. A detection rate of 86% with a reasonable\n number of false alarms was achieved in the first set of experiments and a detection rate of close to 100% with very few false\n alarms was achieved in the second and third sets of experiments. In all the experiments, a novel pre-processing method is\n used to create a pseudo-intensity images from the original LADAR range images. By utilizing Morphological Shared-Weight Neural Networks (MSNN) that have been trained for face recognition, common access restriction points can be enhanced to identify particular individuals of interest. A trained MSNN is a computational intelligence structure that learns representation of a specific face that encodes in its connection weights the feature extraction and classification abilities needed to identify an instance of that face. It has been shown effective in analyzing images that contain the target in a group of faces, even with the target face at varying orientations and lighting, as well as occluded target faces. The experiments presented here show the possible application of the MSNN to perform watch-list scanning of faces as individuals pass through access screening areas. Escherichia coli O157:H7 has been found to cause serious health problems. Traditional methods to identify the organism are quite slow, pulsed-held gel electrophoresis (PFGE) images contain \"banding pattern\" information which can be used to recognize the bacteria. A fuzzy logic rule-based system is used as a guide to find a good feature set for the recognition of E. coli O157:H7. While the fuzzy rule-based system achieved good recognition, the human inspired features used in the rules were incorporated into a multiple neural network fusion approach which gave excellent separation of the target bacteria. The fuzzy integral was utilized in the fusion of neural networks trained with different feature sets to reach an almost perfect classification rate of E. coli O157:H7 PFGE patterns made available for the experiments. Handwriting recognition has challenged computer scientists for years. To succeed, a computing solution must ably recognize complex character patterns and represent imprecise, commonsense knowledge about the general appearance of characters, words, and phrases. Character recognition is a classical computing problem, dating back to neural computing's infancy. One of Frank Rosenblatt's first demonstrations on the Mark I Perceptron neurocomputer in the late 1950s involved character recognition. The Perceptron was one of the first computers based on the idea of a neural network, which is a simplified computational model of neurons in a human brain. It was the first functioning neurocomputer, and it was able to recognize a fixed-font character set. As with many artificial intelligence applications, the difficulty of handwriting recognition was greatly underestimated. Significant progress was not achieved until the late 1980s and early 1990s, when many technologies converged to enable rapid increases in recognition rates for digits, characters, and words so that reliable commercial systems could be developed. Handwriting recognition problems are either online or offline. Online recognition systems use a pressure-sensitive pad that records the pen's pressure and velocity, which would be the case with, for example, a personal digital assistant. In offline recognition, the kind we are concerned with here, system input is a digital image of handwritten letters and numbers. Handwriting recognition requires tools and techniques that recognize complex character patterns and represent imprecise, commonsense knowledge about the general appearance of characters, words, and phrases. Neural networks and fuzzy logic are complementary tools for solving such problems. Neural networks, which are highly nonlinear and highly interconnected for processing imprecise information, can finely approximate complicated decision boundaries. Fuzzy set methods can represent degrees of truth or belonging. Fuzzy logic, one of several fuzzy set methods, encodes imprecise knowledge and naturally maintains multiple hypotheses that result from the uncertainty and vagueness inherent in real problems. By combining the complementary strengths of neural and fuzzy approaches into a hybrid system, we can attain increased recognition capability for solving handwriting recognition problems. This article describes the application of neural and fuzzy methods to three problems:recognition of handwritten words,recognition of numeric fields, andlocation of handwritten street numbers in address images. These problems and methods were part of research we conducted on US Postal Service data and on problems of interest to the USPS."
  },
  {
    "id": "41538",
    "input": "Generate a title for the following abstract of a paper: This paper introduces the problem of Fine-grained Incident Video Retrieval (FIVR). Given a query video, the objective is to retrieve all associated videos, considering several types of associations that range from duplicate videos to videos from the same incident. FIVR offers a single framework that contains several retrieval tasks as special cases. To address the benchmarking needs of all such ta... In this paper, we address the problem of high performance and computationally efficient content-based video retrieval in large-scale datasets. Current methods typically propose either: (i) fine-grained approaches employing spatio-temporal representations and similarity calculations, achieving high performance at a high computational cost or (ii) coarse-grained approaches representing/indexing videos as global vectors, where the spatio-temporal structure is lost, providing low performance but also having low computational cost. In this work, we propose a Knowledge Distillation framework, called Distill-and-Select (DnS), that starting from a well-performing fine-grained Teacher Network learns: (a) Student Networks at different retrieval performance and computational efficiency trade-offs and (b) a Selector Network that at test time rapidly directs samples to the appropriate student to maintain both high retrieval performance and high computational efficiency. We train several students with different architectures and arrive at different trade-offs of performance and efficiency, i.e., speed and storage requirements, including fine-grained students that store/index videos using binary representations. Importantly, the proposed scheme allows Knowledge Distillation in large, unlabelled datasets\u2014this leads to good students. We evaluate DnS on five public datasets on three different video retrieval tasks and demonstrate (a) that our students achieve state-of-the-art performance in several cases and (b) that the DnS framework provides an excellent trade-off between retrieval performance, computational speed, and storage space. In specific configurations, the proposed method achieves similar mAP with the teacher but is 20 times faster and requires 240 times less storage space. The collected dataset and implementation are publicly available: \n                https://github.com/mever-team/distill-and-select\n                \n              . In the recent years, the rapid increase of the volume of multimedia content has led to the development of several automatic annotation approaches. In parallel, the high availability of large amounts of user interaction data, revealed the need for developing automatic annotation techniques that exploit the implicit user feedback during interactive multimedia retrieval tasks. In this context, this paper proposes a method for automatic video annotation by exploiting implicit user feedback during interactive video retrieval, as this is expressed with gaze movements, mouse clicks and queries submitted to a content-based video search engine. We exploit this interaction data to represent video shots with feature vectors based on aggregated gaze movements. This information is used to train a classifier that can identify shots of interest for new users. Subsequently, we propose a framework that during testing: a) identifies topics (expressed by query clusters), for which new users are searching for, based on a novel clustering algorithm and b) associates multimedia data (i.e., video shots) to the identified topics using supervised classification. The novel clustering algorithm is based on random forests and is driven by two factors: first, by the distance measures between different sets of queries and second by the homogeneity of the shots viewed during each query cluster defined by the clustering procedure; this homogeneity is inferred from the performance of the gaze-based classifier on these shots. The evaluation shows that the use of aggregated gaze data can be exploited for video annotation purposes. Computer vision techniques have made considerable progress in recognizing object categories by learning models that normally rely on a set of discriminative features. However, in contrast to human perception that makes extensive use of logic-based rules, these models fail to benefit from knowledge that is explicitly provided. In this paper, we propose a framework that can perform knowledge-assisted analysis of visual content. We use ontologies to model the domain knowledge and a set of conditional probabilities to model the application context. Then, a Bayesian network is used for integrating statistical and explicit knowledge and performing hypothesis testing using evidence-driven probabilistic inference. In addition, we propose the use of a focus-of-attention (FoA) mechanism that is based on the mutual information between concepts. This mechanism selects the most prominent hypotheses to be verified/tested by the BN, hence removing the need to exhaustively test all possible combinations of the hypotheses set. We experimentally evaluate our framework using content from three domains and for the following three tasks: 1) image categorization; 2) localized region labeling; and 3) weak annotation of video shot keyframes. The results obtained demonstrate the improvement in performance compared to a set of baseline concept classifiers that are not aware of any context or domain knowledge. Finally, we also demonstrate the ability of the proposed FoA mechanism to significantly reduce the computational cost of visual inference while obtaining results comparable to the exhaustive case."
  },
  {
    "id": "41303",
    "input": "Generate a title for the following abstract of a paper: In this note we investigate the languages obtained by intersecting slender regular or context-free languages with the set of all primitive Words over the common alphabet. We prove that. these languages axe also re gular, and, respectively, context-free. The statement does not hold anymore for either regular or context-free languages. Moreover, the set of all non-primitive words of a slender context-free language is still context-free. Some possible directions for further research are finally discussed. Abstract: In this paper we propose a Chomsky-Sch\u007futzenberger type characterization of k-poly-slender context-free languages, as the homomorphical image of an intersectionof a Dyck language and a (2k + 1)-poly-slender regular language. A stronger resultis provided, namely the homomorphism and the Dyck language are determined irrespectiveof the given poly-slender context-free language, when considering the familyof all poly-slender context-free languages. A similar characterization is obtained for... We consider a few algorithmic problems regarding the hairpin completion. The first problem we consider is the membership problem of the hairpin and iterated hairpin completion of a language. We propose an O(nf(n)) and O(n^2f(n)) time recognition algorithm for the hairpin completion and iterated hairpin completion, respectively, of a language recognizable in O(f(n)) time. We show that the n factor is not needed in the case of hairpin completion of regular and context-free languages. The n^2 factor is not needed in the case of iterated hairpin completion of context-free languages, but it is reduced to n in the case of iterated hairpin completion of regular languages. We then define the hairpin completion distance between two words and present a cubic time algorithm for computing this distance. A linear time algorithm for deciding whether or not the hairpin completion distance with respect to a given word is connected is also discussed. Finally, we give a short list of open problems which appear attractive to us. The paper discusses some classes of contextual grammars---mainly those with \"maximal use of selectors\"---giving some arguments that these grammars can be considered a good model for natural language syntax.A contextual grammar produces a language starting from a finite set of words and interatively adding contexts to the currently generated words, according to a selection procedure: each context has associated with it a selector, a set of words; the context is adjoined to any occurrence of such a selector in the word to be derived. In grammars with maximal use of selectors, a context is adjoined only to selectros for which no superword is a selector. Maximality can be defined either locally or globally (with respect to all selectors in the grammar). The obtained families of languages are incomparable with that of Chomsky context-free languages (and with other families of languages that contain linear languages and that are not \"too large\"; see Section 5) and have a series of properties supporting the assertion that these grammars are a possible adequate model for the syntax of natural languages. They are able to straightforwardly describe all the usual restrictions appearing in natural (and artificial) languages, which lead to the non-context-freeness of these languages: reduplication, crossed dependencies, and multiple agreements; however, there are center-embedded constructions that cannot be covered by these grammars.While these assertions concern only the weak generative capacity of contextual grammars, some ideas are also proposed for associating a structure to the generated words, in the form of a tree, or of a dependence relation (as considered in descrpitive linguistics and also similar to that in link grammars)."
  },
  {
    "id": "41680",
    "input": "Generate a title for the following abstract of a paper: While the desire to support fast, ad hoc query processing for large data warehouses has motivated the recent introduction of many new indexing structures, with a few notable exceptions (namely, the LSM-Tree [4] and the Stepped Merge Method ill) little attention has been given to developing new indexing schemes that allow fast insertions. Since additions to a large warehouse may number in the millions per day, indices that require a disk seek (or even a significant fraction of a seek) per insertion are not acceptable.In this paper, we offer an alternative to the B+-tree called the Y-tree for indexing huge warehouses having frequent insertions. The Y-tree is a new indexing structure supporting both point and range queries over a single attribute, with retrieval performance comparable to the B+-tree. For processing insertions, however, the Y-tree may exhibit a speedup of 100 times over hatched insertions into a B+-tree. Large, spatial databases may be required to prosses both intense query and intense update loads. For example, a data warehouse which contains spatial data may be required to answer difficult, analytical queries at the same time as it accepts massive amounts of new data, and any downtime to merge new data into existing data organizations may be unacceptable. Traditional, incremental spatial access methods (like the popular R-tree and its variants) may be unacceptable for use in such an environment since they support relatively slow update rates, and may lead to a relatively poor global data organization resulting in slow query evaluation. In this paper, we present the details T2SM, which is an instantiation of the linear file template, for use with spatial data. in T2SM, data are organized as a set of ongoing, external memory sorts based on the STR algorithm. T2SM exhibits query performance similar to (and sometimes superior to) a bulk-loaded R-Tree, but at the same time is truly incremental and can maintain exceptional update rates. The rate of increase in hard disk storage capacity continues to outpace the rate of decrease in hard disk seek time. This trend implies that the value of a seek is increasing exponentially relative to the value of storage.With this trend in mind, we introduce the partitioned exponential file (PE file) which is a generic storage manager that can be customized for many different types of data (e.g., numerical, spatial, or temporal). The PE file is intended for use in environments with intense update loads and concurrent, analytic queries. Such an environment may be found, for example, in long-running scientific applications which can produce petabytes of data. For example, the proposed Large Synoptic Survey Telescope [36] will produce 50---100 petabytes of observational, scientific data over its multi-year lifetime. This database will never be taken off-line, so bursty update loads of tens of terabytes per day must be handled concurrently with data analysis. In the PE file, data are organized as a series of on-disk sorts with a careful, global organization. Because the PE file relies heavily on sequential I/O, only a fraction of a disk seek is required for a typical record insertion or retrieval.In addition to describing the PE file, we also detail a set of benchmarking experiments for T1SM, which is a PE file customized for use with multi-attribute data records ordered on a single numerical attribute. In our benchmarking, we implement and test many competing data organizations that can be used to index and store such data, such as the B+-Tree, the LSM-Tree, the Buffer Tree, the Stepped Merge Method, and the Y-Tree. As expected, no organization is the best over all benchmarks, but our experiments show that T1SM is the best choice in many situations, suggesting that it is the best overall. Specifically, T1SM performs exceptionally well in the case of a heavy query workload that must be handled concurrently with an intense insertion stream. Our experiments show that T1SM (and its close cousin, the T2SM storage manager for spatial data) can handle very heavy mixed workloads of this type, and still maintain acceptably small query latencies. This work is motivated by the observation that hard disk seek capability per byte of storage has declined expo- nentially for decades, and yet most popular access structures are still based on basic ideas proposed 30 years ago. These access structures are typically seek-intensive. In this paper, we propose a new paradigm for data access and organization called the linear file, which aims to drastically reduce disk seeks during query and update evaluation, by organizing data as a set of ongoing, on-disk sorts. In a linear file, updates are processed quickly, structures are 100% packed, and data are carefully organized on disk to reduce seeks among disk pages. Since the linear file is a general template for data organization, in this paper we benchmark T1SM and T2SM, two specific instantiations for use with one-dimensional and spatial data, respectively."
  },
  {
    "id": "412383",
    "input": "Generate a title for the following abstract of a paper: For a given set L of species and a set T of triplets on L, we seek to construct a phylogenetic network which is consistent with T i.e. which represents all triplets of T. The level of a network is defined as the maximum number of hybrid vertices in its biconnected components. When T is dense, there exist polynomial time algorithms to construct level-0; 1 and 2 networks (Aho et al., 1981; Jansson, Nguyen and Sung, 2006; Jansson and Sung, 2006; Iersel et al., 2009). For higher levels, partial answers were obtained in the paper by Iersel and Kelk (2008), with a polynomial time algorithm for simple networks. In this paper, we detail the first complete answer for the general case, solving a problem proposed in Jansson and Sung (2006) and Iersel et al. (2009). For any k fixed, it is possible to construct a level-k network having the minimum number of hybrid vertices and consistent with T, if there is any, in time O(vertical bar T vertical bar(k+1)n(left perpendicular4k/3+1right perpendicular)).(a) For a given dense triplet set $\\mathcal{T}$, there exist two natural questions [7]: Does there exist any phylogenetic network consistent with $\\mathcal{T}$? In case such networks exist, can we find an effective algorithm to construct one? For cases of networks of levels k = 0, 1 or 2, these questions were answered in [1,6,7,8,10] with effective polynomial algorithms. For higher levels k , partial answers were recently obtained in [11] with an $O(|\\mathcal{T}|^{k+1})$ time algorithm for simple networks. In this paper, we give a complete answer to the general case, solving a problem proposed in [7]. The main idea of our proof is to use a special property of SN-sets in a level-k network. As a consequence, for any fixed k , we can also find a level-k network with the minimum number of reticulations, if one exists, in polynomial time. From a well-known decomposition theorem, we propose a tree representation for distributive and simplicial lattices. We show how this representation (called ideal tree) can be efficiently computed (linear time in the size of the lattice given by any graph whose transitive closure is the lattice) and compared with respect to time and space complexity. As far as time complexity is concerned, we simply consider the time needed for computations of basic lattice operations such as meet or join and reachability ( x \u2a7d y ). Therefore an ideal tree can be considered as a good data structure for a distributive lattice, since for a lattice L = ( X , E ) it uses O(\u00a6X\u00a6) space and allows computations of reachability, meet and join operations in O(\u00a6M(L)\u00a6) , where M ( L ) denotes the suborder of the meet irreducible elements in L . Furthermore, optimal bit-vector encoding for distributive lattices can be easily derived from this data structure. Relationships with encoding proposed by A\u00eft-Kaci et al. [3], Caseau [5] are also discussed. Intensive use of this ideal tree allow us to achieve best running time algorithms for most of the applications in which distributive lattices are involved; as for example, constructing the lattice of ideals or generating ideals for a given partial order. Therefore this data structure can be used in many areas such as scheduling theory, in which several algorithms are based on a dynamic programming approach of the lattice of ideals of the precedence ordering; or distributed programming, in which some of the debugging tools rely on the calculation of the lattice of ideals of the causality ordering of the events. The number of families over ground set V is 2^2^^^|^^^V^^^| and by this fact it is not possible to represent such a family using a number of bits polynomial in |V|. However, under some simple conditions, this becomes possible, like in the cases of a symmetric crossing family and a weakly partitive family, both representable using @Q(|V|) space. We give a general framework for representing any set family by a tree. It extends in a natural way the one used for symmetric crossing families in [W. Cunningham, J. Edmonds, A combinatorial decomposition theory, Canadian Journal of Mathematics 32 (1980) 734-765]. We show that it also captures the one used for weakly partitive families in [M. Chein, M. Habib, M.C. Maurer, Partitive hypergraphs, Discrete Mathematics 37 (1) (1981) 35-50]. We introduce two new classes of families: weakly partitive crossing families are those closed under the union, the intersection, and the difference of their crossing members, and union-difference families those closed under the union and the difference of their overlapping members. Each of the two cases encompasses symmetric crossing families and weakly partitive families. Applying our framework, we obtain a linear @Q(|V|) and a quadratic O(|V|^2) space representation based on a tree for them. We introduce the notion of a sesquimodule - one module and a half - in a digraph and in a generalization of digraphs called 2-structure. From our results on set families, we show for any digraph, resp. 2-structure, a unique decomposition tree using its sesquimodules. These decompositions generalize strictly the clan decomposition of a digraph and that of a 2-structure. We give polynomial time algorithms computing the decomposition tree for both cases of sesquimodular decomposition."
  },
  {
    "id": "411263",
    "input": "Generate a title for the following abstract of a paper: Spoken medical dialogue is a valuable source of information, and it forms a foundation for diagnosis, prevention and therapeutic management. However, understanding even a perfect transcript of spoken dialogue is challenging for humans because of the lack of structure and the verbosity of dialogues. This work presents a first step towards automatic analysis of spoken medical dialogue. The backbone of our approach is an abstraction of a dialogue into a sequence of semantic categories. This abstraction uncovers structure in informal, verbose conversation between a caregiver and a patient, thereby facilitating automatic processing of dialogue content. Our method induces this structure based on a range of linguistic and contextual features that are integrated in a supervised machine-learning framework. Our model has a classification accuracy of 73%, compared to 33% achieved by a majority baseline (p<0.01). This work demonstrates the feasibility of automatically processing spoken medical dialogue. A method for automatic analysis of time-oriented clinical narratives would be of significant practical import for medical decision making, data modeling and biomedical research. This paper proposes a robust corpus-based approach for temporal analysis of medical discharge summaries. We characterize temporal organization of clinical narratives in terms of temporal segments and their ordering. We consider a temporal segment to be a fragment of text that does not exhibit abrupt changes in temporal focus. Our method derives temporal order based on a range of linguistic and contextual features that are integrated in a supervised machine-learning framework. Our learning method achieves 83% F-measure in tempo-ral segmentation, and 78.3% accuracy in inferring pairwise temporal relations. We present a model for aggregation of product review snippets by joint aspect identification and sentiment analysis. Our model simultaneously identifies an underlying set of ratable aspects presented in the reviews of a product (e.g., sushi and miso for a Japanese restaurant) and determines the corresponding sentiment of each aspect. This approach directly enables discovery of highly-rated or inconsistent aspects of a product. Our generative model admits an efficient variational mean-field inference algorithm. It is also easily extensible, and we describe several modifications and their effects on model structure and inference. We test our model on two tasks, joint aspect identification and sentiment analysis on a set of Yelp reviews and aspect identification alone on a set of medical summaries. We evaluate the performance of the model on aspect identification, sentiment analysis, and per-word labeling accuracy. We demonstrate that our model outperforms applicable baselines by a considerable margin, yielding up to 32% relative error reduction on aspect identification and up to 20% relative error reduction on sentiment analysis. This paper studies the impact of paraphrases on the accuracy of automatic evaluation. Given a reference sentence and a machine-generated sentence, we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference. We apply our paraphrasing method in the context of machine translation evaluation. Our experiments show that the use of a paraphrased synthetic reference refines the accuracy of automatic evaluation. We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation."
  },
  {
    "id": "411974",
    "input": "Generate a title for the following abstract of a paper: This half-day tutorial covers the salient aspects of thefirst major revision of the Unified Modeling Language \u9a74UML 2.0. In this brief summary, we briefly review someof the main points covered in the tutorial. This half-day tutorial covers the salient features of the first major revision of the Unified Modeling Language - UML 2. This short note summarizes the major topics covered by the tutorial. This half-day tutorial covers the salient aspects of the first major revision of the Unified Modeling Language, UML 2.0. It includes background information on what drove the requirements and the design rationale---from the point of view of one of its primary designers. The overall structure of UML 2.0 is described followed by a more detailed description of the most prominent new modeling features illustrated with many examples. The ability of UML 2.0 to deal with the needs of model-driven development methods is also covered. The general perception and opinion of the Unified Modeling Language in the minds of many software professionals is colored by its early versions. However, the language has evolved into a qualitatively different tool: one that not only supports informal lightweight sketching in early phases of development, but also full implementation capability, if desired. Unfortunately, these powerful new capabilities and features of the language remain little known and are thus underutilized. In this article, we first review how UML has changed over time and what new value it can provide to practitioners. Next, we focus on and explain one particularly important new modeling capability that is often overlooked or misrepresented and explain briefly what is behind it and how it can be used to advantage."
  },
  {
    "id": "412388",
    "input": "Generate a title for the following abstract of a paper: A new class of parallel rewriting devices called persistent ET0L systems is presented. It is shown that these systems are equivalent (in their language-generating power) to some other classes of grammars investigated in the literature. Also this new mechanism sheds some light on the nature of nondeterminism in (variations of) ET0L systems. A notion of synchronized substitution is introduced in the concept of two-level metacontrolled substitution grammar as defined in [6]. Whereas in [6] the top level is essentially grammar-independent, here the mechanism strongly depends on the grammars chosen. Furthermore we illustrate this mechanism on grammars which operate in parallel, namely E0L systems. Their language-generating power is compared with that of ET0L systems and E0L iterated systems. A new language generating mechanism is defined. It is derived from a generalization of the two-level substitution mechanism and involves several (incomplete) grammars which communicate with each other by introducing variables for which they have no productions themselves. When the grammars are context-free, this yields an alternative definition for the context-free programmed grammars. When the grammars are EOL (parallel) a class is found lying (strictly ?) between ETOL and CFP. The availability of computerized lexicons, thesauri and \"ontologies\" -we discuss this termi nology- makes it possible to formalize semantic aspects of information as used in the analysis, design and implementation of information systems (and in fact general software systems) in new and useful ways. We survey a selection of relevant ongoing work, discuss different issues of semantics that arise, and characterize the resulting computerized information systems, called CLASS for Computer-Lexicon Assisted Software Systems. The need for a \"global\" common ontology (lexicon, thesaurus) is conjectured, and some desirable properties are proposed. We give a few examples of such CLASS-s and indicate avenues of current and future research in this area. In particular, certain problems can be identified with well-known existing lexicons such as CYC and WordNet, as well as with sophisticated representation- and inference engines such as KIF or SHOE. We argue nevertheless that large public lexicons should be simple, i.e. their semantics become implicit by agreement among \"all\" users, and ideally completely application independent. In short, the lexicon or thesaurus then becomes the semantic domain for all applications. 1. Introduction The formal treatment of semantics in information systems has always been an important and difficult issue -though often implicit or even tacit- in the analysis, design and implementation of such systems. The literature abounds with different ways and attempts to represent the meaning of data and information for use in and by databases and their applications in computerized information systems. We contend that computer-based \"ontologies\" (we shall make this term precise in what follows, but use it in the meantime to cover lexicons and thesauri) provide a useful way for formalizing the semantics of represented information. In fact we shall argue that such ontologies, in principle, can actually be the semantic domain for an information system in a very concrete and useful manner. The caveat \"in principle\" is needed since current ontologies (lexicons, thesauri) are not (yet) constructed or available in forms that quite suit this purpose. Modern distributed object (DO) technology such as CORBA (OMG95), DCOM (Red97) etc. makes it ever more likely, and desirable, that objects or agents performing interesting services \"meet\" interested consumers and applications without prior knowledge of each other's design and precise functionality. Mediators, wrappers and other devices have been proposed to help make this possible, but these are not at present \"runtime\" solutions. The sheer multitude of such object s/agents in the future dictates that the necessary agreements will have to be negotiated in real-time as the \"meeting\" occurs. Short of a galaxy-wide standard for programming and data modeling it is hard to imagine that these agreements can occur without the use of sophisticated and general lexicons, thesauri or ontologies. These will need to be (a) very large resource(s), possibly only one per language, but (b) with a rather simple, 1 This research was partially supported by the ESPRIT Project \"TREVI\", nr. 23311 under the European"
  },
  {
    "id": "41945",
    "input": "Generate a title for the following abstract of a paper: We consider portable software implementations of hash tables with timeouts. The context is a high volume stream of keyed items. When a new item arrives, we want to know if has been seen recently in terms of a fixed lifespan.This problem has numerous applications as a front-end for Internet traffic processing where the key could be a selection of fields from the header of a packet, e. g., tracing packets through networks, aggregation of netflows from routers, and helping firewalls reuse decisions from recent packets with the same key.We propose time-reversed linear probing tables to deal with timeouts. In experiments, compared with tumbling windows and lazy deletions, our time-reversed tables typically gains at least 25% in speed while using only half the space. The cost per item approaches that of a single random memory access.Our new scheme is cleaner to implement than previous schemes, and also more versatile, e. g., it is trivial to allow different items to have different lifespans; something not possible with tumbling windows. Adding this to the improved time and space efficiency, makes time-reversed linear probing a canonical first choice for hash tables with time-outs. From a high-volume stream of weighted items, we want to create a generic sample of a certain limited size that we can later use to estimate the total weight of arbitrary subsets. Applied to Internet traffic analysis, the items could be records summarizing the flows of packets streaming by a router. Subsets could be flow records from different time intervals of a worm attack whose signature is later determined. The samples taken in the past thus allow us to trace the history of the attack even though the worm was unknown at the time of sampling. Estimation from the samples must be accurate even with heavy-tailed distributions where most of the weight is concentrated on a few heavy items. We want the sample to be weight sensitive, giving priority to heavy items. At the same time, we want sampling without replacement in order to avoid selecting heavy items multiple times. To fulfill these requirements we introduce priority sampling, which is the first weight-sensitive sampling scheme without replacement that works in a streaming context and is suitable for estimating subset sums. Testing priority sampling on Internet traffic analysis, we found it to perform an order of magnitude better than previous schemes. Priority sampling is simple to define and implement: we consider a steam of items i &equals; 0,\u2026,n \u2212 1 with weights wi. For each item i, we generate a random number \u03b1i \u2208 (0,1] and create a priority qi &equals; wi/\u03b1i. The sample S consists of the k highest priority items. Let \u03c4 be the (k &plus; 1)th highest priority. Each sampled item i in S gets a weight estimate &wcirc;i &equals; max{wi, \u03c4}, while nonsampled items get weight estimate &wcirc;i &equals; 0. Magically, it turns out that the weight estimates are unbiased, that is, E[&wcirc;i] &equals; wi, and by linearity of expectation, we get unbiased estimators over any subset sum simply by adding the sampled weight estimates from the subset. Also, we can estimate the variance of the estimates, and find, surprisingly, that the covariance between estimates &wcirc;i and &wcirc;j of different weights is zero. Finally, we conjecture an extremely strong near-optimality; namely that for any weight sequence, there exists no specialized scheme for sampling k items with unbiased weight estimators that gets smaller variance sum than priority sampling with k &plus; 1 items. Szegedy settled this conjecture at STOC'06. Starting with a set of weighted items, we want to create a generic sample of\na certain size that we can later use to estimate the total weight of arbitrary\nsubsets. For this purpose, we propose priority sampling which tested on\nInternet data performed better than previous methods by orders of magnitude.\n  Priority sampling is simple to define and implement: we consider a steam of\nitems i=0,...,n-1 with weights w_i. For each item i, we generate a random\nnumber r_i in (0,1) and create a priority q_i=w_i/r_i. The sample S consists of\nthe k highest priority items. Let t be the (k+1)th highest priority. Each\nsampled item i in S gets a weight estimate W_i=max{w_i,t}, while non-sampled\nitems get weight estimate W_i=0.\n  Magically, it turns out that the weight estimates are unbiased, that is,\nE[W_i]=w_i, and by linearity of expectation, we get unbiased estimators over\nany subset sum simply by adding the sampled weight estimates from the subset.\nAlso, we can estimate the variance of the estimates, and surpricingly, there is\nno co-variance between different weight estimates W_i and W_j.\n  We conjecture an extremely strong near-optimality; namely that for any weight\nsequence, there exists no specialized scheme for sampling k items with unbiased\nestimators that gets smaller total variance than priority sampling with k+1\nitems. Very recently Mario Szegedy has settled this conjecture. Linear probing is one of the most popular implementations of dynamic hash tables storing all keys in a single array. When we get a key, we first hash it to a location. Next we probe consecutive locations until the key or an empty location is found. At STOC'07, Pagh et al. presented data sets where the standard implementation of 2-universal hashing leads to an expected number of \u03a9(log n) probes. They also showed that with 5-universal hashing, the expected number of probes is constant. Unfortunately, we do not have 5-universal hashing for, say, variable length strings. When we want to do such complex hashing from a complex domain, the generic standard solution is that we first do collision free hashing (w.h.p.) into a simpler intermediate domain, and second do the complicated hash function on this intermediate domain. Our contribution is that for an expected constant number of linear probes, it is suffices that each key has O(1) expected collisions with the first hash function, as long as the second hash function is 5-universal. This means that the intermediate domain can be n times smaller, and such a smaller intermediate domain typically means that the overall hash function can be made simpler and at least twice as fast. The same doubling of hashing speed for O(1) expected probes follows for most domains bigger than 32-bit integers, e.g., 64-bit integers and fixed length strings. In addition, we study how the overhead from linear probing diminishes as the array gets larger, and what happens if strings are stored directly as intervals of the array. These cases were not considered by Pagh et al."
  },
  {
    "id": "412124",
    "input": "Generate a title for the following abstract of a paper: Spatio-textual similarity join is a basic and significant operation in many applications. It is an operation that finds all the similar pairs of objects which have similar textual descriptions and are spatially close to each other. With the popularity of GPS and their applications, the size of spatiotextual data is increasing explosively, while the existing methods cannot deal with the spatio-textual similarity join efficiently on massive data. In this paper, we propose several approaches for spatio-textual similarity join using MapReduce. We use the prefix filtering and grid partitioning techniques to filter the spatiotextual objects under the filter-and-refine framework. Besides, we propose two kinds of optimization methods to improve the efficiency of the basic spatio-textual similarity join method. In the end, we conduct extensive experiments using several synthetic datasets that are comprised of real datasets, and the results show that our approaches have good performance in both efficiency and scalability. In this paper, we focus on set similarity join on massive probabilistic data using MapReduce, there is no effective approach that can process this problem efficiently. MapReduce is a popular paradigm that can process large volume data more efficiently, in this paper, we proposed two approaches using MapReduce to deal with this task: Hadoop Join by Map Side Pruning and Hadoop Join by Reduce Side Pruning. Hadoop Join by Map Side Pruning uses the sum of the existence probability to filter out the probabilistic sets directly at the Map task side which have no any chance to be similar with any other probabilistic set. Hadoop Join by Reduce Side Pruning uses probability sum based pruning principle and probability upper bound based pruning principle to reduce the candidate pairs at Reduce task side, it can save the comparison cost. Based on the above approaches, we proposed a hybrid solution that employs both Map-side and Reduce-side pruning methods. Finally we implemented the above approaches on Hadoop-0.20.2 and performed comprehensive experiments to their performance, we also test the speedup ratio compared with the naive method: Block Nested Loop Join. The experiment results show that our approaches have much better performance than that of Block Nested Loop Join and also have good scalability. To the best of our knowledge, this is the first work to try to deal with set similarity join on massive probabilistic data problem using MapReduce paradigm, and the approaches proposed in this paper provide a new way to process the massive probabilistic data. Keyword search over XML data has attracted a lot of research efforts in the last decade, where one of the fundamental research problems is how to efficiently answer a given keyword query w.r.t. a certain query semantics. We found that the key factor resulting in the inefficiency for existing methods is that they all heavily suffer from the common-ancestor-repetition problem. In this paper, we propose a novel form of inverted list, namely the IDList; the IDList for keyword $$k$$ consists of ordered nodes that directly or indirectly contain $$k$$. We then show that finding keyword query results based on the smallest lowest common ancestor and exclusive lowest common ancestor semantics can be reduced to ordered set intersection problem, which has been heavily optimized due to its application in areas such as information retrieval and database systems. We propose several algorithms that exploit set intersection in different directions and with or without using additional indexes. We further propose several algorithms that are based on hash search to simplify the operation of finding common nodes from all involved IDLists. We have conducted an extensive set of experiments using many state-of-the-art algorithms and several large-scale datasets. The results demonstrate that our proposed methods outperform existing methods by up to two orders of magnitude in many cases. The proliferation of geo-social network, such as Foursquare and Facebook Places, enables users to generate location information and its corresponding descriptive tags. Using geo-social networks, users with similar interests can plan for social activities collaboratively. This paper proposes a novel type of query, called Tag-based top-k Collaborative Spatial (TkCoS) query, for users to make outdoor plans collaboratively. This type of queries aim to retrieve groups of geographic objects that can satisfy a group of users' requirements expressed in tags, while ensuring that the objects be within the minimum spatial distance from the users. To answer TkCoS queries efficiently, we introduce a hybrid index structure called Spatial-Tag R-tree (STR-tree), which is an extension of the R-tree. Based on STR-tree, we propose a query processing algorithm that utilizes both spatial and tag similarity constraints to prune search space and identify desired objects quickly. Moreover, a differential impact factor is adopted to fine-tune the returned results in order to maximize the users' overall satisfaction. Extensive experiments on synthetic and real datatsets validate the efficiency and the scalability of the proposed algorithm."
  },
  {
    "id": "411345",
    "input": "Generate a title for the following abstract of a paper: The last few years have witnessed a meteoric rise of microblogging platforms, such as Twitter and Tumblr. The sheer volume of the microblog data and its highly dynamic nature present unique technical challenges for the platforms that provide search services. In particular, the search service must provide real-time response to queries, and continuously update the results as new microblogs are posted. Conventional approaches either cannot keep up with the high update rate, or cannot scale well to handle the large volume of data. We propose Pollux, a system that provides distributed real-time indexing and search service on microblogs. It adopts the distributed stream processing paradigm advocated by the recently developed platforms that are designed for real-time processing of large volume of data, such as Apache S4 and Twitter Storm. Although those open-source platforms have found successful applications in production environments, they lack some critical features required for real-time search. In particular: (1) they only implement partial fault tolerance, and do not provide lossless recovery in the event of a node failure, and (2) they do not have a facility for storing global data, which is necessary in efficiently ranking search results. Addressing those problems, Pollux extends current platforms in two important ways. First, we propose a failover strategy that can ensure high system availability and no data/state loss in the event of a node failure. Second, Pollux adds a global storage facility that supports convenient, efficient, and reliable data storage for shared data. We describe how to apply Pollux to the task of real-time search. We implement Pollux based on Apache S4, and show through extensive experiments on a Twitter dataset that the proposed solutions are effective, and Pollux can achieve excellent scalability. The explosive growth of user-generated contents in social networking websites necessitates the recommendation functionality that can push to the user the content that he/she is most likely to be interested in. Such recommendation should happen in real-time as new contents become available, because \"freshness\" is an important consideration in people's content-consumption behavior. Representing users and contents as feature vectors in a high-dimensional space, we can essentially cast the problem of real-time recommendations as the problem of computing the list of k nearest neighbors of each user, which we call kNN join. Given the vast volume of contents and users, the biggest challenge is how to continuously update the kNN join results as new contents arrive. Existing methods for incremental kNN join on data streams suffer from the \"curse of dimensionality\" and high in-memory search cost. In this paper, we present a solution that first identifies the users whose kNN's might be affected by the newly arrived content, and then update their kNN's respectively. We propose a new index structure named HDR-tree in order to support the efficient search of affected users. HDR-tree performs dimensionality reduction through clustering and principle component analysis (PCA) in order to improve the search effectiveness. To further reduce response time, we propose a variant of HDR-tree, called HDR-tree, that supports more efficient but approximate solutions. The results of extensive experiments show that our methods significantly outperform baseline methods. Central to many applications involving moving objects is the task of processing k-nearest neighbor (k-NN) queries. Most of the existing approaches to this problem are designed for the centralized setting where query processing takes place on a single server; it is difficult, if not impossible, for them to scale to a distributed setting to handle the vast volume of data and concurrent queries that are increasingly common in those applications. To address this problem, we propose a suite of solutions that can support scalable distributed processing of k-NN queries. We first present a new index structure called Dynamic Strip Index (DSI), which can better adapt to different data distributions than exiting grid indexes. Moreover, it can be naturally distributed across the cluster, therefore lending itself well to distributed processing. We further propose a distributed k-NN search (DKNN) algorithm based on DSI. DKNN avoids having an uncertain number of potentially expensive iterations, and is thus more efficient and more predictable than existing approaches. DSI and DKNN are implemented on Apache S4, an open-source platform for distributed stream processing. We perform extensive experiments to study the characteristics of DSI and DKNN, and compare them with three baseline methods. Experimental results show that our proposal scales well and significantly outperforms the alternative methods. In the last decade, keyword search over relational databases has been extensively studied because it promises to allow users lacking knowledge of structured query languages or unaware of the database schema to query the database in an intuitive way. The existing works about keyword search on databases proposed many approaches and have gain remarkable results. However, most of these approaches are designed for the centralized setting where keyword search is processed by only a single server. In reality, the scale of databases increases sharply and centralized methods hardly can handle keyword queries over these large databases. Moreover, processing keyword search over relational databases is a very time-consuming task, and the efficiency of the existing centralized approaches will degrade notably because the single server cannot provide enough computation power for the keyword search over very large databases. To address these challenges, we propose a distributed keyword search (DKS) approach with MapReduce and this approach can be well deployed on a cluster of servers to deal with keyword search over large databases in a parallel way."
  },
  {
    "id": "411524",
    "input": "Generate a title for the following abstract of a paper: In this paper we propose a schema integration technique to integrate multiple autonomous database systems. We adopt the unified model, which is basically a relational model with object-oriented features, as the global model for integrating schemas of multiple databases in the relational, hierarchical, network, and object-oriented models. The features and the advantages of the unified model as the global model are discussed, then we describe the methodologies to integrate heterogeneous databases using this model and show how various kinds of conflicts are resolved. We also show how a global query on the integrated schema is handled in heterogeneous environment. In this paper we propose a schema integration technique to integrate multiple autonomous database systems. We adopt the unified model, which is basically a relational model with object-oriented features, as the global model for integrating schemas of multiple database in the relational, hierarchical, network, and object-oriented models. The features and the advantages of the unified model as the global model are discussed, then we describe the methodologies to integrate heterogeneous databases using this model and show how various types of conflicts are resolved. We also show how a global query on the integrated schema is handled in heterogeneous environment. We also present a transaction management scheme which uses semantic recovery techniques. Our scheme does not impose any restrictions on data items that transactions can read and write and does not violate local autonomy. (C) 1998 Elsevier Science Inc. All rights reserved. In this paper, we propose a method to integrate a preexisting conventional database system with a multimedia server in the multidatabase environment. In the multidatabase environment, changes in the preexisting database system are not allowed because such changes are too expensive. For the integration, high-level semantic description of multimedia data is modeled using the enhanced entity-relationship (EER) model to support content-based retrieval of multimedia data. The EER design is translated into a schema of the preexisting database system, and then the translated schema is integrated with the preexisting database schema. The content description can be used to locate pertinent multimedia data, and the identifiers are used to access the multimedia data stored in the multimedia server. However, with only a simple schema representation of the semantic description of multimedia data, high levels of recall and precision of queries may not be obtained because conventional database systems provide only exact matching answers to the query. Thus, we extended the conventional query processing mechanism by providing a modified cooperative query answering mechanism. The purpose of multidatabase systems is to allow users to access and manipulate data from existing databases in the distributed environment without modifying the existing database management systems (DBMSs). In the multidatabase environment, determining if a schedule for a set of global transactions is globally serializable is difficult because each local DBMS operates autonomously. Thus, we adopt the approach that relaxes the isolation and atomicity requirements of transactions. Moreover, in some applications, multiple subtransactions of a global transaction need to be submitted to a local site. To support this new transaction model, we propose a new correctness criterion for global schedules that is based on the semantic compatibility of global transactions. The conditions that global transactions should satisfy to ensure the correct executions are investigated, and the concept of well-formed global transaction is defined. We also present a concurrency control scheme that ensures the proposed correctness criterion. Our scheme uses both compensation and retrial techniques to eliminate the requirement of local prepare-to-commit operations, and hence it allows the global subtransactions to be committed unilaterally at local. sites before the decision of global commit. (C) Elsevier Science Inc. 1997"
  },
  {
    "id": "41112",
    "input": "Generate a title for the following abstract of a paper: Room reverberation is typically the main obstacle for designing robust microphone-based source localization systems. The purpose of the paper is to analyze the achievable performance of acoustical source localization methods when room reverbera- tion is present. To facilitate the analysis, we apply well known results from room acoustics to develop a simple but useful statistical model for the room transfer function. The properties of the statistical model are found to correlate well with results from real data measurements. The room transfer function model is further applied to analyze the statistical properties of some existing methods for source local- ization. In this respect we consider especially the asymptotic error variance and the probability of an anomalous estimate. A note- worthy outcome of the analysis is that the so-called PHAT time- delay estimator is shown to be optimal among a class of cross- correlation based time-delay estimators. To verify our results on the error variance and the outlier probability we apply the image method for simulation of the room transfer function. The main obstacle for designing robust microphone-based source localization systems is the effects of room reverberation. Utilizing results from statistical room acoustics, we analyze the performance of GCC-based methods for time-delay estimation. An interesting outcome of the analysis is that the so-called PHAT time-delay estimator is shown to be optimal among a class of cross-correlation based time-delay estimators. In this paper we proposed to solve the eye detection and localization problem under a general statistical model based object detection framework. A binary tree representation is used to discover the objects\u9a74 underlying statistical structure. Tree structures enable us to describe the object local statistical structure in a coarse-to-fine fashion. Each subtree explains the statistics for certain local substructure. The tree is built in a top-down fashion. Subsets with negligible conditional independency are found by k-means clustering using mutual information. The conditionally independent features are separated into different subtrees, while more dependent features are tended to appear close in the tree. The distribution of the object can be learned accordingly. Gaussian mixture in the independent component analysis (ICA) subspace is used to model the distribution of each high dependent feature subset, where each independent component explains the local substructure. The use of tree structure enables us to learn the distribution recursively by applying Bayesian criterion. Substantial experiments were done to evaluate the performance over the eyes detection accuracy as well as the localization ability. Experimental results show a better detection accuracy than the Viisage system with a reasonable localization ability, which validate the algorithm. Scene understanding in the context of a smart meeting room involves the extraction of various kinds of cues at different levels of semantic abstraction. Specifically, hu- man activity in a scene is usually monitored using arrays of audio and visual sensors. Tasks such as person local- ization and tracking, speaker ID, focus of attention detec- tion, speech recognition and affective state recognition are among them. In this paper we demonstrate a system that ex- tracts such information by synergistically combining the in- formation from the various tasks to support each other. We exploit the fact that the output of one kind of human activ- ity analysis task contains valuable information for another such block and by interconnecting them, a robust system re- sults. We demonstrate this in a smart meeting room context equipped with 3 cameras and 16 microphones. The system performs the tasks of person tracking, head pose estimation, beamforming, speaker ID and speech recognition using au- dio and visual cues. The novelty lies in putting together the tasks such that they can provide relevant information to one another. We evaluate the performance of our system and present results for tasks such as keyword spotting and track- ing re-identification on real-world meeting scenes collected in our audio-visual testbed."
  },
  {
    "id": "412314",
    "input": "Generate a title for the following abstract of a paper: ABSTRACT We present a model,which allows to deflne in an uniform way information quality dimensions related to heterogeneous types of information, such as structured data managed in data bases, semi-structured and unstructured texts and im- ages. We flrst deflne a set of concepts that allow to represent several basic characteristics of such heterogeneous types of information. Then, we introduce a general categorization of quality dimensions and sub-dimensions, which are then specialized to structured data, semi- and unstructured texts and images. In so doing, we provide, to our knowledge, a flrst attempt to unify the information quality issue for het- erogeneous information types. Keywords data quality, information quality, structured data, semistruc- tured data, unstructured text, image, quality dimension, quality metrics. Recent research has widely explored the problem of aesthetics assessment of images with generic content. However, few approaches have been specifically designed to predict the aesthetic quality of images containing human faces, which make up a massive portion of photos in the web. This paper introduces a method for aesthetic quality assessment of images with faces. We exploit three different Convolutional Neural Networks to encode information regarding perceptual quality, global image aesthetics, and facial attributes; then, a model is trained to combine these features to explicitly predict the aesthetics of images containing faces. Experimental results show that our approach outperforms existing methods for both binary, i.e. low/high, and continuous aesthetic score prediction on four different databases in the state-of-the-art. The interaction of light and object surfaces generates color signals in the visible band that are responsible for digital acquisition system outputs. Inverting this mapping from the sensor space back to the wavelength domain is of great interest for many applications. Since 1964, with the idea of Cohen to exploit the characteristic of smoothness of surface reflectance functions, a lot of work has been done in the analysis, synthesis and recovering of spectral information using linear models. The general use of such models is for the establishment of a one-to-one relationship between sensor's data and reflectance spectrum, with the requirement of ensuring the quality of the recovered spectrum in terms of physical feasibility and naturalness. In this paper, we propose a solution to correct the outcome of a generic recovery method, in order to take into account quality constrains. Our strategy assumes the smoothness of the solution of the recovery method, an assumption implicitly satisfied from the adoption of linear models to represent reflectance functions. The illuminant estimation has an important role in many domain applications such as digital still cameras and mobile phones, where the final image quality could be heavily affected by a poor compensation of the ambient illumination effects. In this paper we present an algorithm, not dependent on the acquiring device, for illuminant estimation and compensation directly in the color filter array (CFA) domain of digital still cameras. The proposed algorithm takes into account both chromaticity and intensity information of the image data, and performs the illuminant compensation by a diagonal transform. It works by combining a spatial segmentation process with empirical designed weighting functions aimed to select the scene objects containing more information for the light chromaticity estimation. This algorithm has been designed exploiting an experimental framework developed by the authors and it has been evaluated on a database of real scene images acquired in different, carefully controlled, illuminant conditions. The results show that a combined multi domain pixel analysis leads to an improvement of the performance when compared to single domain pixel analysis."
  },
  {
    "id": "412301",
    "input": "Generate a title for the following abstract of a paper: The representation problem of independence models is studied by focusing on acyclic directed graph (DAG). We present the algorithm PC* in order to look for a perfect map. However, when a perfect map does not exist, so that PC* fails, it is interesting to find a minimal I--map, which represents as many triples as possible in J*. Therefore we describe an algorithm which finds such a map by means of a backtracking procedure. In this paper we consider conditional independence models closed under graphoid properties. We investigate their representation by means of acyclic directed graphs (DAG). A new algorithm to build a DAG, given an ordering among random variables, is described and peculiarities and advantages of this approach are discussed. Finally, some properties ensuring the existence of perfect maps are provided. These conditions can be used to define a procedure able to find a perfect map for some classes of independence models. In this paper we study the problem of representing probabilistic independence models, in particular those closed under graphoid properties. We focus on acyclic directed graph (DAG): a new algorithm to build a DAG, given an ordering among random variables, is described and peculiarities and advantages of this approach are discussed. Moreover, we provide a necessary and sufficient condition for the existence of a perfect map representing an independence model and we describe an algorithm based on this characterization. We provide a necessary and sufficient condition for the existence of a perfect; map representing an independence model and we give an algorithm for checking this condition and drawing a perfect map, when it exists."
  },
  {
    "id": "411951",
    "input": "Generate a title for the following abstract of a paper: In the all pairs bottleneck paths (APBP) problem, one is given a directed graph with real weights on its edges. Viewing the weights as capacities, one is asked to determine, for all pairs (s,t) of vertices, the maximum amount of flow that can be routed along a single path from s to t. The APBP problem was first studied in operations research, shortly after the introduction of maximum flows and all pairs shortest paths. We present the first truly subcubic algorithm for APBP in general dense graphs. In par- ticular, we give a procedure for computing the (max,min)-product of two arbitrary matrices over R ( {\u00a5, \u00a5} in O(n2+w/3) O(n2.792) time, where n is the number of vertices and w is the exponent for matrix multiplication over rings. Max-min products can be used to compute the maximum bottleneck values for all pairs of vertices together with a \"successor matrix\" from which one can extract an explicit maximum bottleneck path for any pair of vertices in time linear in the length of the path. In the all-pairs bottleneck paths (APBP) problem (a.k.a. all-pairs maximum capacity paths), one is given a directed graph with real non-negative capacities on its edges and is asked to determine, for all pairs of vertices s and t, the capacity of a single path for which a maximum amount of flow can be routed from s to t. The APBP problem was first studied in operations research, shortly after the introduction of maximum flows and all-pairs shortest paths. We present the first truly sub-cubic algorithm for APBP in general dense graphs. In particular, we give a procedure for computing the (max, min)-product of two arbitrary matrices over R \u222a (\u221e,-\u221e) in O(n2+\u03a9/3) \u2264 O(n2.792) time, where n is the number of vertices and \u03a9 is the exponent for matrix multiplication over rings. Using this procedure, an explicit maximum bottleneck path for any pair of nodes can be extracted in time linear in the length of the path. For a graph G with real weights assigned to the vertices (edges), the MAX H-SUBGRAPH problem is to find an H-subgraph of G with maximum total weight, if one exists. Our main results are new strongly polynomial algorithms for the MAX H-SUBGRAPH problem. Some of our algorithms are based, in part, on fast matrix multiplication. For vertex-weighted graphs with n vertices we solve a more general problem: the all pairs MAX H-SUBGRAPH problem, where the task is to find for every pair of vertices u,v, a maximum H-subgraph containing both u and v, if one exists. We obtain an O(nt(\u03c9,h))-time algorithm for the all pairs MAX H-SUBGRAPH problem in the case where H is a fixed graph with h vertices and \u03c9 t(\u03c9,h) is determined by solving a small integer program. In particular, heaviest triangles for all pairs can be found in O(n2+1/(4-\u03c9)) \u2264 o(n2.616)-time. For h&equals;4,5,8 the running time of our algorithm essentially matches that of the (unweighted) H-subgraph detection problem. Using rectangular matrix multiplication, the value of t(\u03c9,h) can be improved; for example, the runtime for triangles becomes O(n2.575). We also present improved algorithms for the MAX H-SUBGRAPH problem in the edge-weighted case. In particular, we obtain an O(m2\u22121/k log n)-time algorithm for the heaviest cycle of length 2k or 2k\u22121 in a graph with m edges and an O(n3/log n)-time randomized algorithm for finding the heaviest cycle of any fixed length. Our methods also yield efficient algorithms for several related problems that are faster than any previously existing algorithms. For example, we show how to find chromatic H-subgraphs in edge-colored graphs, and how to compute the most significant bits of the distance product of two real matrices, in truly subcubic time. This paper considers a natural fault-tolerant shortest paths problem: for some constant integer f, given a directed weighted graph with no negative cycles and two fixed vertices s and t, compute (either explicitly or implicitly) for every tuple of f edges, the distance from s to t if these edges fail. We call this problem f-Fault Replacement Paths (f FRP).We first present an $\\tilde{O}(n^{3}$) time algorithm for 2FRP in n-vertex directed graphs with arbitrary edge weights and no negative cycles. As 2FRP is a generalization of the well-studied Replacement Paths problem (RP) that asks for the distances between s and t for any single edge failure, 2FRP is at least as hard as RP. Since RP in graphs with arbitrary weights is equivalent in a fine-grained sense to All-Pairs Shortest Paths (APSP) [Vassilevska Williams and Williams FOCS\u201910, J. ACM\u201918], 2FRP is at least as hard as APSP, and thus a substantially subcubic time algorithm in the number of vertices for 2FRP would be a breakthrough. Therefore, our algorithm in $\\tilde{O}(n^{3})$ time is conditionally nearly optimal. Our algorithm immediately implies an $\\tilde{O}(n^{f+1})$ time algorithm for the more general f FRP problem, giving the first improvement over the straightforward $O(n^{f+2})$ time algorithm.Then we focus on the restriction of 2FRP to graphs with small integer weights bounded by M in absolute values. We show that similar to $\\mathrm{R}\\mathrm{P}, 2\\mathrm{F}\\mathrm{R}\\mathrm{P}$ has a substantially subcubic time algorithm for small enough M. Using the current best algorithms for rectangular matrix multiplication, we obtain a randomized algorithm that runs in $\\tilde{O}(M^{2/3}n^{2.9153})$ time. This immediately implies an improvement over our $\\tilde{O}(n^{f+1})$ time arbitrary weight algorithm for all $f\\gt1$. We also present a data structure variant of the algorithm that can trade off pre-processing and query time. In addition to the algebraic algorithms, we also give an $n^{8/3-o(1)}$ conditional lower bound for combinatorial 2FRP algorithms in directed unweighted graphs, and more generally, combinatorial lower bounds for the data structure version of $fF\\mathrm{R}\\mathrm{P}$."
  },
  {
    "id": "411469",
    "input": "Generate a title for the following abstract of a paper: Social robots should be able to automatically understand and respond to human touch. The meaning of touch does not only depend on the form of touch but also on the context in which the touch takes place. To gain more insight into the factors that are relevant to interpret the meaning of touch within a social context we elicited touch behaviors by letting participants interact with a robot pet companion in the context of different affective scenarios. In a contextualized lab setting participants (n= 31) acted as if they were coming home in different emotional states (i.e. stressed, depressed, relaxed and excited) without being given specific instructions on the kinds of behaviors that they should display. Based on video footage of the interactions and interviews we explored the use of touch behaviors, the expressed social messages and the expected robot pet responses. Results show that emotional state influenced the social messages that were communicated to the robot pet as well as the expected responses. Furthermore, it was found that multimodal cues were used to communicate with the robot pet, that is, participants often talked to the robot pet while touching it and making eye contact. Additionally, the findings of this study indicate that the categorization of touch behaviors into discrete touch gesture categories based on dictionary definitions is not a suitable approach to capture the complex nature of touch behaviors in less controlled settings. These findings can inform the design of a behavioral model for robot pet companions and future directions to interpret touch behaviors in less controlled settings are discussed. For an artifact such as a robot or a virtual agent to respond appropriately to human social touch behavior, it should be able to automatically detect and recognize touch. This paper describes the data collection of CoST: Corpus of Social Touch, a data set containing 7805 captures of 14 different social touch gestures. All touch gestures were performed in three variants: gentle, normal and rough on a pressure sensor grid wrapped around a mannequin arm. Recognition of these 14 gesture classes using various classifiers yielded accuracies up to 60\u00a0%; moreover, gentle gestures proved to be harder to classify than normal and rough gestures. We further investigated how different classifiers, interpersonal differences, gesture confusions and gesture variants affected the recognition accuracy. Finally, we present directions for further research to ensure proper transfer of the touch modality from interpersonal interaction to areas such as human\u2013robot interaction (HRI). Touch behavior is of great importance during social interaction. To transfer the tactile modality from interpersonal interaction to other areas such as Human-Robot Interaction (HRI) and remote communication automatic recognition of social touch is necessary. This paper introduces CoST: Corpus of Social Touch, a collection containing 7805 instances of 14 different social touch gestures. The gestures were performed in three variations: gentle, normal and rough, on a sensor grid wrapped around a mannequin arm. Recognition of the rough variations of these 14 gesture classes using Bayesian classifiers and Support Vector Machines (SVMs) resulted in an overall accuracy of 54% and 53%, respectively. Furthermore, this paper provides more insight into the challenges of automatic recognition of social touch gestures, including which gestures can be recognized more easily and which are more difficult to recognize. Computational models that attempt to predict when a virtual human should backchannel are often based on the analysis of recordings of face-to-face conversations between humans. Building a model based on a corpus brings with it the problem that people differ in the way they behave. The data provides examples of responses of a single person in a particular context but in the same context another person might not have provided a response. Vice versa, the corpus will contain contexts in which the particular listener recorded did not produce a backchannel response, where another person would have responded. Listeners can differ in the amount, the timing and the type of backchannels they provide to the speaker, because of individual differences - related to personality, gender, or culture, for instance. To gain more insight in this variation we have collected data in which we record the behaviors of three listeners interacting with one speaker. All listeners think they are having a one-on-one conversation with the speaker, while the speaker actually only sees one of the listeners. The context, in this case the speaker's actions, is for all three listeners the same and they respond to it individually. This way we have created data on cases in which different persons show similar behaviors and cases in which they behave differently. With the recordings of this data collection study we can start building our model of backchannel behavior for virtual humans that takes into account similarities and differences between persons."
  },
  {
    "id": "411806",
    "input": "Generate a title for the following abstract of a paper: Recent contributions clearly show that eliminating bloat in a genetic programming system does not necessarily eliminate overfitting and vice-versa. This fact seems to contradict a common agreement of many researchers known as the minimum description length principle, which states that the best model is the one that minimizes the amount of information needed to encode it. Another common agreement is that overfitting should be, in some sense, related to the functional complexity of the model. The goal of this paper is to define three measures to respectively quantify bloat, overfitting and functional complexity of solutions and show their suitability on a set of test problems including a simple bidimensional symbolic regression test function and two real-life multidimensional regression problems. The experimental results are encouraging and should pave the way to further investigation. Advantages and drawbacks of the proposed measures are discussed, and ways to improve them are suggested. In the future, these measures should be useful to study and better understand the relationship between bloat, overfitting and functional complexity of solutions. The relationship between generalization and solutions functional complexity in genetic programming (GP) has been recently investigated. Three main contributions are contained in this paper: (1) a new measure of functional complexity for GP solutions, called Graph Based Complexity (GBC) is defined and we show that it has a higher correlation with GP performance on out-of-sample data than another complexity measure introduced in a recent publication. (2) A new measure is presented, called Graph Based Learning Ability (GBLA). It is inspired by the GBC and its goal is to quantify the ability of GP to learn \"difficult\" training points; we show that GBLA is negatively correlated with the performance of GP on out-of-sample data. (3) Finally, we use the ideas that have inspired the definition of GBC and GBLA to define a new fitness function, whose suitability is empirically demonstrated. The experimental results reported in this paper have been obtained using three real-life multidimensional regression problems. In iterative supervised learning algorithms it is common to reach a point in the search where no further induction seems to be possible with the available data. If the search is continued beyond this point, the risk of overfitting increases significantly. Following the recent developments in inductive semantic stochastic methods, this paper studies the feasibility of using information gathered from the semantic neighborhood to decide when to stop the search. Two semantic stopping criteria are proposed and experimentally assessed in Geometric Semantic Genetic Programming (GSGP) and in the Semantic Learning Machine (SLM) algorithm (the equivalent algorithm for neural networks). The experiments are performed on real-world high-dimensional regression datasets. The results show that the proposed semantic stopping criteria are able to detect stopping points that result in a competitive generalization for both GSGP and SLM. This approach also yields computationally efficient algorithms as it allows the evolution of neural networks in less than 3 seconds on average, and of GP trees in at most 10 seconds. The usage of the proposed semantic stopping criteria in conjunction with the computation of optimal mutation/learning steps also results in small trees and neural networks. Since its introduction, Geometric Semantic Genetic Programming (GSGP) has aroused the interest of numerous researchers and several studies have demonstrated that GSGP is able to effectively optimize training data by means of small variation steps, that also have the effect of limiting overfitting. In order to speed up the search process, in this paper we propose a system that integrates a local search strategy into GSGP (called GSGP-LS). Furthermore, we present a hybrid approach, that combines GSGP and GSGP-LS, aimed at exploiting both the optimization speed of GSGP-LS and the ability to limit overfitting of GSGP. The experimental results we present, performed on a set of complex real-life applications, show that GSGP-LS achieves the best training fitness while converging very quickly, but severely overfits. On the other hand, GSGP converges slowly relative to the other methods, but is basically not affected by overfitting. The best overall results were achieved with the hybrid approach, allowing the search to converge quickly, while also exhibiting a noteworthy ability to limit overfitting. These results are encouraging, and suggest that future GSGP algorithms should focus on finding the correct balance between the greedy optimization of a local search strategy and the more robust geometric semantic operators."
  },
  {
    "id": "41897",
    "input": "Generate a title for the following abstract of a paper: We present a program analysis that can automatically discover the shape of complex pointer data structures. The discovered invariants are, then, used to verify the absence of safety errors in the program, or to check whether the program preserves the data consistency. Our analysis extends the shape analysis of Sagiv et al. with grammar annotations, which can precisely express the shape of complex data structures. We demonstrate the usefulness of our analysis with binomial heap construction and the Schorr-Waite tree traversal. For a binomial heap construction algorithm, our analysis returns a grammar that precisely describes the shape of a binomial heap; for the Schorr-Waite tree traversal, our analysis shows that at the end of the execution, the result is a tree and there are no memory leaks. Shape analysis algorithms statically infer deep properties of the runtime heap, such as whether a variable points to a cyclic or acyclic linked list. Unfortunately, there are unsolved problems that make it difficult for shape analyses being to be used for real-world programs. The problems include: performance of the analysis; dealing with low-level language features; and supporting complex data-structures used in real-world programs, without sacrificing precision or performance of the analysis. In this talk, I will present work on shape analysis for Windows device drivers based on separation logic formulae. Device drivers basically use linked lists, but complex varieties of linked list unlike those usually studied in shape analysis. I will explain the nature of those structures, which open problems matter most for our analysis, and how we approach some of those problems. In particular, I will describe how higher-order predicates let us succinctly describe a variety of data structures, and how discovery of parameters to higher-order predicates allows an analysis that is not tied to specific structures. The accurate and efficient treatment of mutable data structures is one of the outstanding problem areas in automatic program verification and analysis. Shape analysis is a form of program analysis that attempts to infer descriptions of the data structures in a program, and to prove that these structures are not misused or corrupted. It is one of the more challenging and expensive forms of program analysis, due to the complexity of aliasing and the need to look arbitrarily deeply into the program heap. This article describes a method of boosting shape analyses by defining a compositional method, where each procedure is analyzed independently of its callers. The analysis algorithm uses a restricted fragment of separation logic, and assigns a collection of Hoare triples to each procedure; the triples provide an over-approximation of data structure usage. Our method brings the usual benefits of compositionality---increased potential to scale, ability to deal with incomplete programs, graceful way to deal with imprecision---to shape analysis, for the first time. The analysis rests on a generalized form of abduction (inference of explanatory hypotheses), which we call bi-abduction. Bi-abduction displays abduction as a kind of inverse to the frame problem: it jointly infers anti-frames (missing portions of state) and frames (portions of state not touched by an operation), and is the basis of a new analysis algorithm. We have implemented our analysis and we report case studies on smaller programs to evaluate the quality of discovered specifications, and larger code bases (e.g., sendmail, an imap server, a Linux distribution) to illustrate the level of automation and scalability that we obtain from our compositional method. This article makes number of specific technical contributions on proof procedures and analysis algorithms, but in a sense its more important contribution is holistic: the explanation and demonstration of how a massive increase in automation is possible using abductive inference. We call a data structure overlaid, if a node in the structure includes links for multiple data structures and these links are intended to be used at the same time. In this paper, we present a static program analysis for overlaid data structures. Our analysis implements two main ideas. The first is to run multiple sub-analyses that track information about non-overlaid data structures, such as lists. Each sub-analysis infers shape properties of only one component of an overlaid data structure, but the results of these sub-analyses are later combined to derive the desired safety properties about the whole overlaid data structure. The second idea is to control the communication among the sub-analyses using ghost states and ghost instructions. The purpose of this control is to achieve a high level of efficiency by allowing only necessary information to be transferred among sub-analyses and at as few program points as possible. Our analysis has been successfully applied to prove the memory safety of the Linux deadline IO scheduler and AFS server."
  },
  {
    "id": "411801",
    "input": "Generate a title for the following abstract of a paper: Modern mass spectrometry allows the determination of proteomic fingerprints of body fluids like serum, saliva or urine. These measurements can be used in many medical applications in order to diagnose the current state or predict the evolution of a disease. Recent developments in machine learning allow one to exploit such datasets, characterized by small numbers of very high-dimensional samples.We propose a systematic approach based on decision tree ensemble methods, which is used to automatically determine proteomic biomarkers and predictive models. The approach is validated on two datasets of surface-enhanced laser desorption/ionization time of flight measurements, for the diagnosis of rheumatoid arthritis and inflammatory bowel diseases. The results suggest that the methodology can handle a broad class of similar problems. In this paper, a temporal machine learning method is presented which is able to automatically construct rules allowing to detect as soon as possible an event using past and present measurements made on a complex system. This method can take as inputs dynamic scenarios directly described by temporal variables and provides easily readable results in the form of detection trees. The application of this method is discussed in the context of switching control. Switching (or discrete event) control of continuous systems consists in changing the structure of a system in such a way as to control its behavior. Given a particular discrete control switch, detection trees are applied to the induction of rules which decide based on the available measurements whether or not to operate a switch. Two practical applications are discussed in the context of electrical power systems emergency control. Reinforcement learning aims to determine an (infinite time horizon) optimal control policy from interaction with a system. It can be solved by approximating the so-called Q-function from a sample of four-tuples (x(t), u(t), r(t), x(t+1)) where x(t) denotes the system state at time t, ut the control action taken, rt the instantaneous reward obtained and x(t+1) the successor state of the system, and by determining the optimal control from the Q-function. Classical reinforcement learning algorithms use an ad hoc version of stochastic approximation which iterates over the Q-function approximations on a four-tuple by four-tuple basis. In this paper, we reformulate this problem as a sequence of batch mode supervised learning problems which in the limit converges to (an approximation of) the Q-function. Each step of this algorithm uses the full sample of four-tuples gathered from interaction with the system and extends by one step the horizon of the optimality criterion. An advantage of this approach is to allow the use of standard batch mode supervised learning algorithms, instead of the incremental versions used up to now. In addition to a theoretical justification the paper provides empirical tests in the context of the \"Car on the Hill\" control problem based on the use of ensembles of regression trees. The resulting algorithm is in principle able to handle efficiently large scale reinforcement learning problems. Motivation: Univariate statistical tests are widely used for biomarker discovery in bioinformatics. These procedures are simple, fast and their output is easily interpretable by biologists but they can only identify variables that provide a significant amount of information in isolation from the other variables. As biological processes are expected to involve complex interactions between variables, univariate methods thus potentially miss some informative biomarkers. Variable relevance scores provided by machine learning techniques, however, are potentially able to highlight multivariate interacting effects, but unlike the p-values returned by univariate tests, these relevance scores are usually not statistically interpretable. This lack of interpretability hampers the determination of a relevance threshold for extracting a feature subset from the rankings and also prevents the wide adoption of these methods by practicians. Results: We evaluated several, existing and novel, procedures that extract relevant features from rankings derived from machine learning approaches. These procedures replace the relevance scores with measures that can be interpreted in a statistical way, such as p-values, false discovery rates, or family wise error rates, for which it is easier to determine a significance level. Experiments were performed on several artificial problems as well as on real microarray datasets. Although the methods differ in terms of computing times and the tradeoff, they achieve in terms of false positives and false negatives, some of them greatly help in the extraction of truly relevant biomarkers and should thus be of great practical interest for biologists and physicians. As a side conclusion, our experiments also clearly highlight that using model performance as a criterion for feature selection is often counter-productive. Availability and implementation: Python source codes of all tested methods, as well as the MATLAB scripts used for data simulation, can be found in the Supplementary Material. Contact:vahuynh@ulg.ac.be, or p.geurts@ulg.ac.be Supplementary information:Supplementary data are available at Bioinformatics online."
  },
  {
    "id": "41598",
    "input": "Generate a title for the following abstract of a paper: The function of non-coding RNA genes largely depends on their secondary structure and the interaction with other molecules. Thus, an accurate prediction of secondary structure and RNA-RNA interaction is essential for the understanding of biological roles and pathways associated with a specific RNA gene. We present web servers to analyze multiple RNA sequences for common RNA structure and for RNA interaction sites. The web servers are based on the recent PET (Probabilistic Evolutionary and Thermodynamic) models PETfold and PETcofold, but add user friendly features ranging from a graphical layer to interactive usage of the predictors. Additionally, the web servers provide direct access to annotated RNA alignments, such as the Rfam 10.0 database and multiple alignments of 16 vertebrate genomes with human. The web servers are freely available at: http://rth.dk/resources/petfold/. BACKGROUND: Many regulatory non-coding RNAs (ncRNAs) function through complementary binding with mRNAs or other ncRNAs, e.g., microRNAs, snoRNAs and bacterial sRNAs. Predicting these RNA interactions is essential for functional studies of putative ncRNAs or for the design of artificial RNAs. Many ncRNAs show clear signs of undergoing compensating base changes over evolutionary time. Here, we postulate that a non-negligible part of the existing RNA-RNA interactions contain preserved but covarying patterns of interactions. METHODS: We present a novel method that takes compensating base changes across the binding sites into account. The algorithm works in two steps on two pre-generated multiple alignments. In the first step, individual base pairs with high reliability are found using the PETfold algorithm, which includes evolutionary and thermodynamic properties. In step two (where high reliability base pairs from step one are constrained as unpaired), the principle of cofolding is combined with hierarchical folding. The final prediction of intra- and inter-molecular base pairs consists of the reliabilities computed from the constrained expected accuracy scoring, which is an extended version of that used for individual multiple alignments. RESULTS: We derived a rather extensive algorithm. One of the advantages of our approach (in contrast to other RNA-RNA interaction prediction methods) is the application of covariance detection and prediction of pseudoknots between intra- and inter-molecular base pairs. As a proof of concept, we show an example and discuss the strengths and weaknesses of the approach. Recent interests, such as RNA interference and antisense RNA regulation, strongly motivate the problem of predicting whether two nucleic acid strands interact.Regulatory non-coding RNAs (ncRNAs) such as microRNAs play an important role in gene regulation. Studies on both prokaryotic and eukaryotic cells show that such ncRNAs usually bind to their target mRNA to regulate the translation of corresponding genes. The specificity of these interactions depends on the stability of intermolecular and intramolecular base pairing. While methods like deep sequencing allow to discover an ever increasing set of ncRNAs, there are no high-throughput methods available to detect their associated targets. Hence, there is an increasing need for precise computational target prediction. In order to predict base-pairing probability of any two bases in interacting nucleic acids, it is necessary to compute the interaction partition function over the whole ensemble. The partition function is a scalar value from which various thermodynamic quantities can be derived. For example, the equilibrium concentration of each complex nucleic acid species and also the melting temperature of interacting nucleic acids can be calculated based on the partition function of the complex.We present a model for analyzing the thermodynamics of two interacting nucleic acid strands considering the most general type of interactions studied in the literature. We also present a corresponding dynamic programming algorithm that computes the partition function over (almost) all physically possible joint secondary structures formed by two interacting nucleic acids in O(n(6)) time. We verify the predictive power of our algorithm by computing (i) the melting temperature for interacting RNA pairs studied in the literature and (ii) the equilibrium concentration for several variants of the OxyS-fhlA complex. In both experiments, our algorithm shows high accuracy and outperforms competitors.Software and web server is available at http://compbio.cs.sfu.ca/taverna/pirna/.Supplementary data are avaliable at Bioinformatics online. Due to recent algorithmic progress, tools for the gold standard of comparative RNA analysis, namely Sankoff-style simultaneous alignment and folding, are now readily applicable. Such approaches, however, compare RNAs with respect to a simultaneously predicted, single, nested consensus structure. To make multiple alignment of RNAs available in cases, where this limitation of the standard approach is critical, we introduce a web server that provides a complete and convenient interface to the RNA structure alignment tool \". This tool uniquely supports RNAs with multiple conserved structures per RNA and aligns pseudoknots intrinsically; these features are highly desirable for aligning riboswitches, RNAs with conserved folding pathways, or pseudoknots. We represent structural input and output information as base pair probability dot plots; this provides large flexibility in the input, ranging from fixed structures to structure ensembles, and enables immediate visual analysis of the results. In contrast to conventional Sankoff-style approaches, \" optimizes all structural similarities in the input simultaneously, for example across an entire RNA structure ensemble. Even compared with already costly Sankoff-style alignment, \" solves an intrinsically much harder problem by applying advanced, constraint-based, algorithmic techniques. Although \" is specialized to the alignment of RNAs with several conserved structures, its performance on RNAs in general is on par with state-of-the-art general-purpose RNA alignment tools, as we show in a Bralibase 2.1 benchmark. The web server is freely available at http://rna.informatik.uni-freiburg.de/CARNA."
  },
  {
    "id": "41718",
    "input": "Generate a title for the following abstract of a paper: A secure reliable multicast protocol enables a process to send a message to a group of recipients such that all honest destinations receive the same message, despite the malicious efforts of fewer than a third of them, including the sender. This has been shown to be a useful tool in building secure distributed services, albeit with a cost that typically grows linearly with the size of the system. For very large networks, for which such a cost may be too prohibitive, we present two approaches for bringing the cost down: First, we show a protocol whose cost is on the order of the number of tolerated failures. Secondly, we show how relaxing the consistency requirement to a selected probability level of guarantee can bring down the associated cost to a constant. A secure reliable multicast protocol enables a process to send a message to a\ngroup of recipients such that all correct destinations receive the same\nmessage, despite the malicious efforts of fewer than a third of the total\nnumber of processes, including the sender. This has been sh own to be a useful\ntool in building secure distributed services, albeit with a cost that typically\ngrows linearly with the size of the system. For very large networks, for which\nthis is prohibitive, we present two approaches for reducing the cost: First, we\nshow a protocol whose cost is on the order of the number of tolerated failures.\nSecondly, we show how relaxing the consistency requirement to a probabilistic\nguarantee can reduce the associated cost, effectively to a constant. A reliable multicast protocol enables a process to multicast a message to a group of processes in a way that ensures that all honest destination-group members receive the same message, even if some group members and the multicast initiator are maliciously faulty. Reliable multicast has been shown to be useful for building multiparty cryptographic protocols and secure distributed services. We present a high-throughput reliable multicast protocol that tolerates the malicious behavior of up to fewer than one-third of the group members. Our protocol achieves high-throughput using a novel technique for chaining multicasts, whereby the cost of ensuring agreement on each multicast message is amortized over many multicasts. This is coupled with a novel flow-control mechanism that yields low multicast latency. In this paper, we provide a method to safely store a document in perhaps the most challenging settings, a highly decentralized replicated storage system where up to half of the storage servers may incur arbitrary failures, including alterations to data stored in them. Using an error correcting code (ECC), e.g., a Reed-Solomon code, one can take n pieces of a document, replace each piece with another piece of size larger by a factor of such that it is possible to recover the original set even when up to t of the larger pieces are altered. For t close to n/2 the space blowup factor of this scheme is close to n, and the overhead of an ECC such as the Reed-Solomon code degenerates to that of a trivial replication code. We show a technique to reduce this large space overhead for high values of t. Our scheme blows up each piece by a factor slightly larger than two using an erasure code which makes it possible to recover the original set using n/2-O (n/d) of the pieces, where d80 is a fixed constant. Then we attach to each piece O (d log n/log d) additional bits to make it possible to identify a large enough set of unmodified pieces, with negligible error probability, assuming that at least half the pieces are unmodified and with low complexity. For values of t close to n/2 we achieve a large asymptotic space reduction over the best possible space blowup of any ECC in deterministic setting. Our approach makes use of a d-regular expander graph to compute the bits required for the identification of n/2-O (n/d) good pieces."
  }
]